{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXnjCLJyM9XMXXEhf9+v8o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poushali96/ExCIR/blob/main/ExCIRBlockCIR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dICq-8mtuXLn"
      },
      "outputs": [],
      "source": [
        "# excir_bounds_demo.py — fixed LB/UB plotting, robust MMD^2, drift vs q, timing, shading, annotations\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "\n",
        "# ---------- metrics ----------\n",
        "def median_pairwise_distance_pooled(A, B, max_pairs=20000):\n",
        "    \"\"\"Median pairwise distance on pooled data (for RBF bandwidth).\"\"\"\n",
        "    P = np.vstack([A, B])\n",
        "    n = P.shape[0]\n",
        "    if n * (n - 1) // 2 > max_pairs:\n",
        "        idx = np.random.choice(n, size=int(np.sqrt(2 * max_pairs)) + 1, replace=False)\n",
        "        P = P[idx]\n",
        "    d2 = np.sum((P[:, None, :] - P[None, :, :]) ** 2, axis=-1)\n",
        "    iu = np.triu_indices_from(d2, k=1)\n",
        "    med = np.median(np.sqrt(d2[iu]))\n",
        "    return max(med, 1e-6)\n",
        "\n",
        "def pairwise_d2(X, Y):\n",
        "    X2 = np.sum(X**2, axis=1, keepdims=True)\n",
        "    Y2 = np.sum(Y**2, axis=1, keepdims=True).T\n",
        "    return X2 + Y2 - 2.0 * X @ Y.T\n",
        "\n",
        "def mmd2_gaussian_multiscale(Y, Yp, scales=(0.5, 1.0, 2.0, 4.0)):\n",
        "    \"\"\"Unbiased MMD^2 with Gaussian kernel, pooled bandwidth + multiscale.\"\"\"\n",
        "    base_sigma = median_pairwise_distance_pooled(Y, Yp)\n",
        "    n, m = Y.shape[0], Yp.shape[0]\n",
        "    d2_xx = pairwise_d2(Y, Y);   np.fill_diagonal(d2_xx, 0.0)\n",
        "    d2_yy = pairwise_d2(Yp, Yp); np.fill_diagonal(d2_yy, 0.0)\n",
        "    d2_xy = pairwise_d2(Y, Yp)\n",
        "    vals = []\n",
        "    for s in scales:\n",
        "        sigma = base_sigma * s\n",
        "        Kxx = np.exp(-d2_xx / (2 * sigma**2))\n",
        "        Kyy = np.exp(-d2_yy / (2 * sigma**2))\n",
        "        Kxy = np.exp(-d2_xy / (2 * sigma**2))\n",
        "        term_xx = Kxx.sum() / (n * (n - 1)) if n > 1 else 0.0\n",
        "        term_yy = Kyy.sum() / (m * (m - 1)) if m > 1 else 0.0\n",
        "        term_xy = 2.0 * Kxy.mean()\n",
        "        vals.append(max(term_xx + term_yy - term_xy, 0.0))\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "def projection_alignment(Y, Yp):\n",
        "    \"\"\"D_proj = ||Y - (Yp A + 1 b^T)||_F / ||Y||_F with LS A,b.\"\"\"\n",
        "    n, q = Y.shape\n",
        "    Z = np.hstack([Yp, np.ones((n, 1))])\n",
        "    theta, *_ = np.linalg.lstsq(Z, Y, rcond=None)\n",
        "    A, b = theta[:-1, :], theta[-1, :]\n",
        "    resid = Y - (Yp @ A + np.ones((n, 1)) @ b[None, :])\n",
        "    return float(np.linalg.norm(resid, \"fro\") / (np.linalg.norm(Y, \"fro\") + 1e-12))\n",
        "\n",
        "def kde_1d(x, grid, h):\n",
        "    diffs = (grid[:, None] - x[None, :]) / h\n",
        "    ker = np.exp(-0.5 * diffs**2) / (np.sqrt(2 * np.pi) * h)\n",
        "    dens = ker.mean(axis=1)\n",
        "    return np.maximum(dens, 1e-12)\n",
        "\n",
        "def axiswise_kl(Y, Yp, grid_pts=160):\n",
        "    \"\"\"Max over axes of 1D KDE KL(p||q) on standardized coordinates.\"\"\"\n",
        "    n, q = Y.shape\n",
        "    vals = []\n",
        "    for j in range(q):\n",
        "        x = (Y[:, j] - Y[:, j].mean()) / (Y[:, j].std() + 1e-8)\n",
        "        y = (Yp[:, j] - Yp[:, j].mean()) / (Yp[:, j].std() + 1e-8)\n",
        "        lo = min(x.min(), y.min()) - 3.0\n",
        "        hi = max(x.max(), y.max()) + 3.0\n",
        "        grid = np.linspace(lo, hi, grid_pts)\n",
        "        hx = max(1.06 * np.std(x) * (len(x) ** (-1 / 5)), 1e-2)\n",
        "        hy = max(1.06 * np.std(y) * (len(y) ** (-1 / 5)), 1e-2)\n",
        "        px = kde_1d(x, grid, hx)\n",
        "        py = kde_1d(y, grid, hy)\n",
        "        dx = (hi - lo) / (grid_pts - 1)\n",
        "        vals.append(float(np.sum(px * (np.log(px) - np.log(py))) * dx))\n",
        "    return float(np.max(vals))\n",
        "\n",
        "# ---------- synthetic outputs ----------\n",
        "def synth_full_outputs(n_eval, q, signal_dim=4, noise=0.2):\n",
        "    Z = np.random.randn(n_eval, signal_dim)\n",
        "    W = np.random.randn(signal_dim, q)\n",
        "    return Z @ W + noise * np.random.randn(n_eval, q)\n",
        "\n",
        "def random_orthogonal(q):\n",
        "    Q, _ = np.linalg.qr(np.random.randn(q, q))\n",
        "    return Q\n",
        "\n",
        "def synth_lightweight_outputs(Y_full, nprime, sep=0.9):\n",
        "    \"\"\"\n",
        "    Distortion decreases with n' and grows with sqrt(q): rotation + scale + noise.\n",
        "    \"\"\"\n",
        "    n_eval, q = Y_full.shape\n",
        "    eps = (sep * np.sqrt(q)) / np.sqrt(max(nprime, 1) / 500.0 + 1.0)\n",
        "    R = random_orthogonal(q)\n",
        "    A = np.eye(q) + eps * R\n",
        "    b = (eps * 0.25) * np.random.randn(q)\n",
        "    noise = (eps * 0.35) * (Y_full.std() + 1e-8) * np.random.randn(n_eval, q)\n",
        "    return Y_full @ A + b + noise\n",
        "\n",
        "# ---------- plotting ----------\n",
        "def plot_mmd2_vs_nprime(df, out_path):\n",
        "    plt.figure(figsize=(8, 5.0))\n",
        "    all_vals = []\n",
        "    for q in sorted(df[\"q\"].unique()):\n",
        "        sub = df[df[\"q\"] == q].sort_values(\"nprime\")\n",
        "        y = sub[\"MMD2\"].values\n",
        "        all_vals.append(y)\n",
        "        plt.plot(sub[\"nprime\"].values, y, marker=\"o\", linewidth=2, label=f\"q={q}\")\n",
        "    all_vals = np.concatenate(all_vals) if len(all_vals) else np.array([1e-9])\n",
        "    plt.xlabel(\"$n'$ (lightweight sample size)\")\n",
        "    plt.ylabel(\"Empirical MMD$^2$\")\n",
        "    plt.title(\"MMD$^2$ vs. $n'$ across output dimensions\")\n",
        "    plt.grid(True, linestyle=\"--\", linewidth=0.6, alpha=0.6)\n",
        "    plt.yscale(\"log\")\n",
        "    ymin = max(np.nanmin(all_vals) * 0.7, 1e-8)\n",
        "    ymax = max(np.nanmax(all_vals) * 1.3, ymin * 10)\n",
        "    plt.ylim(ymin, ymax)\n",
        "    plt.legend(frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "# ---------- core experiment ----------\n",
        "def run_experiment(\n",
        "    save_dir=\"excir_bounds_demo_out\",\n",
        "    n_eval=800,\n",
        "    q_list=(1, 2, 4, 8, 16, 32),\n",
        "    nprime_grid=np.unique(np.logspace(np.log10(100), np.log10(12000), 50).astype(int)),\n",
        "    eps_acc=0.2, # Changed to a less-strict value\n",
        "    L_l=1.0, C_mmd=0.3, C_kl=0.3, # Changed to less-strict values\n",
        "    time_budget_sec=0.2,\n",
        "    min_pass=3,\n",
        "    seed=33,\n",
        "):\n",
        "    \"\"\"\n",
        "    This function runs the core experiment to determine the statistical and operational\n",
        "    bounds for the sample size n' based on various metrics and runtime constraints.\n",
        "    It generates synthetic data, performs metric calculations, and then plots the\n",
        "    lower and upper bounds to find a feasible operating window.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    out = Path(save_dir); out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # per-metric tolerances\n",
        "    eps_proj = eps_acc / (3.0 * L_l)\n",
        "    eps_mmd  = (eps_acc / (3.0 * C_mmd))**2\n",
        "    eps_kl   = (eps_acc / (3.0 * C_kl))**2\n",
        "\n",
        "    rows = []\n",
        "    for q in q_list:\n",
        "        Y = synth_full_outputs(n_eval, q)\n",
        "        for nprime in nprime_grid:\n",
        "            t0 = time.time()\n",
        "            Yp = synth_lightweight_outputs(Y, nprime, sep=0.9)\n",
        "            D_proj = projection_alignment(Y, Yp)\n",
        "            MMD2   = mmd2_gaussian_multiscale(Y, Yp)\n",
        "            KLax   = axiswise_kl(Y, Yp)\n",
        "            cost_reps = int(0.4 * q)\n",
        "            for _ in range(cost_reps):\n",
        "                _ = pairwise_d2(Y[:200], Yp[:200])\n",
        "            elapsed = time.time() - t0\n",
        "            flags = [D_proj <= eps_proj, MMD2 <= eps_mmd, KLax <= eps_kl]\n",
        "            pass_count = sum(int(b) for b in flags)\n",
        "            rows.append({\n",
        "                \"q\": q, \"nprime\": nprime,\n",
        "                \"D_proj\": D_proj, \"MMD2\": MMD2, \"KL_axiswise\": KLax,\n",
        "                \"pass_proj\": flags[0], \"pass_mmd\": flags[1], \"pass_kl\": flags[2],\n",
        "                \"pass_count\": pass_count,\n",
        "                \"all_pass\": (pass_count == 3),\n",
        "                \"maj_pass\": (pass_count >= 2),\n",
        "                \"runtime_sec\": elapsed\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values([\"q\", \"nprime\"])\n",
        "    df.to_csv(out / \"metrics_results.csv\", index=False)\n",
        "\n",
        "    # --- LB/UB computation ---\n",
        "    lb_any = df[df[\"pass_count\"] >= min_pass].groupby(\"q\")[\"nprime\"].min()\n",
        "    ub_emp = df[df[\"runtime_sec\"] <= time_budget_sec].groupby(\"q\")[\"nprime\"].max()\n",
        "\n",
        "    qs = sorted(df[\"q\"].unique())\n",
        "    lb_vals = [lb_any.get(q, np.nan) for q in qs]\n",
        "    ub_vals = [ub_emp.get(q, np.nan) for q in qs]\n",
        "\n",
        "    # --- two-line plot ---\n",
        "    plt.figure(figsize=(8.2, 5.6))\n",
        "\n",
        "    # Plot the lower bound line with a label\n",
        "    plt.plot(qs, lb_vals, marker=\"o\", linewidth=2.5, markersize=7,\n",
        "             label=\"Statistical lower bound (min $n'$)\")\n",
        "\n",
        "    # Plot the upper bound line with a label\n",
        "    plt.plot(qs, ub_vals, marker=\"s\", linewidth=2.5, markersize=7,\n",
        "             label=f\"Operational upper bound (runtime ≤ {int(time_budget_sec)}s)\")\n",
        "\n",
        "    # Shade the feasible window where LB is less than or equal to UB\n",
        "    x = np.asarray(qs, float)\n",
        "    y1 = np.asarray(lb_vals, float)\n",
        "    y2 = np.asarray(ub_vals, float)\n",
        "\n",
        "    plt.fill_between(x, y1, y2, where=(y2 >= y1), color=\"tab:grey\", alpha=0.15,\n",
        "                     step=\"mid\", label=\"Feasible $n'$ window\")\n",
        "\n",
        "    # Annotate values on the plot\n",
        "    if np.isfinite(np.nanmax(ub_vals)) and np.isfinite(np.nanmin(lb_vals)):\n",
        "        bump = 0.02 * (np.nanmax(ub_vals) - np.nanmin(lb_vals) + 1.0)\n",
        "    else:\n",
        "        bump = 50.0\n",
        "\n",
        "    for x0, y0 in zip(qs, lb_vals):\n",
        "        if np.isfinite(y0):\n",
        "            plt.text(x0, y0 + bump, f\"{int(y0):,}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "    for x0, y0 in zip(qs, ub_vals):\n",
        "        if np.isfinite(y0):\n",
        "            plt.text(x0, y0 + bump, f\"{int(y0):,}\", ha=\"center\", va=\"bottom\",\n",
        "                     fontsize=9, color=\"tab:orange\")\n",
        "\n",
        "    plt.xlabel(\"Output dimension q\", fontsize=12)\n",
        "    plt.ylabel(\"n' (samples)\", fontsize=12)\n",
        "    plt.title(\"Lower vs. Upper bounds on n' across output dimensions\", fontsize=14)\n",
        "    plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
        "    plt.grid(True, linestyle=\"--\", linewidth=0.6, alpha=0.6)\n",
        "    plt.legend(frameon=False, fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    fig_path = out / \"lb_ub_two_lines.png\"\n",
        "    plt.savefig(fig_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # MMD^2 curves\n",
        "    plot_mmd2_vs_nprime(df, out / \"mmd2_vs_nprime.png\")\n",
        "\n",
        "    # table\n",
        "    pd.DataFrame({\"q\": qs, \"LB_nprime\": lb_vals, \"UB_nprime\": ub_vals}).to_csv(\n",
        "        out / \"lb_ub_table.csv\", index=False\n",
        "    )\n",
        "\n",
        "    print(\"Saved:\", fig_path)\n",
        "    print(\"Saved:\", out / \"mmd2_vs_nprime.png\")\n",
        "    print(\"Saved:\", out / \"metrics_results.csv\")\n",
        "    print(\"Saved:\", out / \"lb_ub_table.csv\")\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment(\n",
        "        save_dir=\"excir_bounds_demo_out\",\n",
        "        n_eval=800,\n",
        "        q_list=(1, 2, 4, 8, 16, 32),\n",
        "        nprime_grid=np.unique(np.logspace(np.log10(100), np.log10(12000), 50).astype(int)),\n",
        "        eps_acc=0.2, # Changed to a less-strict value\n",
        "        L_l=1.0,\n",
        "        C_mmd=0.3, # Changed to a less-strict value\n",
        "        C_kl=0.3,  # Changed to a less-strict value\n",
        "        time_budget_sec=0.2,\n",
        "        min_pass=3,\n",
        "        seed=33,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "Q6jRQ1NpukjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CONFIG ==========================\n",
        "TASK        = \"classification\"     # keep \"classification\" for this demo\n",
        "N_SAMPLES   = 6000                 # synthetic rows to generate\n",
        "TEST_SIZE   = 0.20                 # test split\n",
        "VAL_SIZE    = 0.20                 # share of (train) held out for validation\n",
        "LIGHT_FRAC  = 0.35                 # fraction for lightweight subset (0.2–0.5 typical)\n",
        "RANDOM_SEED = 42                   # reproducibility\n",
        "TOP_K_LIST  = [6, 8]               # for sufficiency table\n",
        "TOPN_PLOTS  = 20                   # how many features to show in the CIR plot\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.utils import check_random_state\n",
        "from scipy.special import expit as sigmoid\n",
        "import shap\n",
        "\n",
        "rng = check_random_state(RANDOM_SEED)\n",
        "\n",
        "# ---------- 1) Generate synthetic vehicular dataset ----------\n",
        "feat_names = [\n",
        "    \"speed_kph\",\"rpm\",\"throttle\",\"brake\",\"steering_deg\",\"gear\",\n",
        "    \"accel_long\",\"accel_lat\",\"yaw_rate\",\"road_grade\",\n",
        "    \"ambient_temp\",\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\",\n",
        "    \"engine_load\",\"maf\",\"intake_air_temp\",\"battery_v\",\"fuel_rate\"\n",
        "]\n",
        "\n",
        "n = N_SAMPLES\n",
        "speed = np.clip(rng.normal(80, 15, n), 0, None)\n",
        "throttle = np.clip(rng.beta(2, 2, n), 0, 1)\n",
        "brake = np.clip(1 - throttle + rng.normal(0, 0.15, n), 0, 1)\n",
        "steering = rng.normal(0, 10, n)\n",
        "gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, n), 1, 7)\n",
        "accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, n)\n",
        "accel_lat = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, n)\n",
        "yaw_rate = rng.normal(steering/30 * (speed/60), 0.2, n)\n",
        "road_grade = rng.normal(0, 2, n)\n",
        "ambient_temp = rng.normal(20, 8, n)\n",
        "tire_base = rng.normal(34, 1.0, (n,4))\n",
        "low_mask = rng.uniform(0,1,n) < 0.15\n",
        "tire_drop = rng.normal(4, 1.0, (n,4)) * low_mask[:,None]\n",
        "tires = tire_base - tire_drop\n",
        "engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, n), 0, 100)\n",
        "maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,n), 0, None)\n",
        "intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, n), -10, 80)\n",
        "battery_v = np.clip(rng.normal(13.8, 0.3, n) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,n), 0, None)\n",
        "rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, n), 700, 7000)\n",
        "\n",
        "X_df = pd.DataFrame({\n",
        "    \"speed_kph\": speed,\n",
        "    \"rpm\": rpm,\n",
        "    \"throttle\": throttle,\n",
        "    \"brake\": brake,\n",
        "    \"steering_deg\": steering,\n",
        "    \"gear\": gear,\n",
        "    \"accel_long\": accel_long,\n",
        "    \"accel_lat\": accel_lat,\n",
        "    \"yaw_rate\": yaw_rate,\n",
        "    \"road_grade\": road_grade,\n",
        "    \"ambient_temp\": ambient_temp,\n",
        "    \"tire_fl\": tires[:,0],\n",
        "    \"tire_fr\": tires[:,1],\n",
        "    \"tire_rl\": tires[:,2],\n",
        "    \"tire_rr\": tires[:,3],\n",
        "    \"engine_load\": engine_load,\n",
        "    \"maf\": maf,\n",
        "    \"intake_air_temp\": intake_air_temp,\n",
        "    \"battery_v\": battery_v,\n",
        "    \"fuel_rate\": fuel_rate,\n",
        "})\n",
        "\n",
        "# --- FIXED LINE (use Pandas row-wise min + Series.clip) ---\n",
        "low_tire = (32 - X_df[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "\n",
        "risk_logit = (\n",
        "    1.2*(X_df[\"speed_kph\"]-110)/20\n",
        "    + 1.1*X_df[\"brake\"]\n",
        "    + 0.9*np.abs(X_df[\"steering_deg\"])/15\n",
        "    + 0.7*np.abs(X_df[\"yaw_rate\"])\n",
        "    + 0.8*(low_tire)\n",
        "    + 0.7*(X_df[\"engine_load\"]/100)\n",
        "    + 0.3*(X_df[\"road_grade\"]/5)\n",
        "    - 0.2*(X_df[\"battery_v\"]-13.5)\n",
        ")\n",
        "p = sigmoid(risk_logit + rng.normal(0, 0.4, n))\n",
        "y = (rng.uniform(0,1,n) < p).astype(int)\n",
        "\n",
        "# ---------- 2) Split & scale ----------\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X_df, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=VAL_SIZE, stratify=y_train_full, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "# ---------- 3) Train original model ----------\n",
        "Model = GradientBoostingClassifier\n",
        "orig_model = Model(random_state=RANDOM_SEED).fit(\n",
        "    scaler.fit_transform(pd.concat([X_train, X_val], axis=0)),\n",
        "    np.concatenate([y_train, y_val], axis=0)\n",
        ")\n",
        "\n",
        "def model_outputs(model, Xs):\n",
        "    return model.predict_proba(Xs)[:, -1]\n",
        "\n",
        "yhat_val_orig  = model_outputs(orig_model, scaler.transform(X_val))\n",
        "yhat_test_orig = model_outputs(orig_model, scaler.transform(X_test))\n",
        "\n",
        "# ---------- 4) CIR computation ----------\n",
        "def compute_cir(Xs, yhat, names):\n",
        "    n = Xs.shape[0]\n",
        "    y_bar = yhat.mean()\n",
        "    vals = []\n",
        "    for i in range(Xs.shape[1]):\n",
        "        f = Xs[:, i]\n",
        "        f_bar = f.mean()\n",
        "        m = 0.5*(f_bar + y_bar)\n",
        "        num = n*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "        den = np.sum((f - m)**2) + np.sum((yhat - m)**2)\n",
        "        eta = float(num/den) if den > 0 else 0.0\n",
        "        vals.append(eta)\n",
        "    return pd.Series(vals, index=names).sort_values(ascending=False)\n",
        "\n",
        "cir_orig_val = compute_cir(scaler.transform(X_val), yhat_val_orig, X_df.columns.tolist())\n",
        "\n",
        "# ---------- 5) Lightweight environment ----------\n",
        "full_Xs = scaler.fit_transform(pd.concat([X_train, X_val], axis=0))\n",
        "full_y  = np.concatenate([y_train, y_val], axis=0)\n",
        "full_df = pd.DataFrame(full_Xs, columns=X_df.columns).assign(target=full_y)\n",
        "\n",
        "light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
        "    lambda g: g.sample(max(1, int(len(g)*LIGHT_FRAC)), random_state=RANDOM_SEED)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "X_light_s = light_df[X_df.columns].values\n",
        "y_light   = light_df[\"target\"].values\n",
        "\n",
        "light_model = Model(random_state=RANDOM_SEED).fit(X_light_s, y_light)\n",
        "yhat_val_light = model_outputs(light_model, scaler.transform(X_val))\n",
        "cir_light_val  = compute_cir(scaler.transform(X_val), yhat_val_light, X_df.columns.tolist())\n",
        "\n",
        "# ---------- 6) Figure: CIR Original vs Lightweight ----------\n",
        "order = cir_orig_val.index[:TOPN_PLOTS]\n",
        "vals_o = cir_orig_val.loc[order].values\n",
        "vals_l = cir_light_val.loc[order].values\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ypos = np.arange(len(order))\n",
        "bar_h = 0.35\n",
        "plt.barh(ypos+bar_h/2, vals_l, height=bar_h, label=\"Lightweight Space\")\n",
        "plt.barh(ypos-bar_h/2, vals_o, height=bar_h, label=\"Original Space\")\n",
        "plt.yticks(ypos, order)\n",
        "plt.xlabel(\"Feature Score (CIR)\")\n",
        "plt.title(\"Feature Importance Comparison (Lightweight vs Original)\")\n",
        "plt.legend()\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- 7) SHAP bar chart (Original model) ----------\n",
        "explainer = shap.TreeExplainer(orig_model)\n",
        "val_sample = min(800, X_val_s.shape[0])\n",
        "idx = rng.choice(X_val_s.shape[0], size=val_sample, replace=False)\n",
        "shap_vals = explainer.shap_values(X_val_s[idx])\n",
        "if isinstance(shap_vals, list):\n",
        "    shap_mat = shap_vals[-1]\n",
        "else:\n",
        "    shap_mat = shap_vals\n",
        "mean_abs_shap = np.abs(shap_mat).mean(axis=0)\n",
        "shap_series = pd.Series(mean_abs_shap, index=X_df.columns).sort_values(ascending=False)\n",
        "\n",
        "topN_shap = 10\n",
        "shap_top = shap_series.head(topN_shap)\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.barh(np.arange(len(shap_top))[::-1], shap_top.values[::-1])\n",
        "plt.yticks(np.arange(len(shap_top))[::-1], shap_top.index[::-1])\n",
        "plt.xlabel(\"Mean |SHAP| value\")\n",
        "plt.title(\"Top features by SHAP value (Original model)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- 8) Top-k sufficiency table ----------\n",
        "def train_eval_on_features(cols):\n",
        "    idxs = [list(X_df.columns).index(c) for c in cols]\n",
        "    X_full_sub = full_Xs[:, idxs]\n",
        "    X_test_sub = scaler.transform(X_test)[:, idxs]\n",
        "    m = Model(random_state=RANDOM_SEED).fit(X_full_sub, full_y)\n",
        "    ypred = m.predict(X_test_sub)\n",
        "    return accuracy_score(y_test, ypred) * 100.0\n",
        "\n",
        "rows = []\n",
        "for k in TOP_K_LIST:\n",
        "    excir_feats = cir_orig_val.head(k).index.tolist()\n",
        "    shap_feats  = shap_series.head(k).index.tolist()\n",
        "    acc_excir = train_eval_on_features(excir_feats)\n",
        "    acc_shap  = train_eval_on_features(shap_feats)\n",
        "    rows.append([\"SHAP-Ranked Features\", k, acc_shap])\n",
        "    rows.append([\"ExCIR-Ranked Features\", k, acc_excir])\n",
        "\n",
        "acc_table = pd.DataFrame(rows, columns=[\"Method\", \"No. of Features\", \"Accuracy (%)\"])\n",
        "print(\"\\nTop-k sufficiency results:\")\n",
        "print(acc_table.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vYFWNkG2uluf",
        "outputId": "ffe18014-acbe-45e1-aacb-cfc58fa246ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3231880579.py:140: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-k sufficiency results:\n",
            "               Method  No. of Features  Accuracy (%)\n",
            " SHAP-Ranked Features                6     68.916667\n",
            "ExCIR-Ranked Features                6     69.583333\n",
            " SHAP-Ranked Features                8     69.750000\n",
            "ExCIR-Ranked Features                8     69.416667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig(\"cir_light_vs_orig.png\", dpi=300, bbox_inches=\"tight\")\n",
        "# after SHAP plot:\n",
        "plt.savefig(\"shap_top.png\", dpi=300, bbox_inches=\"tight\")\n",
        "# save sufficiency table\n",
        "acc_table.to_csv(\"topk_sufficiency.csv\", index=False)\n",
        "print(acc_table.to_latex(index=False, float_format=\"%.2f\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "r7D_eh9SupWB",
        "outputId": "e4f406d7-84e6-49b2-e4d3-936828487b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "Method & No. of Features & Accuracy (%) \\\\\n",
            "\\midrule\n",
            "SHAP-Ranked Features & 6 & 68.92 \\\\\n",
            "ExCIR-Ranked Features & 6 & 69.58 \\\\\n",
            "SHAP-Ranked Features & 8 & 69.75 \\\\\n",
            "ExCIR-Ranked Features & 8 & 69.42 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "fractions = [0.2, 0.3, 0.35, 0.4, 0.5]\n",
        "best = None\n",
        "for f in fractions:\n",
        "    t0 = perf_counter()\n",
        "    light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
        "        lambda g: g.sample(max(1, int(len(g)*f)), random_state=RANDOM_SEED)\n",
        "    ).reset_index(drop=True)\n",
        "    X_light_s = light_df[X_df.columns].values\n",
        "    y_light   = light_df[\"target\"].values\n",
        "    m = Model(random_state=RANDOM_SEED).fit(X_light_s, y_light)\n",
        "    yhat_val_light = model_outputs(m, scaler.transform(X_val))\n",
        "    cir_l = compute_cir(scaler.transform(X_val), yhat_val_light, X_df.columns.tolist())\n",
        "    # agreement metrics\n",
        "    from scipy.stats import spearmanr\n",
        "    r, _ = spearmanr(cir_orig_val.rank(), cir_l[cir_orig_val.index].rank())\n",
        "    topk = 8\n",
        "    overlap = len(set(cir_orig_val.head(topk).index) & set(cir_l.head(topk).index))/topk\n",
        "    runtime = perf_counter() - t0\n",
        "    cand = (f, r, overlap, runtime)\n",
        "    if best is None or (r, overlap, -runtime) > (best[1], best[2], -best[3]):\n",
        "        best = cand\n",
        "print(f\"Best LIGHT_FRAC={best[0]:.2f} | Spearman={best[1]:.3f} | Top-8 overlap={best[2]:.2%} | time={best[3]:.1f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2ayJga_u79Z",
        "outputId": "9a65e124-f36b-49a0-905b-0294623d9a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1083029539.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1083029539.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1083029539.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1083029539.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1083029539.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best LIGHT_FRAC=0.20 | Spearman=1.000 | Top-8 overlap=100.00% | time=1.6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def boot_acc(cols, B=200):\n",
        "    # map feature names -> column positions\n",
        "    idxs = [X_df.columns.get_loc(c) for c in cols]\n",
        "\n",
        "    # train on train+val (scaled), using only these columns; evaluate on test\n",
        "    X_full_sub = full_Xs[:, idxs]\n",
        "    X_test_sub = scaler.transform(X_test)[:, idxs]\n",
        "    m = Model(random_state=RANDOM_SEED).fit(X_full_sub, full_y)\n",
        "\n",
        "    # predictions on test\n",
        "    y_pred = m.predict(X_test_sub)\n",
        "\n",
        "    # --- make sure we're indexing by POSITION, not labels ---\n",
        "    y_true = np.asarray(y_test)        # force numpy arrays\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = y_true.shape[0]\n",
        "\n",
        "    base = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # bootstrap CI by resampling ROW POSITIONS\n",
        "    rng = np.random.default_rng(0)\n",
        "    boots = np.empty(B)\n",
        "    for b in range(B):\n",
        "        idx = rng.integers(0, n, n)    # positions 0..n-1\n",
        "        boots[b] = accuracy_score(y_true[idx], y_pred[idx])\n",
        "\n",
        "    lo, hi = np.percentile(boots, [2.5, 97.5])\n",
        "    return base*100, lo*100, hi*100\n",
        "\n"
      ],
      "metadata": {
        "id": "_f9aCeASu_gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in TOP_K_LIST:\n",
        "    excir_feats = cir_orig_val.head(k).index.tolist()\n",
        "    shap_feats  = shap_series.head(k).index.tolist()\n",
        "    print(f\"k={k}  ExCIR (%, 95% CI): {boot_acc(excir_feats)}   SHAP: {boot_acc(shap_feats)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-aV8DgxvNcq",
        "outputId": "02462d55-e4ac-4dc0-82df-017998e85727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k=6  ExCIR (%, 95% CI): (69.58333333333333, np.float64(67.33333333333333), np.float64(72.33333333333334))   SHAP: (68.91666666666667, np.float64(66.41666666666667), np.float64(72.08333333333333))\n",
            "k=8  ExCIR (%, 95% CI): (69.41666666666667, np.float64(67.25), np.float64(72.00208333333333))   SHAP: (69.75, np.float64(67.57916666666665), np.float64(72.58333333333333))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CONFIG ==========================\n",
        "TASK        = \"classification\"     # keep \"classification\" for this demo\n",
        "N_SAMPLES   = 6000                 # synthetic rows to generate\n",
        "TEST_SIZE   = 0.20                 # test split\n",
        "VAL_SIZE    = 0.20                 # share of (train) held out for validation\n",
        "LIGHT_FRAC  = 0.35                 # fraction for lightweight subset (0.2–0.5 typical)\n",
        "RANDOM_SEED = 42                   # reproducibility\n",
        "TOP_K_LIST  = [6, 8]               # for sufficiency table\n",
        "TOPN_PLOTS  = 20                   # how many features to show in the CIR plot\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.utils import check_random_state\n",
        "from scipy.special import expit as sigmoid\n",
        "import shap\n",
        "\n",
        "rng = check_random_state(RANDOM_SEED)\n",
        "\n",
        "# ---------- 1) Generate synthetic vehicular dataset ----------\n",
        "feat_names = [\n",
        "    \"speed_kph\",\"rpm\",\"throttle\",\"brake\",\"steering_deg\",\"gear\",\n",
        "    \"accel_long\",\"accel_lat\",\"yaw_rate\",\"road_grade\",\n",
        "    \"ambient_temp\",\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\",\n",
        "    \"engine_load\",\"maf\",\"intake_air_temp\",\"battery_v\",\"fuel_rate\"\n",
        "]\n",
        "\n",
        "n = N_SAMPLES\n",
        "speed = np.clip(rng.normal(80, 15, n), 0, None)\n",
        "throttle = np.clip(rng.beta(2, 2, n), 0, 1)\n",
        "brake = np.clip(1 - throttle + rng.normal(0, 0.15, n), 0, 1)\n",
        "steering = rng.normal(0, 10, n)\n",
        "gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, n), 1, 7)\n",
        "accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, n)\n",
        "accel_lat = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, n)\n",
        "yaw_rate = rng.normal(steering/30 * (speed/60), 0.2, n)\n",
        "road_grade = rng.normal(0, 2, n)\n",
        "ambient_temp = rng.normal(20, 8, n)\n",
        "tire_base = rng.normal(34, 1.0, (n,4))\n",
        "low_mask = rng.uniform(0,1,n) < 0.15\n",
        "tire_drop = rng.normal(4, 1.0, (n,4)) * low_mask[:,None]\n",
        "tires = tire_base - tire_drop\n",
        "engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, n), 0, 100)\n",
        "maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,n), 0, None)\n",
        "intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, n), -10, 80)\n",
        "battery_v = np.clip(rng.normal(13.8, 0.3, n) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,n), 0, None)\n",
        "rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, n), 700, 7000)\n",
        "\n",
        "X_df = pd.DataFrame({\n",
        "    \"speed_kph\": speed,\n",
        "    \"rpm\": rpm,\n",
        "    \"throttle\": throttle,\n",
        "    \"brake\": brake,\n",
        "    \"steering_deg\": steering,\n",
        "    \"gear\": gear,\n",
        "    \"accel_long\": accel_long,\n",
        "    \"accel_lat\": accel_lat,\n",
        "    \"yaw_rate\": yaw_rate,\n",
        "    \"road_grade\": road_grade,\n",
        "    \"ambient_temp\": ambient_temp,\n",
        "    \"tire_fl\": tires[:,0],\n",
        "    \"tire_fr\": tires[:,1],\n",
        "    \"tire_rl\": tires[:,2],\n",
        "    \"tire_rr\": tires[:,3],\n",
        "    \"engine_load\": engine_load,\n",
        "    \"maf\": maf,\n",
        "    \"intake_air_temp\": intake_air_temp,\n",
        "    \"battery_v\": battery_v,\n",
        "    \"fuel_rate\": fuel_rate,\n",
        "})\n",
        "\n",
        "# --- FIXED LINE (use Pandas row-wise min + Series.clip) ---\n",
        "low_tire = (32 - X_df[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "\n",
        "risk_logit = (\n",
        "    1.2*(X_df[\"speed_kph\"]-110)/20\n",
        "    + 1.1*X_df[\"brake\"]\n",
        "    + 0.9*np.abs(X_df[\"steering_deg\"])/15\n",
        "    + 0.7*np.abs(X_df[\"yaw_rate\"])\n",
        "    + 0.8*(low_tire)\n",
        "    + 0.7*(X_df[\"engine_load\"]/100)\n",
        "    + 0.3*(X_df[\"road_grade\"]/5)\n",
        "    - 0.2*(X_df[\"battery_v\"]-13.5)\n",
        ")\n",
        "p = sigmoid(risk_logit + rng.normal(0, 0.4, n))\n",
        "y = (rng.uniform(0,1,n) < p).astype(int)\n",
        "\n",
        "# ---------- 2) Split & scale ----------\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X_df, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=VAL_SIZE, stratify=y_train_full, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "# ---------- 3) Train original model ----------\n",
        "Model = GradientBoostingClassifier\n",
        "orig_model = Model(random_state=RANDOM_SEED).fit(\n",
        "    scaler.fit_transform(pd.concat([X_train, X_val], axis=0)),\n",
        "    np.concatenate([y_train, y_val], axis=0)\n",
        ")\n",
        "\n",
        "def model_outputs(model, Xs):\n",
        "    return model.predict_proba(Xs)[:, -1]\n",
        "\n",
        "yhat_val_orig  = model_outputs(orig_model, scaler.transform(X_val))\n",
        "yhat_test_orig = model_outputs(orig_model, scaler.transform(X_test))\n",
        "\n",
        "# ---------- 4) CIR computation ----------\n",
        "def compute_cir(Xs, yhat, names):\n",
        "    n = Xs.shape[0]\n",
        "    y_bar = yhat.mean()\n",
        "    vals = []\n",
        "    for i in range(Xs.shape[1]):\n",
        "        f = Xs[:, i]\n",
        "        f_bar = f.mean()\n",
        "        m = 0.5*(f_bar + y_bar)\n",
        "        num = n*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "        den = np.sum((f - m)**2) + np.sum((yhat - m)**2)\n",
        "        eta = float(num/den) if den > 0 else 0.0\n",
        "        vals.append(eta)\n",
        "    return pd.Series(vals, index=names).sort_values(ascending=False)\n",
        "\n",
        "cir_orig_val = compute_cir(scaler.transform(X_val), yhat_val_orig, X_df.columns.tolist())\n",
        "\n",
        "# ---------- 5) Lightweight environment ----------\n",
        "full_Xs = scaler.fit_transform(pd.concat([X_train, X_val], axis=0))\n",
        "full_y  = np.concatenate([y_train, y_val], axis=0)\n",
        "full_df = pd.DataFrame(full_Xs, columns=X_df.columns).assign(target=full_y)\n",
        "\n",
        "light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
        "    lambda g: g.sample(max(1, int(len(g)*LIGHT_FRAC)), random_state=RANDOM_SEED)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "X_light_s = light_df[X_df.columns].values\n",
        "y_light   = light_df[\"target\"].values\n",
        "\n",
        "light_model = Model(random_state=RANDOM_SEED).fit(X_light_s, y_light)\n",
        "yhat_val_light = model_outputs(light_model, scaler.transform(X_val))\n",
        "cir_light_val  = compute_cir(scaler.transform(X_val), yhat_val_light, X_df.columns.tolist())\n",
        "\n",
        "from time import perf_counter\n",
        "fractions = [0.2, 0.3, 0.35, 0.4, 0.5]\n",
        "best = None\n",
        "for f in fractions:\n",
        "    t0 = perf_counter()\n",
        "    light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
        "        lambda g: g.sample(max(1, int(len(g)*f)), random_state=RANDOM_SEED)\n",
        "    ).reset_index(drop=True)\n",
        "    X_light_s = light_df[X_df.columns].values\n",
        "    y_light   = light_df[\"target\"].values\n",
        "    m = Model(random_state=RANDOM_SEED).fit(X_light_s, y_light)\n",
        "    yhat_val_light = model_outputs(m, scaler.transform(X_val))\n",
        "    cir_l = compute_cir(scaler.transform(X_val), yhat_val_light, X_df.columns.tolist())\n",
        "    # agreement metrics\n",
        "    from scipy.stats import spearmanr\n",
        "    r, _ = spearmanr(cir_orig_val.rank(), cir_l[cir_orig_val.index].rank())\n",
        "    topk = 8\n",
        "    overlap = len(set(cir_orig_val.head(topk).index) & set(cir_l.head(topk).index))/topk\n",
        "    runtime = perf_counter() - t0\n",
        "    cand = (f, r, overlap, runtime)\n",
        "    if best is None or (r, overlap, -runtime) > (best[1], best[2], -best[3]):\n",
        "        best = cand\n",
        "print(f\"Best LIGHT_FRAC={best[0]:.2f} | Spearman={best[1]:.3f} | Top-8 overlap={best[2]:.2%} | time={best[3]:.1f}s\")\n",
        "\n",
        "# ---------- 6) Figure: CIR Original vs Lightweight ----------\n",
        "order = cir_orig_val.index[:TOPN_PLOTS]\n",
        "vals_o = cir_orig_val.loc[order].values\n",
        "vals_l = cir_light_val.loc[order].values\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ypos = np.arange(len(order))\n",
        "bar_h = 0.35\n",
        "plt.barh(ypos+bar_h/2, vals_l, height=bar_h, label=\"Lightweight Space\")\n",
        "plt.barh(ypos-bar_h/2, vals_o, height=bar_h, label=\"Original Space\")\n",
        "plt.yticks(ypos, order)\n",
        "plt.xlabel(\"Feature Score (CIR)\")\n",
        "plt.title(\"Feature Importance Comparison (Lightweight vs Original)\")\n",
        "plt.legend()\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- 7) SHAP bar chart (Original model) ----------\n",
        "explainer = shap.TreeExplainer(orig_model)\n",
        "val_sample = min(800, X_val_s.shape[0])\n",
        "idx = rng.choice(X_val_s.shape[0], size=val_sample, replace=False)\n",
        "shap_vals = explainer.shap_values(X_val_s[idx])\n",
        "if isinstance(shap_vals, list):\n",
        "    shap_mat = shap_vals[-1]\n",
        "else:\n",
        "    shap_mat = shap_vals\n",
        "mean_abs_shap = np.abs(shap_mat).mean(axis=0)\n",
        "shap_series = pd.Series(mean_abs_shap, index=X_df.columns).sort_values(ascending=False)\n",
        "\n",
        "topN_shap = 10\n",
        "shap_top = shap_series.head(topN_shap)\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.barh(np.arange(len(shap_top))[::-1], shap_top.values[::-1])\n",
        "plt.yticks(np.arange(len(shap_top))[::-1], shap_top.index[::-1])\n",
        "plt.xlabel(\"Mean |SHAP| value\")\n",
        "plt.title(\"Top features by SHAP value (Original model)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- 8) Top-k sufficiency table ----------\n",
        "def train_eval_on_features(cols):\n",
        "    idxs = [list(X_df.columns).index(c) for c in cols]\n",
        "    X_full_sub = full_Xs[:, idxs]\n",
        "    X_test_sub = scaler.transform(X_test)[:, idxs]\n",
        "    m = Model(random_state=RANDOM_SEED).fit(X_full_sub, full_y)\n",
        "    ypred = m.predict(X_test_sub)\n",
        "    return accuracy_score(y_test, ypred) * 100.0\n",
        "\n",
        "rows = []\n",
        "for k in TOP_K_LIST:\n",
        "    excir_feats = cir_orig_val.head(k).index.tolist()\n",
        "    shap_feats  = shap_series.head(k).index.tolist()\n",
        "    acc_excir = train_eval_on_features(excir_feats)\n",
        "    acc_shap  = train_eval_on_features(shap_feats)\n",
        "    rows.append([\"SHAP-Ranked Features\", k, acc_shap])\n",
        "    rows.append([\"ExCIR-Ranked Features\", k, acc_excir])\n",
        "\n",
        "acc_table = pd.DataFrame(rows, columns=[\"Method\", \"No. of Features\", \"Accuracy (%)\"])\n",
        "print(\"\\nTop-k sufficiency results:\")\n",
        "print(acc_table.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r6kSY5bHvQ5P",
        "outputId": "36f6a952-27a6-4285-984d-e955c7c20177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1221047036.py:140: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1221047036.py:156: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1221047036.py:156: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1221047036.py:156: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1221047036.py:156: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1221047036.py:156: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  light_df = full_df.groupby(\"target\", group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best LIGHT_FRAC=0.20 | Spearman=1.000 | Top-8 overlap=100.00% | time=1.5s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-k sufficiency results:\n",
            "               Method  No. of Features  Accuracy (%)\n",
            " SHAP-Ranked Features                6     68.916667\n",
            "ExCIR-Ranked Features                6     69.583333\n",
            " SHAP-Ranked Features                8     69.750000\n",
            "ExCIR-Ranked Features                8     69.416667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOBLp_utvWbJ",
        "outputId": "1a79716d-9056-44f9-8a9b-4314d964a5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lime) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.12/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=ad486e298fbebc403bd3c114b30c95887a3d0425d7dbeac8d1fe55f803966127\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/5d/0e/4b4fff9a47468fed5633211fb3b76d1db43fe806a17fb7486a\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 7b) LIME global ranking (Original model) ----------\n",
        "try:\n",
        "    from lime.lime_tabular import LimeTabularExplainer\n",
        "    lime_available = True\n",
        "except ImportError:\n",
        "    lime_available = False\n",
        "    print(\"LIME not installed. Run: pip install lime\")\n",
        "\n",
        "lime_series = None\n",
        "if lime_available:\n",
        "    # LIME perturbs in the original feature space (unscaled).\n",
        "    explainer_lime = LimeTabularExplainer(\n",
        "        training_data=X_train.values,\n",
        "        feature_names=X_df.columns.tolist(),\n",
        "        class_names=['safe','unsafe'] if TASK == \"classification\" else None,\n",
        "        mode='classification' if TASK == \"classification\" else 'regression',\n",
        "        discretize_continuous=True,\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    # LIME will call this with raw (unscaled) features; we wrap scaling + model\n",
        "    def predict_proba_raw(Xraw):\n",
        "        Xraw = np.array(Xraw)\n",
        "        Xs = scaler.transform(pd.DataFrame(Xraw, columns=X_df.columns))\n",
        "        if TASK == \"classification\":\n",
        "            return orig_model.predict_proba(Xs)\n",
        "        else:\n",
        "            # For regression mode, return 1D predictions\n",
        "            return orig_model.predict(Xs)\n",
        "\n",
        "    # Aggregate mean |LIME weight| over a validation subsample to get a global ranking\n",
        "    agg = np.zeros(len(X_df.columns), dtype=float)\n",
        "    count = 0\n",
        "    val_sample_lime = min(400, X_val.shape[0])  # keep it reasonable\n",
        "    idx = rng.choice(X_val.shape[0], size=val_sample_lime, replace=False)\n",
        "\n",
        "    for i in idx:\n",
        "        exp = explainer_lime.explain_instance(\n",
        "            X_val.iloc[i].values,\n",
        "            predict_proba_raw,\n",
        "            labels=[1] if TASK == \"classification\" else None,\n",
        "            num_features=len(X_df.columns)\n",
        "        )\n",
        "        # Use positive class map for classification; for regression use the only label\n",
        "        if TASK == \"classification\":\n",
        "            pairs = exp.as_map()[1]   # list of (feature_index, weight)\n",
        "        else:\n",
        "            label = list(exp.as_map().keys())[0]\n",
        "            pairs = exp.as_map()[label]\n",
        "        for j, w in pairs:\n",
        "            agg[j] += abs(w)\n",
        "        count += 1\n",
        "\n",
        "    lime_series = pd.Series(agg / max(count, 1), index=X_df.columns).sort_values(ascending=False)\n",
        "\n",
        "    # Plot top features by LIME\n",
        "    topN_lime = 10\n",
        "    lime_top = lime_series.head(topN_lime)\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.barh(np.arange(len(lime_top))[::-1], lime_top.values[::-1])\n",
        "    plt.yticks(np.arange(len(lime_top))[::-1], lime_top.index[::-1])\n",
        "    plt.xlabel(\"Mean |LIME weight|\")\n",
        "    plt.title(\"Top features by LIME (Original model)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "kkUCT-9bvXCi",
        "outputId": "a5933ba9-015f-4f5f-9169-1a4a200d4479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 8b) Top-k sufficiency with 95% bootstrap CIs (ExCIR vs SHAP vs LIME) ----------\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def boot_acc_ci(cols, B=500, seed=0):\n",
        "    \"\"\"\n",
        "    Retrain the classifier on train+val using only `cols`, evaluate on test,\n",
        "    and return (accuracy%, CI_low%, CI_high%) with nonparametric bootstrap over test rows.\n",
        "    \"\"\"\n",
        "    # map feature names -> column indices\n",
        "    idxs = [X_df.columns.get_loc(c) for c in cols]\n",
        "\n",
        "    # training (train+val) and test matrices restricted to the subset\n",
        "    X_full_sub = full_Xs[:, idxs]                    # full_Xs is scaler.fit_transform(train+val)\n",
        "    X_test_sub = scaler.transform(X_test)[:, idxs]   # test, scaled, subset\n",
        "\n",
        "    # fit and predict\n",
        "    model = Model(random_state=RANDOM_SEED).fit(X_full_sub, full_y)\n",
        "    y_pred = model.predict(X_test_sub)\n",
        "\n",
        "    # base accuracy\n",
        "    y_true = np.asarray(y_test)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = y_true.shape[0]\n",
        "    base = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # bootstrap CI (model fixed, resample test rows)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    boots = np.empty(B, dtype=float)\n",
        "    for b in range(B):\n",
        "        idx = rng.integers(0, n, n)  # positions 0..n-1\n",
        "        boots[b] = accuracy_score(y_true[idx], y_pred[idx])\n",
        "\n",
        "    lo, hi = np.percentile(boots, [2.5, 97.5])\n",
        "    return base*100, lo*100, hi*100\n",
        "\n",
        "# assemble rankings (skip LIME if not computed)\n",
        "rankings = [\n",
        "    (\"ExCIR-Ranked Features\", cir_orig_val),\n",
        "    (\"SHAP-Ranked Features\",  shap_series),\n",
        "]\n",
        "if 'lime_series' in globals() and lime_series is not None:\n",
        "    rankings.append((\"LIME-Ranked Features\", lime_series))\n",
        "\n",
        "rows = []\n",
        "for k in TOP_K_LIST:  # e.g., [6, 8]\n",
        "    for name, series in rankings:\n",
        "        feats = series.head(k).index.tolist()\n",
        "        acc, lo, hi = boot_acc_ci(feats, B=500, seed=0)\n",
        "        rows.append([name, k, acc, lo, hi])\n",
        "\n",
        "acc_ci_table = pd.DataFrame(\n",
        "    rows, columns=[\"Method\", \"No. of Features\", \"Accuracy (%)\", \"95% CI low\", \"95% CI high\"]\n",
        ")\n",
        "\n",
        "# pretty print\n",
        "fmt = {\n",
        "    \"Accuracy (%)\":  lambda x: f\"{x:.2f}\",\n",
        "    \"95% CI low\":    lambda x: f\"{x:.2f}\",\n",
        "    \"95% CI high\":   lambda x: f\"{x:.2f}\",\n",
        "}\n",
        "print(\"\\nTop-k sufficiency with 95% bootstrap CIs:\")\n",
        "print(acc_ci_table.to_string(index=False, formatters=fmt))\n",
        "\n",
        "# optional: save artifacts\n",
        "acc_ci_table.to_csv(\"topk_sufficiency_with_ci.csv\", index=False)\n",
        "print(\"\\nLaTeX table:\")\n",
        "print(acc_ci_table.to_latex(index=False, float_format=\"%.2f\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhIb4kScve-P",
        "outputId": "12ecb351-ac6e-4e69-e72d-5ddf7d119435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-k sufficiency with 95% bootstrap CIs:\n",
            "               Method  No. of Features Accuracy (%) 95% CI low 95% CI high\n",
            "ExCIR-Ranked Features                6        69.58      67.12       72.17\n",
            " SHAP-Ranked Features                6        68.92      66.25       71.79\n",
            " LIME-Ranked Features                6        68.92      66.25       71.79\n",
            "ExCIR-Ranked Features                8        69.42      66.83       71.83\n",
            " SHAP-Ranked Features                8        69.75      67.17       72.25\n",
            " LIME-Ranked Features                8        69.83      67.21       72.50\n",
            "\n",
            "LaTeX table:\n",
            "\\begin{tabular}{lrrrr}\n",
            "\\toprule\n",
            "Method & No. of Features & Accuracy (%) & 95% CI low & 95% CI high \\\\\n",
            "\\midrule\n",
            "ExCIR-Ranked Features & 6 & 69.58 & 67.12 & 72.17 \\\\\n",
            "SHAP-Ranked Features & 6 & 68.92 & 66.25 & 71.79 \\\\\n",
            "LIME-Ranked Features & 6 & 68.92 & 66.25 & 71.79 \\\\\n",
            "ExCIR-Ranked Features & 8 & 69.42 & 66.83 & 71.83 \\\\\n",
            "SHAP-Ranked Features & 8 & 69.75 & 67.17 & 72.25 \\\\\n",
            "LIME-Ranked Features & 8 & 69.83 & 67.21 & 72.50 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ExCIR Benchmarks: 6 Experiments with Synthetic Vehicular Data\n",
        "# ============================================================\n",
        "\n",
        "# --------- Config ---------\n",
        "TASK        = \"classification\"\n",
        "N_SAMPLES   = 6000\n",
        "TEST_SIZE   = 0.20\n",
        "VAL_SIZE    = 0.20\n",
        "RANDOM_SEED = 42\n",
        "TOP_K_LIST  = list(range(2, 21, 2))   # for curves\n",
        "TOP_K_SMALL = [6, 8]                  # for summary tables\n",
        "VAL_SHAP_MAX = 800                    # SHAP/LIME sample on val\n",
        "NOISE_TRIALS = 200\n",
        "NOISE_SIGMA  = 0.05                   # relative (on standardized X)\n",
        "LW_FRACTIONS = [0.20, 0.30, 0.35, 0.40, 0.50]\n",
        "CORR_LEVELS  = [0.0, 0.3, 0.6, 0.9]    # tire correlation sweep\n",
        "SAVE_DIR     = \".\"\n",
        "# --------------------------\n",
        "\n",
        "import os, time, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from scipy.special import expit as sigmoid\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# SHAP and LIME\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "rng = check_random_state(RANDOM_SEED)\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Synthetic Vehicular Data\n",
        "# -----------------------------\n",
        "def make_vehicle_dataset(n=6000, random_state=42, tire_corr=0.3):\n",
        "    rng = check_random_state(random_state)\n",
        "    feat_names = [\n",
        "        \"speed_kph\",\"rpm\",\"throttle\",\"brake\",\"steering_deg\",\"gear\",\n",
        "        \"accel_long\",\"accel_lat\",\"yaw_rate\",\"road_grade\",\"ambient_temp\",\n",
        "        \"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\",\n",
        "        \"engine_load\",\"maf\",\"intake_air_temp\",\"battery_v\",\"fuel_rate\"\n",
        "    ]\n",
        "\n",
        "    speed = np.clip(rng.normal(80, 15, n), 0, None)\n",
        "    throttle = np.clip(rng.beta(2, 2, n), 0, 1)\n",
        "    brake = np.clip(1 - throttle + rng.normal(0, 0.15, n), 0, 1)\n",
        "    steering = rng.normal(0, 10, n)\n",
        "    gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, n), 1, 7)\n",
        "    accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, n)\n",
        "    accel_lat = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, n)\n",
        "    yaw_rate = rng.normal(steering/30 * (speed/60), 0.2, n)\n",
        "    road_grade = rng.normal(0, 2, n)\n",
        "    ambient_temp = rng.normal(20, 8, n)\n",
        "\n",
        "    # Correlated tires via Gaussian copula-like construction\n",
        "    # Build covariance with off-diagonal = tire_corr\n",
        "    Sigma = (1 - tire_corr) * np.eye(4) + tire_corr * np.ones((4,4))\n",
        "    L = np.linalg.cholesky(Sigma)\n",
        "    z = rng.normal(size=(n,4)) @ L.T\n",
        "    tires_base = 34 + 1.0*z  # ~ N(34,1) with correlation\n",
        "    low_mask = (rng.uniform(0,1,n) < 0.15).astype(float)\n",
        "    tire_drop = (rng.normal(4, 1.0, (n,4)) * low_mask[:,None])\n",
        "    tires = tires_base - tire_drop\n",
        "\n",
        "    engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, n), 0, 100)\n",
        "    maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,n), 0, None)\n",
        "    intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, n), -10, 80)\n",
        "    battery_v = np.clip(rng.normal(13.8, 0.3, n) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "    fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,n), 0, None)\n",
        "    rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, n), 700, 7000)\n",
        "\n",
        "    X_df = pd.DataFrame({\n",
        "        \"speed_kph\": speed,\n",
        "        \"rpm\": rpm,\n",
        "        \"throttle\": throttle,\n",
        "        \"brake\": brake,\n",
        "        \"steering_deg\": steering,\n",
        "        \"gear\": gear,\n",
        "        \"accel_long\": accel_long,\n",
        "        \"accel_lat\": accel_lat,\n",
        "        \"yaw_rate\": yaw_rate,\n",
        "        \"road_grade\": road_grade,\n",
        "        \"ambient_temp\": ambient_temp,\n",
        "        \"tire_fl\": tires[:,0],\n",
        "        \"tire_fr\": tires[:,1],\n",
        "        \"tire_rl\": tires[:,2],\n",
        "        \"tire_rr\": tires[:,3],\n",
        "        \"engine_load\": engine_load,\n",
        "        \"maf\": maf,\n",
        "        \"intake_air_temp\": intake_air_temp,\n",
        "        \"battery_v\": battery_v,\n",
        "        \"fuel_rate\": fuel_rate,\n",
        "    })\n",
        "\n",
        "    low_tire = (32 - X_df[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "    risk_logit = (\n",
        "        1.2*(X_df[\"speed_kph\"]-110)/20\n",
        "        + 1.1*X_df[\"brake\"]\n",
        "        + 0.9*np.abs(X_df[\"steering_deg\"])/15\n",
        "        + 0.7*np.abs(X_df[\"yaw_rate\"])\n",
        "        + 0.8*(low_tire)\n",
        "        + 0.7*(X_df[\"engine_load\"]/100)\n",
        "        + 0.3*(X_df[\"road_grade\"]/5)\n",
        "        - 0.2*(X_df[\"battery_v\"]-13.5)\n",
        "    )\n",
        "    p = sigmoid(risk_logit + rng.normal(0, 0.4, n))\n",
        "    y = (rng.uniform(0,1,n) < p).astype(int)\n",
        "    return X_df, y, feat_names\n",
        "\n",
        "# ---------------------------------\n",
        "# 1) Core utilities (ExCIR/SHAP/LIME)\n",
        "# ---------------------------------\n",
        "def compute_cir(Xs, yhat, names):\n",
        "    \"\"\"Compute ExCIR per feature on standardized Xs and prob yhat.\"\"\"\n",
        "    n = Xs.shape[0]\n",
        "    y_bar = float(yhat.mean())\n",
        "    vals = []\n",
        "    for i in range(Xs.shape[1]):\n",
        "        f = Xs[:, i]\n",
        "        f_bar = float(f.mean())\n",
        "        m = 0.5*(f_bar + y_bar)\n",
        "        num = n*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "        den = float(np.sum((f - m)**2) + np.sum((yhat - m)**2))\n",
        "        eta = float(num/den) if den > 0 else 0.0\n",
        "        vals.append(eta)\n",
        "    return pd.Series(vals, index=names).sort_values(ascending=False)\n",
        "\n",
        "def shap_mean_abs(model, X_val_s, feature_names, max_rows=800, seed=RANDOM_SEED):\n",
        "    rng = check_random_state(seed)\n",
        "    idx = rng.choice(X_val_s.shape[0], size=min(max_rows, X_val_s.shape[0]), replace=False)\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    sv = explainer.shap_values(X_val_s[idx])\n",
        "    if isinstance(sv, list):  # tree binary sometimes returns [neg,pos]\n",
        "        sv = sv[-1]\n",
        "    mean_abs = np.abs(sv).mean(axis=0)\n",
        "    return pd.Series(mean_abs, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "def lime_global_ranking(model, X_train_s, X_val_s, feature_names, max_rows=400, seed=RANDOM_SEED):\n",
        "    rng = check_random_state(seed)\n",
        "    # wrap predict_proba on standardized inputs\n",
        "    def predict_fn(x):\n",
        "        return model.predict_proba(x)\n",
        "    explainer = LimeTabularExplainer(\n",
        "        X_train_s, mode='classification', feature_names=feature_names,\n",
        "        discretize_continuous=True, random_state=seed\n",
        "    )\n",
        "    idx = rng.choice(X_val_s.shape[0], size=min(max_rows, X_val_s.shape[0]), replace=False)\n",
        "    W = np.zeros(X_val_s.shape[1])\n",
        "    for i in idx:\n",
        "        exp = explainer.explain_instance(X_val_s[i], predict_fn, num_features=X_val_s.shape[1])\n",
        "        for j, w in exp.as_map()[1]:  # class 1 map\n",
        "            W[j] += abs(w)\n",
        "    W /= len(idx)\n",
        "    return pd.Series(W, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "# Train/eval helper on chosen columns\n",
        "def train_eval_model(model_cls, X_train_full, y_train_full, X_test, y_test, cols):\n",
        "    idxs = [list(X_train_full.columns).index(c) for c in cols]\n",
        "    Xtr = X_train_full.iloc[:, idxs].values\n",
        "    Xte = X_test.iloc[:, idxs].values\n",
        "    scaler = StandardScaler().fit(Xtr)\n",
        "    Xtr_s = scaler.transform(Xtr)\n",
        "    Xte_s = scaler.transform(Xte)\n",
        "    m = model_cls(random_state=RANDOM_SEED).fit(Xtr_s, y_train_full)\n",
        "    yprob = m.predict_proba(Xte_s)[:,1]\n",
        "    yhat = (yprob>=0.5).astype(int)\n",
        "    return accuracy_score(y_test, yhat), log_loss(y_test, yprob), roc_auc_score(y_test, yprob)\n",
        "\n",
        "def bootstrap_ci(accs, alpha=0.05):\n",
        "    lo, hi = np.percentile(accs, [100*alpha/2, 100*(1-alpha/2)])\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "def boot_acc(model_cls, X_train_full, y_train_full, X_test, y_test, cols, B=200, seed=0):\n",
        "    rng = check_random_state(seed)\n",
        "    idxs = [list(X_train_full.columns).index(c) for c in cols]\n",
        "    Xtr = X_train_full.iloc[:, idxs].values\n",
        "    Xte = X_test.iloc[:, idxs].values\n",
        "    scaler = StandardScaler().fit(Xtr)\n",
        "    Xtr_s = scaler.transform(Xtr)\n",
        "    Xte_s = scaler.transform(Xte)\n",
        "    m = model_cls(random_state=RANDOM_SEED).fit(Xtr_s, y_train_full)\n",
        "    yprob = m.predict_proba(Xte_s)[:,1]\n",
        "    yhat = (yprob>=0.5).astype(int)\n",
        "    base = accuracy_score(y_test, yhat)\n",
        "    accs = []\n",
        "    for _ in range(B):\n",
        "        b = rng.integers(0, len(y_test), len(y_test))\n",
        "        accs.append(accuracy_score(y_test[b], yhat[b]))\n",
        "    lo, hi = bootstrap_ci(accs)\n",
        "    return base*100, lo*100, hi*100\n",
        "\n",
        "# ------------------------\n",
        "# Build base dataset/model\n",
        "# ------------------------\n",
        "X_df, y, feat_names = make_vehicle_dataset(n=N_SAMPLES, random_state=RANDOM_SEED, tire_corr=0.3)\n",
        "\n",
        "# splits (keep pandas for convenience)\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X_df, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=VAL_SIZE, stratify=y_train_full, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# standardize on train for ExCIR/SHAP/LIME computations\n",
        "scaler = StandardScaler().fit(X_train.values)\n",
        "X_train_s = scaler.transform(X_train.values)\n",
        "X_val_s   = scaler.transform(X_val.values)\n",
        "X_test_s  = scaler.transform(X_test.values)\n",
        "\n",
        "# Original model (Gradient Boosting)\n",
        "Model = GradientBoostingClassifier\n",
        "orig_model = Model(random_state=RANDOM_SEED).fit(\n",
        "    np.vstack([X_train_s, X_val_s]),\n",
        "    np.concatenate([y_train, y_val])\n",
        ")\n",
        "def model_outputs(model, Xs): return model.predict_proba(Xs)[:, -1]\n",
        "\n",
        "yhat_val = model_outputs(orig_model, X_val_s)\n",
        "\n",
        "# Rankings\n",
        "cir_val  = compute_cir(X_val_s, yhat_val, X_val.columns.tolist())\n",
        "shap_val = shap_mean_abs(orig_model, X_val_s, X_val.columns.tolist(), max_rows=VAL_SHAP_MAX)\n",
        "lime_val = lime_global_ranking(orig_model, X_train_s, X_val_s, X_val.columns.tolist(), max_rows=min(400, len(X_val)))\n",
        "\n",
        "# =============================\n",
        "# Experiment 1: Top-k Sufficiency Curves\n",
        "# =============================\n",
        "def exp1_sufficiency_curves():\n",
        "    methods = {\"ExCIR\": cir_val, \"SHAP\": shap_val, \"LIME\": lime_val}\n",
        "    results = {m: [] for m in methods}\n",
        "    for k in TOP_K_LIST:\n",
        "        for name, rank in methods.items():\n",
        "            cols = rank.head(k).index.tolist()\n",
        "            acc, ll, auc = train_eval_model(\n",
        "                GradientBoostingClassifier, X_train_full, y_train_full, X_test, y_test, cols\n",
        "            )\n",
        "            results[name].append(acc*100)\n",
        "    # plot\n",
        "    plt.figure(figsize=(7,5))\n",
        "    for name, accs in results.items():\n",
        "        plt.plot(TOP_K_LIST, accs, marker='o', label=name)\n",
        "    plt.xlabel(\"k (top features kept)\"); plt.ylabel(\"Test Accuracy (%)\")\n",
        "    plt.title(\"Top-k Sufficiency (Keep only top-k)\")\n",
        "    plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
        "    out = os.path.join(SAVE_DIR, \"exp1_topk_sufficiency.png\")\n",
        "    plt.savefig(out, dpi=150)\n",
        "    print(f\"[Exp1] Saved {out}\")\n",
        "exp1_sufficiency_curves()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nAll experiments completed. Check the generated PNGs and CSV in:\", os.path.abspath(SAVE_DIR))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "yREkEDz6voek",
        "outputId": "0feb1afa-20db-4295-843f-f89cbaf1db4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Exp1] Saved ./exp1_topk_sufficiency.png\n",
            "\n",
            "All experiments completed. Check the generated PNGs and CSV in: /content\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# FIXED Experiment 2: Necessity curves (robust)\n",
        "# ===========================================\n",
        "def exp2_necessity_curves_fixed(rankers=('ExCIR','SHAP','LIME'),\n",
        "                                max_remove=None,\n",
        "                                out=\"exp2_necessity_curves.png\"):\n",
        "    \"\"\"\n",
        "    For each ranking method, remove the top-m features (m=1..max_remove),\n",
        "    retrain on remaining features (train+val), and evaluate on test.\n",
        "    Skips the zero-feature case to avoid scaler/estimator errors.\n",
        "    Uses precomputed standardized arrays via column indexing.\n",
        "    \"\"\"\n",
        "    import math\n",
        "    from scipy.stats import sem\n",
        "\n",
        "    # Helper to fetch the available ranking series\n",
        "    def _get_series(name):\n",
        "        if name.upper() == 'EXCIR':\n",
        "            return cir_orig_val if 'cir_orig_val' in globals() else None\n",
        "        if name.upper() == 'SHAP':\n",
        "            return shap_series if 'shap_series' in globals() else None\n",
        "        if name.upper() == 'LIME':\n",
        "            return lime_series if 'lime_series' in globals() else None\n",
        "        return None\n",
        "\n",
        "    d = X_df.shape[1]\n",
        "    if max_remove is None:\n",
        "        # cap removal so at least 1 feature remains\n",
        "        max_remove = max(1, min(20, d-1))\n",
        "    ms = list(range(1, max_remove+1))\n",
        "\n",
        "    # We’ll reuse these to avoid re-standardizing inside the loop.\n",
        "    # full_Xs: standardized (train+val), full_y: labels (train+val)\n",
        "    # scaler.transform(X_test): standardized test\n",
        "    results = {}  # name -> list of accuracies\n",
        "\n",
        "    for name in rankers:\n",
        "        s = _get_series(name)\n",
        "        if s is None:\n",
        "            print(f\"[Exp2] Skipping {name}: ranking not available.\")\n",
        "            continue\n",
        "\n",
        "        accs = []\n",
        "        for m in ms:\n",
        "            # features to drop: top-m by the chosen ranking\n",
        "            drop = set(s.head(m).index.tolist())\n",
        "            keep = [c for c in X_df.columns if c not in drop]\n",
        "            if len(keep) == 0:\n",
        "                # no features left; append NaN and continue\n",
        "                accs.append(np.nan)\n",
        "                continue\n",
        "\n",
        "            # column indices for the kept features\n",
        "            idxs = [list(X_df.columns).index(c) for c in keep]\n",
        "\n",
        "            # subset pre-standardized arrays by column index\n",
        "            X_full_sub = full_Xs[:, idxs]                              # (train+val)\n",
        "            X_test_sub = scaler.transform(X_test)[:, idxs]             # test\n",
        "\n",
        "            # train and evaluate\n",
        "            mdl = Model(random_state=RANDOM_SEED).fit(X_full_sub, full_y)\n",
        "            ypred = mdl.predict(X_test_sub)\n",
        "            accs.append(accuracy_score(y_test, ypred) * 100.0)\n",
        "\n",
        "        results[name] = accs\n",
        "\n",
        "    # ---- Plot curves ----\n",
        "    plt.figure(figsize=(7.5, 5.0))\n",
        "    for name, accs in results.items():\n",
        "        plt.plot(ms, accs, marker='o', label=name)\n",
        "    plt.xlabel(\"Number of top features removed (m)\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"Experiment 2 (fixed): Necessity curves\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out, dpi=150)\n",
        "    print(f\"[Exp2] Saved {out}\")\n",
        "\n",
        "# Run the fixed Exp2\n",
        "exp2_necessity_curves_fixed()\n"
      ],
      "metadata": {
        "id": "v6MrZfAAvsGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Experiment 3: Noise Robustness of ExCIR Rank\n",
        "# ===========================================\n",
        "def exp3_noise_robustness():\n",
        "    # Re-compute ExCIR on noisy val features; measure Spearman & top-k overlap\n",
        "    from scipy.stats import spearmanr\n",
        "    spears, overlaps = [], []\n",
        "    base_rank = cir_val\n",
        "    base_top = set(base_rank.head(10).index)\n",
        "    for _ in range(NOISE_TRIALS):\n",
        "        noise = rng.normal(0, NOISE_SIGMA, X_val_s.shape)\n",
        "        Xv_noisy = X_val_s + noise\n",
        "        yhat_noisy = model_outputs(orig_model, Xv_noisy)\n",
        "        cir_noisy = compute_cir(Xv_noisy, yhat_noisy, X_val.columns.tolist())\n",
        "        r, _ = spearmanr(base_rank.rank(), cir_noisy[base_rank.index].rank())\n",
        "        spears.append(r)\n",
        "        overlaps.append(len(base_top & set(cir_noisy.head(10).index))/10.0)\n",
        "    # Plot histograms\n",
        "    fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "    ax[0].hist(spears, bins=20, edgecolor='k')\n",
        "    ax[0].set_title(\"Spearman rank corr. (ExCIR under noise)\")\n",
        "    ax[0].set_xlabel(\"Spearman\"); ax[0].set_ylabel(\"Count\")\n",
        "    ax[1].hist(overlaps, bins=11, range=(0,1), edgecolor='k')\n",
        "    ax[1].set_title(\"Top-10 overlap (ExCIR under noise)\")\n",
        "    ax[1].set_xlabel(\"Overlap fraction\"); ax[1].set_ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    out = os.path.join(SAVE_DIR, \"exp3_noise_robustness.png\")\n",
        "    plt.savefig(out, dpi=150)\n",
        "    print(f\"[Exp3] Saved {out}\")\n",
        "    print(f\"[Exp3] Median Spearman={np.median(spears):.3f}, 5–95%: ({np.percentile(spears,5):.3f},{np.percentile(spears,95):.3f})\")\n",
        "    print(f\"[Exp3] Median Top-10 overlap={np.median(overlaps):.2%}, 5–95%: ({np.percentile(overlaps,5):.2%},{np.percentile(overlaps,95):.2%})\")\n",
        "    print(f\"[Exp3] Saved {out}\")\n",
        "exp3_noise_robustness()\n",
        "\n",
        "# ======================================================\n",
        "# Experiment 4: Correlation Stress Test (tire correlation)\n",
        "# ======================================================\n",
        "def exp4_correlation_sweep():\n",
        "    from scipy.stats import spearmanr\n",
        "    rows = []\n",
        "    for rho in CORR_LEVELS:\n",
        "        X_df_r, y_r, _ = make_vehicle_dataset(n=N_SAMPLES, random_state=RANDOM_SEED, tire_corr=rho)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X_df_r, y_r, test_size=TEST_SIZE, stratify=y_r, random_state=RANDOM_SEED)\n",
        "        Xtr_in, Xval_in, ytr_in, yval_in = train_test_split(Xtr, ytr, test_size=VAL_SIZE, stratify=ytr, random_state=RANDOM_SEED)\n",
        "        sc = StandardScaler().fit(Xtr_in.values)\n",
        "        Xtr_s, Xval_s = sc.transform(Xtr_in.values), sc.transform(Xval_in.values)\n",
        "        model = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(np.vstack([Xtr_s, Xval_s]), np.concatenate([ytr_in, yval_in]))\n",
        "        yhat_val = model.predict_proba(Xval_s)[:,1]\n",
        "        cir = compute_cir(Xval_s, yhat_val, Xval_in.columns.tolist())\n",
        "        shap_s = shap_mean_abs(model, Xval_s, Xval_in.columns.tolist(), max_rows=VAL_SHAP_MAX)\n",
        "        lime_s = lime_global_ranking(model, sc.transform(Xtr_in.values), Xval_s, Xval_in.columns.tolist(), max_rows=min(400,len(Xval_in)))\n",
        "        # pairwise Spearman among methods\n",
        "        r_ES, _ = spearmanr(cir.rank(), shap_s[cir.index].rank())\n",
        "        r_EL, _ = spearmanr(cir.rank(), lime_s[cir.index].rank())\n",
        "        r_SL, _ = spearmanr(shap_s.rank(), lime_s[shap_s.index].rank())\n",
        "        # sufficiency at k=6\n",
        "        def acc_for(rank, k=6):\n",
        "            cols = rank.head(k).index.tolist()\n",
        "            a, _, _ = train_eval_model(GradientBoostingClassifier, Xtr, ytr, Xte, yte, cols)\n",
        "            return a*100\n",
        "        rows.append([rho, r_ES, r_EL, r_SL, acc_for(cir,6), acc_for(shap_s,6), acc_for(lime_s,6)])\n",
        "    df = pd.DataFrame(rows, columns=[\"tire_corr\",\"Spearman(ExCIR,SHAP)\",\"Spearman(ExCIR,LIME)\",\"Spearman(SHAP,LIME)\",\"Acc k6 ExCIR\",\"Acc k6 SHAP\",\"Acc k6 LIME\"])\n",
        "    print(\"\\n[Exp4] Correlation sweep summary:\")\n",
        "    print(df.round(3).to_string(index=False))\n",
        "    # Plot Spearman vs rho\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(df[\"tire_corr\"], df[\"Spearman(ExCIR,SHAP)\"], marker='o', label=\"ExCIR–SHAP\")\n",
        "    plt.plot(df[\"tire_corr\"], df[\"Spearman(ExCIR,LIME)\"], marker='o', label=\"ExCIR–LIME\")\n",
        "    plt.plot(df[\"tire_corr\"], df[\"Spearman(SHAP,LIME)\"], marker='o', label=\"SHAP–LIME\")\n",
        "    plt.xlabel(\"Tire correlation (ρ)\"); plt.ylabel(\"Spearman rank correlation\")\n",
        "    plt.title(\"Experiment 4: Rank agreement vs correlation\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout()\n",
        "    out = os.path.join(SAVE_DIR, \"exp4_correlation_spearman.png\")\n",
        "    plt.savefig(out, dpi=150); print(f\"[Exp4] Saved {out}\")\n",
        "    print(f\"[Exp4] Saved {out}\")\n",
        "exp4_correlation_sweep()\n"
      ],
      "metadata": {
        "id": "V_RTgPurv2Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================================================\n",
        "# Experiment 5: Lightweight environment sweep (agreement/cost)\n",
        "# =========================================================\n",
        "def exp5_lightweight_sweep():\n",
        "    from scipy.stats import spearmanr\n",
        "    # full baseline rankings on val\n",
        "    base_cir = cir_val\n",
        "    base_top = set(base_cir.head(8).index)\n",
        "    # build full (train+val) pool standardized\n",
        "    sc_all = StandardScaler().fit(np.vstack([X_train.values, X_val.values]))\n",
        "    full_Xs = sc_all.transform(pd.concat([X_train, X_val], axis=0).values)\n",
        "    full_y  = np.concatenate([y_train, y_val])\n",
        "    pool = pd.DataFrame(full_Xs, columns=X_train.columns).assign(target=full_y)\n",
        "\n",
        "    rows = []\n",
        "    for frac in LW_FRACTIONS:\n",
        "        t0 = time.perf_counter()\n",
        "        # class-stratified subsample\n",
        "        light_df = pool.groupby(\"target\", group_keys=False).apply(\n",
        "            lambda g: g.sample(max(1, int(len(g)*frac)), random_state=RANDOM_SEED)\n",
        "        ).reset_index(drop=True)\n",
        "        Xl = light_df[X_train.columns].values\n",
        "        yl = light_df[\"target\"].values\n",
        "        m = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(Xl, yl)\n",
        "        # compute CIR on same validation split (standardized by sc_all)\n",
        "        yhat_light = m.predict_proba(sc_all.transform(X_val.values))[:,1]\n",
        "        cir_light  = compute_cir(sc_all.transform(X_val.values), yhat_light, X_val.columns.tolist())\n",
        "        # agreement metrics\n",
        "        r, _ = spearmanr(base_cir.rank(), cir_light[base_cir.index].rank())\n",
        "        overlap8 = len(base_top & set(cir_light.head(8).index))/8.0\n",
        "        runtime = time.perf_counter() - t0\n",
        "        # simple risk gap using accuracy on shared test (for illustration)\n",
        "        # train full vs light on pool/test; compare accuracy difference\n",
        "        mf = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(full_Xs, full_y)\n",
        "        yprob_full  = mf.predict_proba(sc_all.transform(X_test.values))[:,1]\n",
        "        yprob_light = m.predict_proba(sc_all.transform(X_test.values))[:,1]\n",
        "        acc_gap = accuracy_score(y_test, (yprob_light>=0.5)) - accuracy_score(y_test, (yprob_full>=0.5))\n",
        "        rows.append([frac, r, overlap8, runtime, acc_gap])\n",
        "    df = pd.DataFrame(rows, columns=[\"fraction\",\"Spearman\",\"Top-8 overlap\",\"runtime_s\",\"acc_gap\"])\n",
        "    print(\"\\n[Exp5] Lightweight sweep:\")\n",
        "    print(df.round(3).to_string(index=False))\n",
        "\n",
        "    fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
        "    ax[0].plot(df[\"fraction\"], df[\"Spearman\"], marker='o'); ax[0].set_title(\"Rank agreement (Spearman)\")\n",
        "    ax[0].set_xlabel(\"Fraction\"); ax[0].set_ylabel(\"ρ\")\n",
        "    ax[1].plot(df[\"fraction\"], df[\"Top-8 overlap\"], marker='o'); ax[1].set_title(\"Top-8 overlap\")\n",
        "    ax[1].set_xlabel(\"Fraction\"); ax[1].set_ylabel(\"Overlap\")\n",
        "    ax[2].plot(df[\"fraction\"], df[\"runtime_s\"], marker='o'); ax[2].set_title(\"Runtime (s)\")\n",
        "    ax[2].set_xlabel(\"Fraction\"); ax[2].set_ylabel(\"Seconds\")\n",
        "    plt.tight_layout()\n",
        "    out = os.path.join(SAVE_DIR, \"exp5_lightweight_sweep.png\")\n",
        "    plt.savefig(out, dpi=150); print(f\"[Exp5] Saved {out}\")\n",
        "exp5_lightweight_sweep()\n",
        "\n",
        "# ======================================================\n",
        "# Experiment 6: Cross-model consistency (GBM vs RF)\n",
        "# ======================================================\n",
        "def exp6_cross_model():\n",
        "    # Train RF on same pool; compare ExCIR rankings on val\n",
        "    rf = RandomForestClassifier(n_estimators=400, max_depth=None, random_state=RANDOM_SEED, n_jobs=-1)\n",
        "    rf.fit(np.vstack([X_train_s, X_val_s]), np.concatenate([y_train, y_val]))\n",
        "    yhat_val_rf = rf.predict_proba(X_val_s)[:,1]\n",
        "    cir_rf = compute_cir(X_val_s, yhat_val_rf, X_val.columns.tolist())\n",
        "\n",
        "    from scipy.stats import spearmanr\n",
        "    r = spearmanr(cir_val.rank(), cir_rf[cir_val.index].rank())[0]\n",
        "    top8_overlap = len(set(cir_val.head(8).index) & set(cir_rf.head(8).index))/8.0\n",
        "    print(f\"\\n[Exp6] ExCIR agreement across models (GBM vs RF): Spearman={r:.3f}, Top-8 overlap={top8_overlap:.2%}\")\n",
        "\n",
        "    # Side-by-side bar for top 15 by GBM ExCIR\n",
        "    order = cir_val.head(15).index\n",
        "    vals_gbm = cir_val.loc[order].values\n",
        "    vals_rf  = cir_rf.loc[order].values\n",
        "    ypos = np.arange(len(order))\n",
        "    plt.figure(figsize=(8,5))\n",
        "    bar_h = 0.35\n",
        "    plt.barh(ypos-bar_h/2, vals_gbm, height=bar_h, label=\"GBM\")\n",
        "    plt.barh(ypos+bar_h/2, vals_rf,  height=bar_h, label=\"RF\")\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.yticks(ypos, order)\n",
        "    plt.xlabel(\"CIR score\"); plt.title(\"Experiment 6: ExCIR feature scores (GBM vs RF)\"); plt.legend()\n",
        "    plt.tight_layout()\n",
        "    out = os.path.join(SAVE_DIR, \"exp6_cross_model_excir.png\")\n",
        "    plt.savefig(out, dpi=150)\n",
        "    print(f\"[Exp6] Saved {out}\")\n",
        "exp6_cross_model()\n",
        "\n",
        "# ======================================================\n",
        "# Small summary table at k = {6,8} with bootstrap CIs\n",
        "# ======================================================\n",
        "def summarize_topk_with_ci():\n",
        "    methods = {\"ExCIR\": cir_val, \"SHAP\": shap_val, \"LIME\": lime_val}\n",
        "    rows = []\n",
        "    for k in TOP_K_SMALL:\n",
        "        for name, rank in methods.items():\n",
        "            cols = rank.head(k).index.tolist()\n",
        "            mean, lo, hi = boot_acc(GradientBoostingClassifier, X_train_full, y_train_full, X_test, y_test, cols, B=200)\n",
        "            rows.append([name, k, mean, lo, hi])\n",
        "    df = pd.DataFrame(rows, columns=[\"Method\",\"k\",\"Acc(%)\",\"95% CI low\",\"95% CI high\"])\n",
        "    print(\"\\n[Summary] Top-k sufficiency with 95% bootstrap CIs:\")\n",
        "    print(df.round(2).to_string(index=False))\n",
        "    df.to_csv(os.path.join(SAVE_DIR, \"summary_topk_ci.csv\"), index=False)\n",
        "    print(f\"Saved CSV to {os.path.join(SAVE_DIR, 'summary_topk_ci.csv')}\")\n",
        "summarize_topk_with_ci()\n"
      ],
      "metadata": {
        "id": "vDuxuj6Wv6s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Experiment 7: Calibration & Threshold Stability\n",
        "# =============================\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc, brier_score_loss\n",
        "\n",
        "def exp7_calibration_thresholds():\n",
        "    # Use original model on the standardized test split\n",
        "    yprob = orig_model.predict_proba(X_test_s)[:,1]\n",
        "    yhat  = (yprob >= 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    fpr, tpr, _ = roc_curve(y_test, yprob); roc_auc = auc(fpr, tpr)\n",
        "    prec, rec, thr_pr = precision_recall_curve(y_test, yprob)\n",
        "    pr_auc = auc(rec, prec)\n",
        "    brier = brier_score_loss(y_test, yprob)\n",
        "\n",
        "    print(f\"\\n[Exp7] ROC AUC={roc_auc:.3f} | PR AUC={pr_auc:.3f} | Brier={brier:.3f}\")\n",
        "\n",
        "    # Calibration curve\n",
        "    frac_pos, mean_pred = calibration_curve(y_test, yprob, n_bins=10, strategy='quantile')\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(mean_pred, frac_pos, marker='o', label='Model')\n",
        "    plt.plot([0,1],[0,1],'--',color='gray',label='Perfect')\n",
        "    plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Fraction positive\")\n",
        "    plt.title(\"Experiment 7: Calibration curve\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(\"exp7_calibration_curve.png\", dpi=150)\n",
        "    print(\"[Exp7] Saved exp7_calibration_curve.png\")\n",
        "\n",
        "    # Threshold stability: accuracy vs threshold\n",
        "    thresholds = np.linspace(0.1, 0.9, 17)\n",
        "    accs = []\n",
        "    for t in thresholds:\n",
        "        yhat_t = (yprob >= t).astype(int)\n",
        "        accs.append(accuracy_score(y_test, yhat_t)*100)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(thresholds, accs, marker='o')\n",
        "    plt.xlabel(\"Decision threshold\"); plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"Experiment 7: Threshold stability (accuracy)\")\n",
        "    plt.grid(True, alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(\"exp7_threshold_stability.png\", dpi=150)\n",
        "    print(\"[Exp7] Saved exp7_threshold_stability.png\")\n",
        "\n",
        "exp7_calibration_thresholds()\n"
      ],
      "metadata": {
        "id": "k9VF8Jepv-O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Experiment 8: Temporal/Distribution Drift\n",
        "# =========================================\n",
        "def exp8_drift_analysis():\n",
        "    # Create a drifted copy of TEST data (unscaled), then standardize with the SAME scaler\n",
        "    X_drift = X_test.copy()\n",
        "    X_drift[\"speed_kph\"] += 15\n",
        "    X_drift[\"brake\"] = np.clip(X_drift[\"brake\"] + 0.2, 0, 1)\n",
        "    for tcol in [\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]:\n",
        "        X_drift[tcol] -= 2.0\n",
        "    X_drift[\"road_grade\"] += 0.5\n",
        "\n",
        "    X_drift_s = scaler.transform(X_drift.values)\n",
        "    yprob_drift = orig_model.predict_proba(X_drift_s)[:,1]\n",
        "    yhat_drift  = (yprob_drift >= 0.5).astype(int)\n",
        "\n",
        "    # Performance change\n",
        "    yprob_base = orig_model.predict_proba(X_test_s)[:,1]\n",
        "    acc_base = accuracy_score(y_test, (yprob_base>=0.5))\n",
        "    acc_drift = accuracy_score(y_test, yhat_drift)\n",
        "    print(f\"\\n[Exp8] Accuracy base={acc_base*100:.2f}% | drift={acc_drift*100:.2f}% | delta={(acc_drift-acc_base)*100:.2f}%\")\n",
        "\n",
        "    # CIR on drifted set vs base (use val-based feature scaling for comparability)\n",
        "    # We recompute ExCIR on the drifted TEST set (standardized the same way).\n",
        "    cir_base_test  = compute_cir(X_test_s,  yprob_base,  X_test.columns.tolist())\n",
        "    cir_drift_test = compute_cir(X_drift_s, yprob_drift, X_test.columns.tolist())\n",
        "\n",
        "    # Rank correlation & top-10 overlap\n",
        "    from scipy.stats import spearmanr\n",
        "    r,_ = spearmanr(cir_base_test.rank(), cir_drift_test[cir_base_test.index].rank())\n",
        "    overlap10 = len(set(cir_base_test.head(10).index) & set(cir_drift_test.head(10).index))/10.0\n",
        "    print(f\"[Exp8] ExCIR rank agreement under drift: Spearman={r:.3f}, Top-10 overlap={overlap10:.2%}\")\n",
        "\n",
        "    # Plot delta on top 15 baseline features\n",
        "    order = cir_base_test.head(15).index\n",
        "    delta = cir_drift_test.loc[order].values - cir_base_test.loc[order].values\n",
        "    plt.figure(figsize=(8,5))\n",
        "    ypos = np.arange(len(order))\n",
        "    plt.barh(ypos, delta)\n",
        "    plt.yticks(ypos, order); plt.gca().invert_yaxis()\n",
        "    plt.axvline(0, color='k', linewidth=1)\n",
        "    plt.xlabel(\"Δ CIR (drift - base)\"); plt.title(\"Experiment 8: ExCIR change under drift (top-15 by base)\")\n",
        "    plt.tight_layout(); plt.savefig(\"exp8_cir_delta_drift.png\", dpi=150)\n",
        "    print(\"[Exp8] Saved exp8_cir_delta_drift.png\")\n",
        "\n",
        "exp8_drift_analysis()\n"
      ],
      "metadata": {
        "id": "3sBPfr7VwBfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# Experiment 9: Block Whitening (tire channels only)\n",
        "# ==================================================\n",
        "def exp9_block_whitening():\n",
        "    cols = list(X_val.columns)\n",
        "    tire_cols = [\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]\n",
        "    idxs = [cols.index(c) for c in tire_cols]\n",
        "\n",
        "    Xv = X_val_s.copy()\n",
        "    # PCA/ZCA whitening on the tire block\n",
        "    T = Xv[:, idxs]\n",
        "    C = np.cov(T, rowvar=False)\n",
        "    eigvals, eigvecs = np.linalg.eigh(C + 1e-8*np.eye(C.shape[0]))\n",
        "    W = eigvecs @ np.diag(1.0/np.sqrt(eigvals)) @ eigvecs.T\n",
        "    T_wh = (T - T.mean(axis=0)) @ W   # center + whiten\n",
        "    Xv_wh = Xv.copy()\n",
        "    Xv_wh[:, idxs] = T_wh\n",
        "\n",
        "    # Recompute ExCIR before/after\n",
        "    yprob = orig_model.predict_proba(X_val_s)[:,1]\n",
        "    yprob_wh = orig_model.predict_proba(Xv_wh)[:,1]\n",
        "    cir_before = compute_cir(X_val_s, yprob, cols)\n",
        "    cir_after  = compute_cir(Xv_wh, yprob_wh, cols)\n",
        "\n",
        "    # Compare top-15 by before\n",
        "    order = cir_before.head(15).index\n",
        "    vals_b = cir_before.loc[order].values\n",
        "    vals_a = cir_after.loc[order].values\n",
        "    ypos = np.arange(len(order))\n",
        "    plt.figure(figsize=(8,5))\n",
        "    bar_h=0.35\n",
        "    plt.barh(ypos-bar_h/2, vals_b, height=bar_h, label=\"Before whitening\")\n",
        "    plt.barh(ypos+bar_h/2, vals_a, height=bar_h, label=\"After whitening\")\n",
        "    plt.gca().invert_yaxis(); plt.yticks(ypos, order)\n",
        "    plt.xlabel(\"CIR\"); plt.title(\"Experiment 9: ExCIR before/after tire-block whitening\")\n",
        "    plt.legend(); plt.tight_layout(); plt.savefig(\"exp9_whitening_cir.png\", dpi=150)\n",
        "    print(\"\\n[Exp9] Saved exp9_whitening_cir.png\")\n",
        "\n",
        "    # Rank agreement on full list\n",
        "    from scipy.stats import spearmanr\n",
        "    r,_ = spearmanr(cir_before.rank(), cir_after[cir_before.index].rank())\n",
        "    print(f\"[Exp9] Spearman(before vs after whitening)={r:.3f}\")\n",
        "\n",
        "exp9_block_whitening()\n"
      ],
      "metadata": {
        "id": "qkCsTf2HwEfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# Experiment 10: Bootstrap uncertainty for ExCIR (val set)\n",
        "# =======================================================\n",
        "# =======================================================\n",
        "# Experiment 10 (fixed): Bootstrap uncertainty for ExCIR\n",
        "# =======================================================\n",
        "def exp10_cir_bootstrap(B=200, topN=12, seed=0):\n",
        "    \"\"\"\n",
        "    Bootstraps the validation set to quantify uncertainty in ExCIR scores\n",
        "    for the topN features from the base run. Saves a CSV of CIs and a PNG.\n",
        "    \"\"\"\n",
        "    # 1) Base CIR on the full validation set (global variables used: X_val_s, X_val, orig_model)\n",
        "    base = compute_cir(\n",
        "        X_val_s,\n",
        "        orig_model.predict_proba(X_val_s)[:, 1],\n",
        "        X_val.columns.tolist()\n",
        "    )\n",
        "    top_feats = base.head(topN).index.tolist()\n",
        "\n",
        "    # 2) Bootstrap resampling on validation rows\n",
        "    #    Support both RandomState (randint) and Generator (integers)\n",
        "    rng = check_random_state(seed)  # returns RandomState\n",
        "    n = X_val_s.shape[0]\n",
        "    samples = {f: [] for f in top_feats}\n",
        "\n",
        "    for _ in range(B):\n",
        "        try:\n",
        "            idx = rng.integers(0, n, size=n)  # works for Generator\n",
        "        except AttributeError:\n",
        "            idx = rng.randint(0, n, size=n)   # works for RandomState\n",
        "\n",
        "        Xb  = X_val_s[idx]\n",
        "        ypb = orig_model.predict_proba(Xb)[:, 1]\n",
        "        cir_b = compute_cir(Xb, ypb, X_val.columns.tolist())\n",
        "        for f in top_feats:\n",
        "            samples[f].append(cir_b[f])\n",
        "\n",
        "    # 3) Build CI table\n",
        "    rows, lows, meds, highs, labels = [], [], [], [], []\n",
        "    for f in top_feats:\n",
        "        arr = np.asarray(samples[f], dtype=float)\n",
        "        lo, med, hi = np.percentile(arr, [2.5, 50, 97.5])\n",
        "        rows.append([f, float(base[f]), float(lo), float(med), float(hi)])\n",
        "        labels.append(f); lows.append(lo); meds.append(med); highs.append(hi)\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"feature\",\"base_CIR\",\"boot_lo\",\"boot_med\",\"boot_hi\"])\n",
        "    df.to_csv(\"exp10_cir_bootstrap_ci.csv\", index=False)\n",
        "    print(\"[Exp10] Saved exp10_cir_bootstrap_ci.csv (CIR CIs for top features)\")\n",
        "\n",
        "    # 4) Errorbar plot (median with 95% CI)\n",
        "    idxs = np.arange(len(labels))\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    # xerr expects [lower_err, upper_err]\n",
        "    lower_err = np.array(meds) - np.array(lows)\n",
        "    upper_err = np.array(highs) - np.array(meds)\n",
        "    plt.errorbar(meds, idxs, xerr=[lower_err, upper_err], fmt='o', capsize=3)\n",
        "    plt.yticks(idxs, labels)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.xlabel(\"CIR (median with 95% CI)\")\n",
        "    plt.title(\"Experiment 10: ExCIR uncertainty (bootstrap)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"exp10_cir_bootstrap_ci.png\", dpi=150)\n",
        "    print(\"[Exp10] Saved exp10_cir_bootstrap_ci.png\")\n",
        "\n",
        "exp10_cir_bootstrap()\n"
      ],
      "metadata": {
        "id": "76InZSY8wHzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================\n",
        "# Experiment 11: Counterfactual sanity check\n",
        "# =======================================\n",
        "def exp11_counterfactual_curves(features=(\"brake\",\"speed_kph\",\"tire_rr\")):\n",
        "    base = X_val_s.copy()\n",
        "    yprob_base = orig_model.predict_proba(base)[:,1]\n",
        "    deltas = np.linspace(-1.5, 1.5, 13)  # steps in standardized units\n",
        "\n",
        "    plt.figure(figsize=(7,5))\n",
        "    for fname in features:\n",
        "        j = list(X_val.columns).index(fname)\n",
        "        avg_probs = []\n",
        "        for d in deltas:\n",
        "            Xc = base.copy()\n",
        "            Xc[:, j] = Xc[:, j] + d\n",
        "            p = orig_model.predict_proba(Xc)[:,1]\n",
        "            avg_probs.append(p.mean())\n",
        "        plt.plot(deltas, avg_probs, marker='o', label=fname)\n",
        "\n",
        "    plt.xlabel(\"Feature nudge (std units)\"); plt.ylabel(\"Average predicted risk\")\n",
        "    plt.title(\"Experiment 11: Counterfactual sanity curves\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(\"exp11_counterfactual_curves.png\", dpi=150)\n",
        "    print(\"\\n[Exp11] Saved exp11_counterfactual_curves.png\")\n",
        "\n",
        "exp11_counterfactual_curves()\n"
      ],
      "metadata": {
        "id": "XqognkrqwLfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAll experiments completed. Check the generated PNGs and CSV in:\", os.path.abspath(SAVE_DIR))"
      ],
      "metadata": {
        "id": "U3xfwi0gwOBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ExCIR Benchmarks: 12 Experiments vs. SHAP / LIME (and more)\n",
        "# ============================================================\n",
        "# This file assumes you may already have:\n",
        "# - Synthetic vehicular dataset (X_df, y, splits, scaler, Model, orig_model, full_Xs, full_y, etc.)\n",
        "# - compute_cir(Xs, yhat, names)\n",
        "# If not, it will create everything it needs.\n",
        "\n",
        "import os, time, math, warnings\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.utils import check_random_state\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "from dataclasses import dataclass\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Optional deps\n",
        "try:\n",
        "    import shap\n",
        "except Exception:\n",
        "    shap = None\n",
        "\n",
        "try:\n",
        "    from lime.lime_tabular import LimeTabularExplainer\n",
        "except Exception:\n",
        "    LimeTabularExplainer = None\n",
        "\n",
        "try:\n",
        "    import psutil, os as _os\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "rng = check_random_state(RANDOM_SEED)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Section 0 — Ensure data, model, helpers exist\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def ensure_synthetic_vehicular():\n",
        "    \"\"\"Create the synthetic vehicular dataset + splits + model + SHAP/LIME rankings if not present.\"\"\"\n",
        "    globals_ns = globals()\n",
        "    need = any(k not in globals_ns for k in\n",
        "               [\"X_df\",\"y\",\"X_train\",\"X_val\",\"X_test\",\"y_train\",\"y_val\",\"y_test\",\n",
        "                \"full_Xs\",\"full_y\",\"scaler\",\"Model\",\"orig_model\",\"cir_orig_val\",\n",
        "                \"shap_series\",\"lime_series\"])\n",
        "\n",
        "    if not need:\n",
        "        return\n",
        "\n",
        "    # ----- Generate dataset (same structure as your earlier notebook) -----\n",
        "    N_SAMPLES   = 6000\n",
        "    TEST_SIZE   = 0.20\n",
        "    VAL_SIZE    = 0.20\n",
        "\n",
        "    feat_names = [\n",
        "        \"speed_kph\",\"rpm\",\"throttle\",\"brake\",\"steering_deg\",\"gear\",\n",
        "        \"accel_long\",\"accel_lat\",\"yaw_rate\",\"road_grade\",\n",
        "        \"ambient_temp\",\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\",\n",
        "        \"engine_load\",\"maf\",\"intake_air_temp\",\"battery_v\",\"fuel_rate\"\n",
        "    ]\n",
        "    n = N_SAMPLES\n",
        "    speed = np.clip(rng.normal(80, 15, n), 0, None)\n",
        "    throttle = np.clip(rng.beta(2, 2, n), 0, 1)\n",
        "    brake = np.clip(1 - throttle + rng.normal(0, 0.15, n), 0, 1)\n",
        "    steering = rng.normal(0, 10, n)\n",
        "    gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, n), 1, 7)\n",
        "    accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, n)\n",
        "    accel_lat = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, n)\n",
        "    yaw_rate = rng.normal(steering/30 * (speed/60), 0.2, n)\n",
        "    road_grade = rng.normal(0, 2, n)\n",
        "    ambient_temp = rng.normal(20, 8, n)\n",
        "    tire_base = rng.normal(34, 1.0, (n,4))\n",
        "    low_mask = rng.uniform(0,1,n) < 0.15\n",
        "    tire_drop = rng.normal(4, 1.0, (n,4)) * low_mask[:,None]\n",
        "    tires = tire_base - tire_drop\n",
        "    engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, n), 0, 100)\n",
        "    maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,n), 0, None)\n",
        "    intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, n), -10, 80)\n",
        "    battery_v = np.clip(rng.normal(13.8, 0.3, n) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "    fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,n), 0, None)\n",
        "    rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, n), 700, 7000)\n",
        "\n",
        "    X_df = pd.DataFrame({\n",
        "        \"speed_kph\": speed,\n",
        "        \"rpm\": rpm,\n",
        "        \"throttle\": throttle,\n",
        "        \"brake\": brake,\n",
        "        \"steering_deg\": steering,\n",
        "        \"gear\": gear,\n",
        "        \"accel_long\": accel_long,\n",
        "        \"accel_lat\": accel_lat,\n",
        "        \"yaw_rate\": yaw_rate,\n",
        "        \"road_grade\": road_grade,\n",
        "        \"ambient_temp\": ambient_temp,\n",
        "        \"tire_fl\": tires[:,0],\n",
        "        \"tire_fr\": tires[:,1],\n",
        "        \"tire_rl\": tires[:,2],\n",
        "        \"tire_rr\": tires[:,3],\n",
        "        \"engine_load\": engine_load,\n",
        "        \"maf\": maf,\n",
        "        \"intake_air_temp\": intake_air_temp,\n",
        "        \"battery_v\": battery_v,\n",
        "        \"fuel_rate\": fuel_rate,\n",
        "    })\n",
        "    low_tire = (32 - X_df[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "    risk_logit = (\n",
        "        1.2*(X_df[\"speed_kph\"]-110)/20\n",
        "        + 1.1*X_df[\"brake\"]\n",
        "        + 0.9*np.abs(X_df[\"steering_deg\"])/15\n",
        "        + 0.7*np.abs(X_df[\"yaw_rate\"])\n",
        "        + 0.8*(low_tire)\n",
        "        + 0.7*(X_df[\"engine_load\"]/100)\n",
        "        + 0.3*(X_df[\"road_grade\"]/5)\n",
        "        - 0.2*(X_df[\"battery_v\"]-13.5)\n",
        "    )\n",
        "    p = 1/(1+np.exp(-(risk_logit + rng.normal(0, 0.4, n))))\n",
        "    y = (rng.uniform(0,1,n) < p).astype(int)\n",
        "\n",
        "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "        X_df, y, test_size=0.20, stratify=y, random_state=RANDOM_SEED\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_full, y_train_full, test_size=0.20, stratify=y_train_full, random_state=RANDOM_SEED\n",
        "    )\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_val_s   = scaler.transform(X_val)\n",
        "    X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "    Model = GradientBoostingClassifier\n",
        "    orig_model = Model(random_state=RANDOM_SEED).fit(\n",
        "        scaler.fit_transform(pd.concat([X_train, X_val], axis=0)),\n",
        "        np.concatenate([y_train, y_val], axis=0)\n",
        "    )\n",
        "\n",
        "    def compute_cir(Xs, yhat, names):\n",
        "        n = Xs.shape[0]\n",
        "        y_bar = float(np.mean(yhat))\n",
        "        vals = []\n",
        "        for i in range(Xs.shape[1]):\n",
        "            f = Xs[:, i]\n",
        "            f_bar = float(np.mean(f))\n",
        "            m = 0.5*(f_bar + y_bar)\n",
        "            num = n*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "            den = float(np.sum((f - m)**2) + np.sum((yhat - m)**2))\n",
        "            eta = float(num/den) if den > 0 else 0.0\n",
        "            vals.append(eta)\n",
        "        return pd.Series(vals, index=names).sort_values(ascending=False)\n",
        "\n",
        "    yhat_val_orig = orig_model.predict_proba(scaler.transform(X_val))[:,1]\n",
        "    cir_orig_val = compute_cir(scaler.transform(X_val), yhat_val_orig, X_df.columns.tolist())\n",
        "\n",
        "    # SHAP global ranking\n",
        "    if shap is not None:\n",
        "        try:\n",
        "            explainer = shap.TreeExplainer(orig_model)\n",
        "            idx = rng.choice(scaler.transform(X_val).shape[0], size=min(800, X_val.shape[0]), replace=False)\n",
        "            sv = explainer.shap_values(scaler.transform(X_val).take(idx, axis=0))\n",
        "            if isinstance(sv, list): sv = sv[-1]\n",
        "            shap_series = pd.Series(np.abs(sv).mean(0), index=X_df.columns).sort_values(ascending=False)\n",
        "        except Exception:\n",
        "            shap_series = cir_orig_val * 0.0\n",
        "    else:\n",
        "        shap_series = cir_orig_val * 0.0\n",
        "\n",
        "    # LIME global ranking\n",
        "    def lime_global_ranking():\n",
        "        if LimeTabularExplainer is None: return cir_orig_val*0.0\n",
        "        X_train_s_local = scaler.transform(X_train)\n",
        "        expl = LimeTabularExplainer(X_train_s_local, feature_names=X_df.columns.tolist(),\n",
        "                                    class_names=[\"safe\",\"unsafe\"], discretize_continuous=True,\n",
        "                                    mode=\"classification\", random_state=RANDOM_SEED)\n",
        "        Xv = scaler.transform(X_val)\n",
        "        preds = lambda Z: orig_model.predict_proba(Z)\n",
        "        sample = min(400, Xv.shape[0])\n",
        "        rows = rng.choice(Xv.shape[0], size=sample, replace=False)\n",
        "        agg = pd.Series(0.0, index=X_df.columns)\n",
        "        cnt = pd.Series(0, index=X_df.columns, dtype=int)\n",
        "        for r in rows:\n",
        "            exp = expl.explain_instance(Xv[r], preds, num_features=min(10, Xv.shape[1]))\n",
        "            for name, w in exp.as_list():\n",
        "                if name in agg.index:\n",
        "                    agg[name] += abs(w); cnt[name] += 1\n",
        "        cnt[cnt==0] = 1\n",
        "        return (agg/cnt).sort_values(ascending=False)\n",
        "\n",
        "    lime_series = lime_global_ranking()\n",
        "\n",
        "    # Export to globals for downstream experiments\n",
        "    for k, v in dict(\n",
        "        X_df=X_df, y=y, X_train=X_train, X_val=X_val, X_test=X_test,\n",
        "        y_train=y_train, y_val=y_val, y_test=y_test,\n",
        "        scaler=skaler_guard(scaler), Model=Model, orig_model=orig_model,\n",
        "        full_Xs=scaler.fit_transform(pd.concat([X_train, X_val], axis=0)),\n",
        "        full_y=np.concatenate([y_train, y_val], axis=0),\n",
        "        compute_cir=compute_cir, cir_orig_val=cir_orig_val,\n",
        "        shap_series=shap_series, lime_series=lime_series\n",
        "    ).items():\n",
        "        globals()[k] = v\n",
        "\n",
        "def skaler_guard(s):\n",
        "    # small guard for pickling/closures; no-op\n",
        "    return s\n",
        "\n",
        "ensure_synthetic_vehicular()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Helpers\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def model_outputs(model, Xs):\n",
        "    return model.predict_proba(Xs)[:, -1]\n",
        "\n",
        "def train_eval_on_features(cols, model_ctor=GradientBoostingClassifier):\n",
        "    idxs = [list(X_df.columns).index(c) for c in cols]\n",
        "    X_full_sub = full_Xs[:, idxs]\n",
        "    X_test_sub = scaler.transform(X_test)[:, idxs]\n",
        "    m = model_ctor(random_state=RANDOM_SEED).fit(X_full_sub, full_y)\n",
        "    ypred = m.predict(X_test_sub)\n",
        "    return accuracy_score(y_test, ypred) * 100.0\n",
        "\n",
        "def topk_bootstrap_ci(rank_series, k, B=400, seed=0, model_ctor=GradientBoostingClassifier):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    feats = rank_series.head(k).index.tolist()\n",
        "    idxs = [list(X_df.columns).index(c) for c in feats]\n",
        "\n",
        "    X_full_sub = full_Xs[:, idxs]\n",
        "    X_test_sub = scaler.transform(X_test)[:, idxs]\n",
        "    m = model_ctor(random_state=RANDOM_SEED).fit(X_full_sub, full_y)\n",
        "    ypred = m.predict(X_test_sub)\n",
        "\n",
        "    yt = np.asarray(y_test)\n",
        "    yp = np.asarray(ypred)\n",
        "\n",
        "    base = accuracy_score(yt, yp)\n",
        "    n = yt.shape[0]\n",
        "    boots=[]\n",
        "    for _ in range(B):\n",
        "        b = rng.integers(0, n, n)\n",
        "        boots.append(accuracy_score(yt[b], yp[b]))\n",
        "    lo, hi = np.percentile(boots, [2.5, 97.5])\n",
        "    return base*100, lo*100, hi*100\n",
        "\n",
        "\n",
        "def safe_plot_save(path):\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150)\n",
        "    print(f\"Saved {path}\")\n",
        "\n",
        "# =========================\n",
        "# Extra global baselines\n",
        "# =========================\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import partial_dependence\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# ---- 0) small helpers ----\n",
        "def _stdize(X):  # just to ensure numeric stability in HSIC/PDP\n",
        "    m = X.mean(axis=0, keepdims=True); s = X.std(axis=0, keepdims=True) + 1e-12\n",
        "    return (X - m) / s\n",
        "\n",
        "def _downsample_idx(n, cap=3000, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    return np.arange(n) if n <= cap else rng.choice(n, size=cap, replace=False)\n",
        "\n",
        "# ---- 1) PFI (Permutation Feature Importance) ----\n",
        "def compute_pfi(model, X_test_s, y_test, feat_names, n_repeats=5, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    base = accuracy_score(y_test, model.predict(X_test_s))\n",
        "    scores = []\n",
        "    Xw = X_test_s.copy()\n",
        "    for j in range(Xw.shape[1]):\n",
        "        drops = []\n",
        "        col = Xw[:, j].copy()\n",
        "        for _ in range(n_repeats):\n",
        "            rng.shuffle(Xw[:, j])\n",
        "            drops.append(base - accuracy_score(y_test, model.predict(Xw)))\n",
        "            Xw[:, j] = col  # restore\n",
        "        scores.append(np.mean(drops))\n",
        "    return pd.Series(scores, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "# ---- 2) Tree gain (fast) ----\n",
        "def compute_tree_gain(model, feat_names):\n",
        "    try:\n",
        "        imp = model.feature_importances_\n",
        "        return pd.Series(imp, index=feat_names).sort_values(ascending=False)\n",
        "    except Exception:\n",
        "        return pd.Series(0.0, index=feat_names)\n",
        "\n",
        "# ---- 3) PDP/ALE amplitude (variance of PD curve wrt proba) ----\n",
        "def compute_pdp_amplitude(model, X_val_s, feat_names, grid=20):\n",
        "    vals = []\n",
        "    for j, name in enumerate(feat_names):\n",
        "        try:\n",
        "            pdp = partial_dependence(model, X_val_s, [j], kind=\"average\").average[0]\n",
        "            vals.append(np.var(pdp))\n",
        "        except Exception:\n",
        "            vals.append(0.0)\n",
        "    return pd.Series(vals, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "# ---- 4) Mutual information with predicted probability ----\n",
        "def compute_mi_with_pred(model, X_val_s, feat_names):\n",
        "    yhat = model.predict_proba(X_val_s)[:,1]\n",
        "    mi = mutual_info_regression(X_val_s, yhat, random_state=0)\n",
        "    return pd.Series(mi, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "# ---- 5) HSIC (RBF kernel) between feature and predicted probability ----\n",
        "def compute_hsic(model, X_val_s, feat_names, cap=3000, seed=0):\n",
        "    idx = _downsample_idx(X_val_s.shape[0], cap=cap, seed=seed)\n",
        "    Xs = X_val_s[idx]\n",
        "    y = model.predict_proba(Xs)[:,1:2]  # (n,1)\n",
        "    y = _stdize(y)\n",
        "\n",
        "    def rbf(X, sigma=None):\n",
        "        # median heuristic if sigma not given\n",
        "        if sigma is None:\n",
        "            d = np.sqrt(((X[None,:,:]-X[:,None,:])**2).sum(-1))\n",
        "            sigma = np.median(d) + 1e-12\n",
        "        G = ((X[None,:,:]-X[:,None,:])**2).sum(-1)\n",
        "        return np.exp(-G/(2*sigma**2))\n",
        "\n",
        "    Ky = rbf(y)\n",
        "    Hy = np.eye(len(idx)) - np.ones((len(idx),len(idx)))/len(idx)\n",
        "    Kyc = Hy @ Ky @ Hy\n",
        "\n",
        "    out=[]\n",
        "    for j in range(Xs.shape[1]):\n",
        "        x = _stdize(Xs[:, j:j+1])\n",
        "        Kx = rbf(x)\n",
        "        Hx = Hy  # same centering\n",
        "        Kxc = Hx @ Kx @ Hx\n",
        "        # biased HSIC (enough for ranking)\n",
        "        hsic = np.trace(Kxc @ Kyc) / (len(idx)**2)\n",
        "        out.append(hsic)\n",
        "    return pd.Series(out, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "# ---- 6) SAGE (optional; will skip if package missing) ----\n",
        "def compute_sage_global(model, X_train_s, X_val_s, feat_names, seed=0, m_samples=2000):\n",
        "    try:\n",
        "        import sage  # pip install sage-importance\n",
        "        rng = np.random.default_rng(seed)\n",
        "        # Fit a conditional model for X|X_-j using a tree ensemble (built-in in SAGE)\n",
        "        imputer = sage.PermutationImputer(model, X_train_s, random_state=seed)\n",
        "        estimator = sage.MarginalEstimator(imputer, loss=\"crossentropy\")\n",
        "        idx = _downsample_idx(X_val_s.shape[0], cap=m_samples, seed=seed)\n",
        "        phi = estimator(X_val_s[idx], y=None)  # y not needed for probabilistic outputs\n",
        "        vals = np.array(phi.values)  # (d,)\n",
        "        return pd.Series(vals, index=feat_names).sort_values(ascending=False)\n",
        "    except Exception as e:\n",
        "        print(f\"[SAGE] skipped: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---- One-shot function to collect all new baselines ----\n",
        "def get_more_rankings():\n",
        "    # assumes you already have: X_df, scaler, orig_model, X_val, X_test, y_test, compute_cir, cir_orig_val, etc.\n",
        "    feat_names = X_df.columns.tolist()\n",
        "    X_val_s  = scaler.transform(X_val)\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "\n",
        "    extra = {}\n",
        "\n",
        "    # ExCIR is already available as cir_orig_val\n",
        "    extra[\"PFI\"] = compute_pfi(orig_model, X_test_s, y_test, feat_names, n_repeats=5)\n",
        "    extra[\"TreeGain\"] = compute_tree_gain(orig_model, feat_names)\n",
        "    extra[\"PDP-var\"] = compute_pdp_amplitude(orig_model, X_val_s, feat_names, grid=20)\n",
        "    extra[\"MI(pred)\"] = compute_mi_with_pred(orig_model, X_val_s, feat_names)\n",
        "    extra[\"HSIC(pred)\"] = compute_hsic(orig_model, X_val_s, feat_names, cap=3000)\n",
        "\n",
        "    sage_series = compute_sage_global(\n",
        "        orig_model,\n",
        "        scaler.transform(pd.concat([X_train, X_val], axis=0)),\n",
        "        X_val_s,\n",
        "        feat_names,\n",
        "        m_samples=2000\n",
        "    )\n",
        "    if sage_series is not None:\n",
        "        extra[\"SAGE\"] = sage_series\n",
        "\n",
        "    return extra\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mqrTc0j_wRnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT 1 — Deletion / Insertion (AOPC curves)\n",
        "# ============================================================\n",
        "# ============================================================\n",
        "# EXPERIMENT 1 — Deletion / Insertion (AOPC curves) [FIXED]\n",
        "# ============================================================\n",
        "def exp1_aopc(rankings: dict, step=1, tag=\"veh\"):\n",
        "    def curve(rank_series, mode=\"deletion\"):\n",
        "        feats = list(rank_series.index)\n",
        "        X_test_s = scaler.transform(X_test)\n",
        "        xs, accs = [], []\n",
        "        for m in range(0, len(feats)+1, step):\n",
        "            if mode == \"deletion\":\n",
        "                use = [f for f in feats if f not in feats[:m]]\n",
        "            else:  # insertion\n",
        "                use = feats[:max(1, m)]\n",
        "            if len(use) == 0:\n",
        "                continue  # skip empty set\n",
        "            idxs = [list(X_df.columns).index(c) for c in use]\n",
        "            mdl = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(full_Xs[:, idxs], full_y)\n",
        "            acc = accuracy_score(y_test, mdl.predict(X_test_s[:, idxs])) * 100\n",
        "            xs.append(m); accs.append(acc)\n",
        "        return np.array(xs), np.array(accs)\n",
        "\n",
        "    # Deletion\n",
        "    plt.figure(figsize=(7,5))\n",
        "    aucs_del={}\n",
        "    for name, s in rankings.items():\n",
        "        xs, acc = curve(s, \"deletion\")\n",
        "        if len(xs) < 2:\n",
        "            continue\n",
        "        auc = np.trapz(100-acc, x=xs)/(xs[-1]-xs[0])  # smaller is better\n",
        "        aucs_del[name]=auc\n",
        "        plt.plot(xs, acc, label=f\"{name} (AUC={auc:.1f})\")\n",
        "    plt.xlabel(\"# removed (rank order)\"); plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"AOPC Deletion\"); plt.legend()\n",
        "    safe_plot_save(f\"exp1_aopc_deletion_{tag}.png\")\n",
        "\n",
        "    # Insertion\n",
        "    plt.figure(figsize=(7,5))\n",
        "    aucs_ins={}\n",
        "    for name, s in rankings.items():\n",
        "        xs, acc = curve(s, \"insertion\")\n",
        "        if len(xs) < 2:\n",
        "            continue\n",
        "        auc = np.trapz(acc, x=xs)/(xs[-1]-xs[0])      # larger is better\n",
        "        aucs_ins[name]=auc\n",
        "        plt.plot(xs, acc, label=f\"{name} (AUC={auc:.1f})\")\n",
        "    plt.xlabel(\"# added (rank order)\"); plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"AOPC Insertion\"); plt.legend()\n",
        "    safe_plot_save(f\"exp1_aopc_insertion_{tag}.png\")\n",
        "\n",
        "    pd.DataFrame({\"deletion_auc_smaller_better\":aucs_del,\n",
        "                  \"insertion_auc_larger_better\":aucs_ins}).to_csv(f\"exp1_aopc_aucs_{tag}.csv\")\n",
        "    print(\"[Exp1] Saved exp1_aopc_deletion_*.png, exp1_aopc_insertion_*.png and AUC CSV.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "upicC5S8wUIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exp2_randomization(rank_func, tag=\"veh\"):\n",
        "    base = rank_func(orig_model)\n",
        "    recs = []\n",
        "\n",
        "    # Label randomization\n",
        "    y_perm = np.random.permutation(full_y)\n",
        "    mdl_lbl = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(full_Xs, y_perm)\n",
        "    s_lbl = rank_func(mdl_lbl)\n",
        "    rho_lbl, _ = spearmanr(base.rank(), s_lbl[base.index].rank())\n",
        "    recs.append([\"label_shuffle\", rho_lbl])\n",
        "\n",
        "    # Weight randomization (random init vs trained)\n",
        "    mdl_w = GradientBoostingClassifier(random_state=None).fit(full_Xs, full_y)\n",
        "    s_w = rank_func(mdl_w)\n",
        "    rho_w, _ = spearmanr(base.rank(), s_w[base.index].rank())\n",
        "    recs.append([\"weight_random\", rho_w])\n",
        "\n",
        "    df = pd.DataFrame(recs, columns=[\"perturbation\",\"spearman_vs_base\"])\n",
        "    df.to_csv(f\"exp2_randomization_{tag}.csv\", index=False)\n",
        "    print(\"Exp2 CSV:\\n\", df)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.bar(df[\"perturbation\"], df[\"spearman_vs_base\"])\n",
        "    plt.ylim(-1, 1); plt.ylabel(\"Spearman vs base\"); plt.title(\"Randomization sanity check\")\n",
        "    safe_plot_save(f\"exp2_randomization_{tag}.png\")\n"
      ],
      "metadata": {
        "id": "l9EGOAYbwWpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exp3_correlated_blocks(seed=0, tag=\"blocks\"):\n",
        "    rng = check_random_state(seed)\n",
        "    n, d = 5000, 24\n",
        "    B = 3; bsize = d//B\n",
        "    W_true = np.array([1.2, 0.8, 0.4])\n",
        "    Z = rng.normal(size=(n, B))\n",
        "    X = np.zeros((n, d))\n",
        "    for b in range(B):\n",
        "        block = Z[:, [b]] @ rng.normal(1.0, 0.1, size=(1, bsize)) + rng.normal(0, 0.2, size=(n, bsize))\n",
        "        X[:, b*bsize:(b+1)*bsize] = block\n",
        "    logit = sum(W_true[b]*Z[:, b] for b in range(B)) + rng.normal(0,0.5,n)\n",
        "    p = 1/(1+np.exp(-logit)); y = (rng.uniform(0,1,n)<p).astype(int)\n",
        "\n",
        "    scl = StandardScaler().fit(X); Xs = scl.transform(X)\n",
        "    mdl = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(Xs, y)\n",
        "    yv = mdl.predict_proba(Xs)[:,1]\n",
        "\n",
        "    def _cir(Xs, yhat):\n",
        "        y_bar = float(np.mean(yhat)); n0 = Xs.shape[0]; out=[]\n",
        "        for i in range(Xs.shape[1]):\n",
        "            f = Xs[:, i]; f_bar=float(np.mean(f)); m=0.5*(f_bar+y_bar)\n",
        "            num = n0*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "            den = float(np.sum((f - m)**2) + np.sum((yhat - m)**2))\n",
        "            out.append(0.0 if den==0 else num/den)\n",
        "        return np.array(out)\n",
        "\n",
        "    cir = _cir(Xs, yv)\n",
        "    cir_block = np.array([cir[b*bsize:(b+1)*bsize].mean() for b in range(B)])\n",
        "\n",
        "    # SHAP group (optional)\n",
        "    if shap is not None:\n",
        "        expl = shap.TreeExplainer(mdl)\n",
        "        sv = expl.shap_values(Xs)\n",
        "        if isinstance(sv, list): sv = sv[-1]\n",
        "        shap_abs = np.abs(sv).mean(0)\n",
        "        shap_block = np.array([shap_abs[b*bsize:(b+1)*bsize].mean() for b in range(B)])\n",
        "    else:\n",
        "        shap_block = np.zeros(B)\n",
        "\n",
        "    res = pd.DataFrame({\n",
        "        \"block\": [f\"B{b+1}\" for b in range(B)],\n",
        "        \"true_weight\": W_true,\n",
        "        \"ExCIR\": cir_block,\n",
        "        \"SHAP\": shap_block\n",
        "    })\n",
        "    res.to_csv(f\"exp3_blocks_{tag}.csv\", index=False)\n",
        "    print(\"[Exp3] Saved exp3_blocks CSV.\")\n",
        "\n",
        "    # Plot grouped bars\n",
        "    x = np.arange(B); w = 0.28\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(x - w, res[\"true_weight\"], width=w, label=\"True\")\n",
        "    plt.bar(x,       res[\"ExCIR\"],      width=w, label=\"ExCIR\")\n",
        "    plt.bar(x + w,   res[\"SHAP\"],       width=w, label=\"SHAP\")\n",
        "    plt.xticks(x, res[\"block\"]); plt.ylabel(\"Block importance (scaled)\")\n",
        "    plt.title(\"Correlated blocks: truth vs estimates\"); plt.legend()\n",
        "    safe_plot_save(f\"exp3_blocks_{tag}.png\")\n"
      ],
      "metadata": {
        "id": "IR3Ivcvqwa9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exp4_drift_noise(knoise_list=(0.0, 0.1, 0.2, 0.3), tag=\"veh\"):\n",
        "    n = full_Xs.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    slices = np.array_split(idx, 3)\n",
        "    ranks=[]\n",
        "    for s in slices:\n",
        "        mdl = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(full_Xs[s], full_y[s])\n",
        "        yv = mdl.predict_proba(scaler.transform(X_val))[:,1]\n",
        "        r = compute_cir(scaler.transform(X_val), yv, X_df.columns.tolist())\n",
        "        ranks.append(r)\n",
        "\n",
        "    r12 = spearmanr(ranks[0].rank(), ranks[1][ranks[0].index].rank()).correlation\n",
        "    r23 = spearmanr(ranks[1].rank(), ranks[2][ranks[1].index].rank()).correlation\n",
        "    r13 = spearmanr(ranks[0].rank(), ranks[2][ranks[0].index].rank()).correlation\n",
        "    df_pairs = pd.DataFrame({\"pair\":[\"1-2\",\"2-3\",\"1-3\"], \"spearman\":[r12,r23,r13]})\n",
        "    df_pairs.to_csv(f\"exp4_slices_rank_agreement_{tag}.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.bar(df_pairs[\"pair\"], df_pairs[\"spearman\"])\n",
        "    plt.ylim(-1,1); plt.ylabel(\"Spearman\"); plt.title(\"Rank agreement across temporal slices\")\n",
        "    safe_plot_save(f\"exp4_slices_rank_agreement_{tag}.png\")\n",
        "\n",
        "    # Noise robustness\n",
        "    rec=[]\n",
        "    Xv = scaler.transform(X_val); yv = orig_model.predict_proba(Xv)[:,1]\n",
        "    base = compute_cir(Xv, yv, X_df.columns.tolist())\n",
        "    for s in knoise_list:\n",
        "        Xvn = Xv + np.random.normal(0, s, Xv.shape)\n",
        "        yvn = orig_model.predict_proba(Xvn)[:,1]\n",
        "        r = compute_cir(Xvn, yvn, X_df.columns.tolist())\n",
        "        rho = spearmanr(base.rank(), r[base.index].rank()).correlation\n",
        "        rec.append([s, rho])\n",
        "    df_noise = pd.DataFrame(rec, columns=[\"noise_sigma\",\"spearman_vs_base\"])\n",
        "    df_noise.to_csv(f\"exp4_noise_{tag}.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(df_noise[\"noise_sigma\"], df_noise[\"spearman_vs_base\"], \"-o\")\n",
        "    plt.ylim(-1,1); plt.xlabel(\"Noise sigma\"); plt.ylabel(\"Spearman vs base\")\n",
        "    plt.title(\"Noise robustness (validation)\");\n",
        "    safe_plot_save(f\"exp4_noise_{tag}.png\")\n",
        "    print(\"[Exp4] Saved drift/noise plots and CSVs.\")\n"
      ],
      "metadata": {
        "id": "hp9iY1hEwbuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exp5_multi_output(n=5000, d=40, classes=3, seed=0, tag=\"multi\"):\n",
        "    X, y = make_classification(n_samples=n, n_features=d, n_informative=d//4,\n",
        "                               n_redundant=0, n_classes=classes, random_state=seed)\n",
        "    scl = StandardScaler().fit(X); Xs = scl.transform(X)\n",
        "    mdl = GradientBoostingClassifier(random_state=seed).fit(Xs, y)\n",
        "    P = mdl.predict_proba(Xs)  # n x C\n",
        "\n",
        "    names = [f\"f{i}\" for i in range(d)]\n",
        "    def cir_univariate(Xs, yhat):\n",
        "        n0 = Xs.shape[0]; y_bar=float(np.mean(yhat)); out=[]\n",
        "        for i in range(Xs.shape[1]):\n",
        "            f = Xs[:, i]; f_bar=float(np.mean(f)); m=0.5*(f_bar+y_bar)\n",
        "            num = n0*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "            den = float(np.sum((f - m)**2) + np.sum((yhat - m)**2))\n",
        "            out.append(0.0 if den==0 else num/den)\n",
        "        return np.array(out)\n",
        "\n",
        "    cir_per_class = np.stack([cir_univariate(Xs, P[:,c]) for c in range(classes)], axis=1)  # d x C\n",
        "    cir_multi = pd.Series(cir_per_class.mean(axis=1), index=names).sort_values(ascending=False)\n",
        "    cir_multi.to_csv(f\"exp5_cir_multi_{tag}.csv\")\n",
        "\n",
        "    top = 12\n",
        "    order = np.array([names.index(f) for f in cir_multi.index[:top]])\n",
        "    M = cir_per_class[order, :]   # top features x classes\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.imshow(M.T, aspect=\"auto\")\n",
        "    plt.yticks(range(classes), [f\"class {c}\" for c in range(classes)])\n",
        "    plt.xticks(range(top), cir_multi.index[:top], rotation=45, ha=\"right\")\n",
        "    plt.colorbar(label=\"CIR per class\")\n",
        "    plt.title(\"Multi-output CIR (top features)\")\n",
        "    safe_plot_save(f\"exp5_cir_multi_{tag}.png\")\n",
        "    print(\"[Exp5] Saved multi-output CSV + heatmap.\")\n"
      ],
      "metadata": {
        "id": "_3Sn5Zinweev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT 6 — Image last-layer channels (optional)\n",
        "# ============================================================\n",
        "def exp6_image_channels(tag=\"cifar\"):\n",
        "    try:\n",
        "        import torch, torchvision\n",
        "        from torchvision import datasets, transforms, models\n",
        "    except Exception:\n",
        "        print(\"Exp6 skipped (PyTorch/torchvision not available).\")\n",
        "        return\n",
        "    # Small subset CIFAR10\n",
        "    tfm = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
        "    ds = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfm)\n",
        "    loader = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=False)\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "    model.eval()\n",
        "\n",
        "    feats_list=[]; probs_list=[]\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            # get penultimate layer activations\n",
        "            def hook(module, inp, out):\n",
        "                feats_list.append(out.detach().cpu().numpy())\n",
        "            h = model.avgpool.register_forward_hook(hook)\n",
        "            out = model(x)\n",
        "            h.remove()\n",
        "            p = torch.softmax(out, dim=1).detach().cpu().numpy()\n",
        "            probs_list.append(p)\n",
        "    F = np.concatenate(feats_list, axis=0).reshape(len(ds), -1)  # channels\n",
        "    P = np.concatenate(probs_list, axis=0)\n",
        "    scl = StandardScaler().fit(F); Fs = scl.transform(F)\n",
        "    yhat = P.max(axis=1)  # scalar conf\n",
        "    names = [f\"ch{i}\" for i in range(Fs.shape[1])]\n",
        "    cir = pd.Series(\n",
        "        [0.0]*Fs.shape[1], index=names\n",
        "    )\n",
        "    # Quick CIR on channels (may be large; downsample)\n",
        "    idx = np.random.choice(Fs.shape[0], size=min(3000, Fs.shape[0]), replace=False)\n",
        "    def _cir(Xs, yhat):\n",
        "        n = Xs.shape[0]; y_bar=float(np.mean(yhat)); out=[]\n",
        "        for i in range(Xs.shape[1]):\n",
        "            f = Xs[:, i]; f_bar=float(np.mean(f)); m=0.5*(f_bar+y_bar)\n",
        "            num = n*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "            den = float(np.sum((f - m)**2) + np.sum((yhat - m)**2))\n",
        "            out.append(0.0 if den==0 else num/den)\n",
        "        return np.array(out)\n",
        "    cir_vals = _cir(Fs[idx], yhat[idx])\n",
        "    pd.Series(cir_vals, index=names).sort_values(ascending=False).head(50).to_csv(f\"exp6_channels_top_{tag}.csv\")\n",
        "    print(\"Exp6 saved channel CIR top list.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSvz8limwg80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT 7 — NLP (token/word) bag-of-words (optional)\n",
        "# ============================================================\n",
        "def exp7_nlp_bow(tag=\"nlp\"):\n",
        "    try:\n",
        "        from sklearn.datasets import fetch_20newsgroups\n",
        "        from sklearn.feature_extraction.text import CountVectorizer\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "    except Exception:\n",
        "        print(\"Exp7 skipped (sklearn fetch or text packages unavailable).\")\n",
        "        return\n",
        "    ds = fetch_20newsgroups(subset=\"train\", categories=[\"sci.med\",\"sci.space\"], remove=(\"headers\",\"footers\",\"quotes\"))\n",
        "    dv = fetch_20newsgroups(subset=\"test\", categories=[\"sci.med\",\"sci.space\"], remove=(\"headers\",\"footers\",\"quotes\"))\n",
        "    vec = CountVectorizer(max_features=5000, ngram_range=(1,1))\n",
        "    Xtr = vec.fit_transform(ds.data).astype(float); ytr = (ds.target==1).astype(int)\n",
        "    Xte = vec.transform(dv.data).astype(float); yte = (dv.target==1).astype(int)\n",
        "    scl = StandardScaler(with_mean=False).fit(Xtr)\n",
        "    Xtr_s = scl.transform(Xtr); Xte_s = scl.transform(Xte)\n",
        "    mdl = LogisticRegression(max_iter=1000).fit(Xtr_s, ytr)\n",
        "    yhat = mdl.predict_proba(Xte_s)[:,1]\n",
        "    names = vec.get_feature_names_out()\n",
        "    # CIR on sparse: compute on dense slices (top variance words)\n",
        "    var = np.array((Xte_s.power(2)).mean(axis=0)).ravel()\n",
        "    keep = np.argsort(var)[-2000:]\n",
        "    Xdense = Xte_s[:, keep].toarray()\n",
        "    cir = []\n",
        "    y_bar = float(np.mean(yhat)); n = Xdense.shape[0]\n",
        "    for i in range(Xdense.shape[1]):\n",
        "        f = Xdense[:, i]; f_bar=float(np.mean(f)); m=0.5*(f_bar+y_bar)\n",
        "        num = n*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "        den = float(np.sum((f - m)**2) + np.sum((yhat - m)**2))\n",
        "        cir.append(0.0 if den==0 else num/den)\n",
        "    pd.Series(cir, index=names[keep]).sort_values(ascending=False).head(50).to_csv(f\"exp7_bow_cir_{tag}.csv\")\n",
        "    print(\"Exp7 saved word CIR top list.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3dPMr8L4wjaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT 8 — Runtime/Memory scaling (n,d)\n",
        "# ============================================================\n",
        "def exp8_scaling(ns=(2000, 5000, 10000), ds=(20, 100, 500), tag=\"scale\"):\n",
        "    rows=[]\n",
        "    for n in ns:\n",
        "        for d in ds:\n",
        "            X, y = make_classification(n_samples=int(n), n_features=int(d),\n",
        "                                       n_informative=max(2, int(d//5)), random_state=RANDOM_SEED)\n",
        "            scl = StandardScaler().fit(X); Xs = scl.transform(X)\n",
        "            mdl = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(Xs, y)\n",
        "            yv = mdl.predict_proba(Xs)[:,1]\n",
        "            t0=time.time()\n",
        "            # CIR timing\n",
        "            nrows = Xs.shape[0]; y_bar=float(np.mean(yv)); vals=[]\n",
        "            for i in range(Xs.shape[1]):\n",
        "                f = Xs[:, i]; f_bar=float(np.mean(f)); m=0.5*(f_bar+y_bar)\n",
        "                num = nrows*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "                den = float(np.sum((f - m)**2) + np.sum((yv - m)**2))\n",
        "                vals.append(0.0 if den==0 else num/den)\n",
        "            t1=time.time()\n",
        "            mem = psutil.Process(_os.getpid()).memory_info().rss if psutil else np.nan\n",
        "            rows.append([\"ExCIR\", n, d, t1-t0, mem])\n",
        "            # SHAP time (simple, small nsamples)\n",
        "            if shap is not None and d <= 200:\n",
        "                t0=time.time()\n",
        "                expl = shap.TreeExplainer(mdl)\n",
        "                _ = expl.shap_values(Xs[:min(800, Xs.shape[0])])\n",
        "                t1=time.time()\n",
        "                mem2 = psutil.Process(_os.getpid()).memory_info().rss if psutil else np.nan\n",
        "                rows.append([\"SHAP(~800)\", n, d, t1-t0, mem2])\n",
        "            # LIME time (very rough on 200 samples)\n",
        "            if LimeTabularExplainer is not None and d <= 200:\n",
        "                t0=time.time()\n",
        "                expl = LimeTabularExplainer(Xs[:100], feature_names=[f\"f{i}\" for i in range(d)],\n",
        "                                            class_names=[\"0\",\"1\"], discretize_continuous=True,\n",
        "                                            mode=\"classification\", random_state=RANDOM_SEED)\n",
        "                for r in range(min(50, Xs.shape[0])):\n",
        "                    _ = expl.explain_instance(Xs[r], lambda Z: mdl.predict_proba(Z), num_features=10)\n",
        "                t1=time.time()\n",
        "                mem3 = psutil.Process(_os.getpid()).memory_info().rss if psutil else np.nan\n",
        "                rows.append([\"LIME(~50)\", n, d, t1-t0, mem3])\n",
        "    pd.DataFrame(rows, columns=[\"method\",\"n\",\"d\",\"sec\",\"rss\"]).to_csv(f\"exp8_scaling_{tag}.csv\", index=False)\n",
        "    print(\"Exp8 saved exp8_scaling_*.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ugR4Nd_JwmP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT 9 — Spurious correlation stress test\n",
        "# ============================================================\n",
        "def exp9_spurious(tag=\"spur\"):\n",
        "    # Env A: spurious s correlated with y; Env B: flip correlation\n",
        "    n=6000\n",
        "    x_core = rng.normal(size=(n, 5))\n",
        "    beta = np.array([1.2, 0.8, 0.5, 0.0, 0.0])\n",
        "    logit = x_core @ beta + rng.normal(0,0.6,n)\n",
        "    p = 1/(1+np.exp(-logit))\n",
        "    y = (rng.uniform(0,1,n)<p).astype(int)\n",
        "    s = (y ^ (rng.uniform(0,1,n)<0.05)).astype(int)  # spurious, ~95% aligned with y\n",
        "\n",
        "    X_A = np.column_stack([x_core, s])\n",
        "    X_B = np.column_stack([x_core, 1-s])  # flip\n",
        "    names = [f\"core{i}\" for i in range(5)] + [\"spurious\"]\n",
        "\n",
        "    for env, X in [(\"A\", X_A), (\"B\", X_B)]:\n",
        "        scl = StandardScaler().fit(X); Xs = scl.transform(X)\n",
        "        mdl = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(Xs, y)\n",
        "        yv = mdl.predict_proba(Xs)[:,1]\n",
        "        cir=[]\n",
        "        y_bar=float(np.mean(yv)); nrows=Xs.shape[0]\n",
        "        for i in range(Xs.shape[1]):\n",
        "            f = Xs[:, i]; f_bar=float(np.mean(f)); m=0.5*(f_bar+y_bar)\n",
        "            num = nrows*((f_bar - m)**2 + (y_bar - m)**2); den = float(np.sum((f-m)**2)+np.sum((yv-m)**2))\n",
        "            cir.append(0.0 if den==0 else num/den)\n",
        "        pd.Series(cir, index=names).sort_values(ascending=False).to_csv(f\"exp9_spurious_env{env}_{tag}.csv\")\n",
        "    print(\"Exp9 saved spurious env A/B CSVs.\")\n"
      ],
      "metadata": {
        "id": "SwfanRHGwosW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT 10 — Baseline sensitivity (SHAP sample size, LIME kernel)\n",
        "# ============================================================\n",
        "def exp10_baseline_sensitivity(k_list=(6,8),\n",
        "                               val_sample_sizes=(100, 500, 800),\n",
        "                               lime_kernels=(0.25, 0.5, 1.0),\n",
        "                               B=300, tag=\"sens\"):\n",
        "    \"\"\"\n",
        "    Sensitivity of top-k sufficiency accuracy to baseline settings.\n",
        "      - SHAP: vary how many validation rows are used to build the global ranking.\n",
        "      - LIME: vary kernel_width.\n",
        "    Saves CSV and a summary plot.\n",
        "    \"\"\"\n",
        "    # --- make sure data/model exist ---\n",
        "    if 'orig_model' not in globals() or 'X_val' not in globals():\n",
        "        ensure_synthetic_vehicular()\n",
        "\n",
        "    rec = []\n",
        "\n",
        "    # ---------- SHAP sensitivity: vary the number of validation rows used ----------\n",
        "    if shap is not None:\n",
        "        try:\n",
        "            expl = shap.TreeExplainer(orig_model)\n",
        "            Xv = scaler.transform(X_val)\n",
        "            n  = Xv.shape[0]\n",
        "            for m in val_sample_sizes:\n",
        "                m_eff = int(min(m, n))\n",
        "                idx = np.random.default_rng(0).choice(n, size=m_eff, replace=False)\n",
        "                try:\n",
        "                    sv = expl.shap_values(Xv[idx])\n",
        "                    if isinstance(sv, list):  # tree explainer sometimes returns list\n",
        "                        sv = sv[-1]\n",
        "                    s = pd.Series(np.abs(sv).mean(0), index=X_df.columns).sort_values(ascending=False)\n",
        "                    for k in k_list:\n",
        "                        acc, lo, hi = topk_bootstrap_ci(s, k, B=B, seed=0)\n",
        "                        rec.append([\"SHAP\", f\"val_sample={m_eff}\", k, acc, lo, hi])\n",
        "                except Exception as e:\n",
        "                    print(f\"[Exp10][SHAP val_sample={m_eff}] skipped:\", e)\n",
        "        except Exception as e:\n",
        "            print(\"[Exp10] SHAP unavailable/failed:\", e)\n",
        "\n",
        "    # ---------- LIME sensitivity: vary kernel_width ----------\n",
        "    if LimeTabularExplainer is not None:\n",
        "        Xtr = scaler.transform(X_train)\n",
        "        Xv  = scaler.transform(X_val)\n",
        "        preds = lambda Z: orig_model.predict_proba(Z)\n",
        "        for kw in lime_kernels:\n",
        "            try:\n",
        "                expl = LimeTabularExplainer(\n",
        "                    Xtr, feature_names=X_df.columns.tolist(),\n",
        "                    class_names=[\"safe\",\"unsafe\"],\n",
        "                    discretize_continuous=True, kernel_width=kw,\n",
        "                    mode=\"classification\", random_state=RANDOM_SEED\n",
        "                )\n",
        "                idx = np.random.default_rng(0).choice(Xv.shape[0], size=min(300, Xv.shape[0]), replace=False)\n",
        "                agg = pd.Series(0.0, index=X_df.columns); cnt = pd.Series(0, index=X_df.columns, dtype=int)\n",
        "                for r in idx:\n",
        "                    E = expl.explain_instance(Xv[r], preds, num_features=min(10, Xv.shape[1]))\n",
        "                    for name, w in E.as_list():\n",
        "                        if name in agg.index:\n",
        "                            agg[name] += abs(w); cnt[name] += 1\n",
        "                cnt[cnt==0] = 1\n",
        "                s = (agg/cnt).sort_values(ascending=False)\n",
        "                for k in k_list:\n",
        "                    acc, lo, hi = topk_bootstrap_ci(s, k, B=B, seed=0)\n",
        "                    rec.append([\"LIME\", f\"kernel={kw}\", k, acc, lo, hi])\n",
        "            except Exception as e:\n",
        "                print(f\"[Exp10][LIME kernel={kw}] skipped:\", e)\n",
        "\n",
        "    # ---------- Save CSV ----------\n",
        "    df = pd.DataFrame(rec, columns=[\"method\",\"setting\",\"k\",\"acc\",\"ci_lo\",\"ci_hi\"])\n",
        "    out_csv = f\"exp10_sensitivity_{tag}.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(\"Exp10 saved\", out_csv)\n",
        "\n",
        "    # ---------- Make a quick plot ----------\n",
        "    if len(df):\n",
        "        plt.figure(figsize=(8,5))\n",
        "        # keep x labels stable & ordered\n",
        "        xlabels = sorted(df[\"setting\"].unique(), key=lambda s: (s.split(\"=\")[0], float(s.split(\"=\")[1])))\n",
        "        xmap = {lab:i for i,lab in enumerate(xlabels)}\n",
        "        for method in df[\"method\"].unique():\n",
        "            for k in sorted(df[df[\"method\"]==method][\"k\"].unique()):\n",
        "                sub = df[(df[\"method\"]==method) & (df[\"k\"]==k)].copy()\n",
        "                sub[\"x\"] = sub[\"setting\"].map(xmap)\n",
        "                sub = sub.sort_values(\"x\")\n",
        "                plt.plot(sub[\"x\"], sub[\"acc\"], \"-o\", label=f\"{method}, k={k}\")\n",
        "                # optional CI bars\n",
        "                yerr = [sub[\"acc\"]-sub[\"ci_lo\"], sub[\"ci_hi\"]-sub[\"acc\"]]\n",
        "                plt.errorbar(sub[\"x\"], sub[\"acc\"], yerr=yerr, fmt=\"none\", capsize=3, alpha=0.5)\n",
        "        plt.xticks(range(len(xlabels)), xlabels, rotation=30, ha=\"right\")\n",
        "        plt.ylabel(\"Accuracy (%)\"); plt.title(\"Exp10: Sensitivity of top-k accuracy\")\n",
        "        plt.legend()\n",
        "        safe_plot_save(f\"exp10_sensitivity_{tag}.png\")\n"
      ],
      "metadata": {
        "id": "bU6mSACkwrcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exp11_grouped(tag=\"groups\"):\n",
        "    groups = {\n",
        "        \"tires\": [\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"],\n",
        "        \"dynamics\": [\"accel_lat\",\"yaw_rate\",\"steering_deg\"],\n",
        "        \"powertrain\": [\"rpm\",\"engine_load\",\"maf\",\"fuel_rate\",\"gear\"],\n",
        "        \"speed\": [\"speed_kph\"],\n",
        "        \"brake\": [\"brake\"],\n",
        "        \"grade\": [\"road_grade\"]\n",
        "    }\n",
        "    yv = orig_model.predict_proba(scaler.transform(X_val))[:,1]\n",
        "    cir = compute_cir(scaler.transform(X_val), yv, X_df.columns.tolist())\n",
        "    gcir = pd.Series({g: cir[v].mean() for g, v in groups.items()}).sort_values(ascending=False)\n",
        "    gcir.to_csv(f\"exp11_group_cir_{tag}.csv\")\n",
        "\n",
        "    # Plot ExCIR groups\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.barh(gcir.index[::-1], gcir.values[::-1])\n",
        "    plt.xlabel(\"Group CIR (mean of members)\"); plt.title(\"Grouped ExCIR\"); plt.tight_layout()\n",
        "    safe_plot_save(f\"exp11_group_cir_{tag}.png\")\n",
        "\n",
        "    # Group SHAP\n",
        "    if shap is not None:\n",
        "        expl = shap.TreeExplainer(orig_model)\n",
        "        sv = expl.shap_values(scaler.transform(X_val.iloc[:min(800, len(X_val))].to_numpy()))\n",
        "        if isinstance(sv, list): sv = sv[-1]\n",
        "        sabs = pd.Series(np.abs(sv).mean(0), index=X_df.columns)\n",
        "        gshap = pd.Series({g: sabs[v].sum() for g,v in groups.items()}).sort_values(ascending=False)\n",
        "        gshap.to_csv(f\"exp11_group_shap_{tag}.csv\")\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.barh(gshap.index[::-1], gshap.values[::-1])\n",
        "        plt.xlabel(\"Group SHAP (sum of |weights|)\"); plt.title(\"Grouped SHAP\"); plt.tight_layout()\n",
        "        safe_plot_save(f\"exp11_group_shap_{tag}.png\")\n",
        "\n",
        "    # Group LIME\n",
        "    if LimeTabularExplainer is not None:\n",
        "        Xtr = scaler.transform(X_train); Xv = scaler.transform(X_val)\n",
        "        expl = LimeTabularExplainer(Xtr, feature_names=X_df.columns.tolist(),\n",
        "                                    class_names=[\"safe\",\"unsafe\"], discretize_continuous=True,\n",
        "                                    mode=\"classification\", random_state=RANDOM_SEED)\n",
        "        preds = lambda Z: orig_model.predict_proba(Z)\n",
        "        idx = np.random.default_rng(0).choice(Xv.shape[0], size=min(300, Xv.shape[0]), replace=False)\n",
        "        agg = pd.Series(0.0, index=X_df.columns); cnt = pd.Series(0, index=X_df.columns, dtype=int)\n",
        "        for r in idx:\n",
        "            E = expl.explain_instance(Xv[r], preds, num_features=min(10, Xv.shape[1]))\n",
        "            for name, w in E.as_list():\n",
        "                fname = name.split()[0]\n",
        "                if fname in agg.index: agg[fname]+=abs(w); cnt[fname]+=1\n",
        "        cnt[cnt==0]=1\n",
        "        lime_feat = (agg/cnt)\n",
        "        glime = pd.Series({g: lime_feat[v].sum() for g,v in groups.items()}).sort_values(ascending=False)\n",
        "        glime.to_csv(f\"exp11_group_lime_{tag}.csv\")\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.barh(glime.index[::-1], glime.values[::-1])\n",
        "        plt.xlabel(\"Group LIME (sum of |weights|)\"); plt.title(\"Grouped LIME\"); plt.tight_layout()\n",
        "        safe_plot_save(f\"exp11_group_lime_{tag}.png\")\n",
        "\n",
        "    print(\"[Exp11] Saved grouped plots/CSVs.\")\n"
      ],
      "metadata": {
        "id": "TN_NJIHewt80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT 12 — Lightweight transfer curves (agreement vs frac)\n",
        "# ============================================================\n",
        "def exp12_lightweight_transfer(fracs=(0.1,0.15,0.2,0.3,0.4,0.5), topk=8, tag=\"lw\"):\n",
        "    base = cir_orig_val  # ExCIR on full model (val)\n",
        "    rec=[]\n",
        "    for f in fracs:\n",
        "        # stratified subsample of train+val in full_df form\n",
        "        df = pd.DataFrame(full_Xs, columns=X_df.columns).assign(target=full_y)\n",
        "        light = df.groupby(\"target\", group_keys=False).apply(\n",
        "            lambda g: g.sample(max(1, int(len(g)*f)), random_state=RANDOM_SEED)\n",
        "        ).reset_index(drop=True)\n",
        "        Xl = light[X_df.columns].values; yl = light[\"target\"].values\n",
        "        t0=time.time()\n",
        "        m = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(Xl, yl)\n",
        "        yv = m.predict_proba(scaler.transform(X_val))[:,1]\n",
        "        cir_l = compute_cir(scaler.transform(X_val), yv, X_df.columns.tolist())\n",
        "        rho = spearmanr(base.rank(), cir_l[base.index].rank()).correlation\n",
        "        overlap = len(set(base.head(topk).index) & set(cir_l.head(topk).index))/topk\n",
        "        dt=time.time()-t0\n",
        "        rec.append([f, rho, overlap, dt])\n",
        "    df = pd.DataFrame(rec, columns=[\"frac\",\"spearman\",\"topk_overlap\",\"sec\"])\n",
        "    df.to_csv(f\"exp12_lightweight_transfer_{tag}.csv\", index=False)\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(df[\"frac\"], df[\"spearman\"], \"-o\", label=\"Spearman (ranks)\")\n",
        "    plt.plot(df[\"frac\"], df[\"topk_overlap\"], \"-o\", label=f\"Top-{topk} overlap\")\n",
        "    plt.xlabel(\"Lightweight fraction f\"); plt.ylim(0,1.05)\n",
        "    plt.ylabel(\"Agreement\"); plt.title(\"Lightweight transfer\")\n",
        "    plt.legend(); safe_plot_save(f\"exp12_lightweight_transfer_{tag}.png\")\n",
        "    print(\"Exp12 done.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uSxGt_Jzwwtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Optional: quick ExCIR/SHAP/LIME ranking dict (if available)\n",
        "# ============================================================\n",
        "def get_rankings_dict():\n",
        "    ranks = {\"ExCIR\": cir_orig_val}\n",
        "    if shap is not None and shap_series is not None:\n",
        "        ranks[\"SHAP\"] = shap_series\n",
        "    if LimeTabularExplainer is not None and lime_series is not None:\n",
        "        ranks[\"LIME\"] = lime_series\n",
        "    return ranks"
      ],
      "metadata": {
        "id": "oRuLgHUJwy1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SAFE RANKINGS DICT (drop this above the __main__ block) ----------\n",
        "def get_rankings_dict():\n",
        "    \"\"\"\n",
        "    Returns a dict of available global rankings:\n",
        "      {\"ExCIR\": cir_orig_val, \"SHAP\": shap_series, \"LIME\": lime_series}\n",
        "    Builds the synthetic setup if needed.\n",
        "    \"\"\"\n",
        "    # If the synthetic setup hasn't been created in this session, build it.\n",
        "    if 'cir_orig_val' not in globals():\n",
        "        try:\n",
        "            ensure_synthetic_vehicular()  # defined earlier in your script\n",
        "        except NameError as e:\n",
        "            raise RuntimeError(\n",
        "                \"ensure_synthetic_vehicular() is missing. \"\n",
        "                \"Make sure the data/model setup function is defined above.\"\n",
        "            ) from e\n",
        "\n",
        "    ranks = {}\n",
        "\n",
        "    # ExCIR (required)\n",
        "    excir = globals().get('cir_orig_val', None)\n",
        "    if excir is None:\n",
        "        raise RuntimeError(\"cir_orig_val not available; ensure ensure_synthetic_vehicular() ran successfully.\")\n",
        "    ranks[\"ExCIR\"] = excir\n",
        "\n",
        "    # SHAP (optional)\n",
        "    shap_series = globals().get('shap_series', None)\n",
        "    if shap_series is not None:\n",
        "        ranks[\"SHAP\"] = shap_series\n",
        "\n",
        "    # LIME (optional)\n",
        "    lime_series = globals().get('lime_series', None)\n",
        "    if lime_series is not None:\n",
        "        ranks[\"LIME\"] = lime_series\n",
        "\n",
        "    return ranks\n",
        "# ---------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "7KxYzjRCw1XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Optional: quick ExCIR/SHAP/LIME ranking dict (if available)\n",
        "# ============================================================\n",
        "def get_rankings_dict():\n",
        "    ranks = {\"ExCIR\": cir_orig_val}\n",
        "    if shap is not None and shap_series is not None:\n",
        "        ranks[\"SHAP\"] = shap_series\n",
        "    if LimeTabularExplainer is not None and lime_series is not None:\n",
        "        ranks[\"LIME\"] = lime_series\n",
        "    return ranks\n",
        "\n",
        "# ============================================================\n",
        "# RUN TOGGLES\n",
        "# ============================================================\n",
        "RUN_EXP1 = True\n",
        "RUN_EXP2 = True\n",
        "RUN_EXP3 = True\n",
        "RUN_EXP4 = True\n",
        "RUN_EXP5 = True\n",
        "RUN_EXP6 = False  # needs torchvision\n",
        "RUN_EXP7 = False  # needs 20newsgroups fetch\n",
        "RUN_EXP8 = True\n",
        "RUN_EXP9 = True\n",
        "RUN_EXP10 = True\n",
        "RUN_EXP11 = True\n",
        "RUN_EXP12 = True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ranks = get_rankings_dict()\n",
        "\n",
        "    if RUN_EXP1:\n",
        "        exp1_aopc(ranks, step=1, tag=\"veh\")\n",
        "\n",
        "    if RUN_EXP2:\n",
        "        rank_func = lambda mdl: compute_cir(\n",
        "            scaler.transform(X_val),\n",
        "            mdl.predict_proba(scaler.transform(X_val))[:,1],\n",
        "            X_df.columns.tolist()\n",
        "        )\n",
        "        exp2_randomization(rank_func, tag=\"veh\")\n",
        "\n",
        "    if RUN_EXP3:\n",
        "        exp3_correlated_blocks()\n",
        "\n",
        "    if RUN_EXP4:\n",
        "        exp4_drift_noise()\n",
        "\n",
        "    if RUN_EXP5:\n",
        "        exp5_multi_output()\n",
        "\n",
        "    if RUN_EXP6:\n",
        "        exp6_image_channels()\n",
        "\n",
        "    if RUN_EXP7:\n",
        "        exp7_nlp_bow()\n",
        "\n",
        "    if RUN_EXP8:\n",
        "        exp8_scaling()\n",
        "\n",
        "    if RUN_EXP9:\n",
        "        exp9_spurious()\n",
        "\n",
        "    if RUN_EXP10:\n",
        "        exp10_baseline_sensitivity()\n",
        "\n",
        "    if RUN_EXP11:\n",
        "        exp11_grouped()\n",
        "\n",
        "    if RUN_EXP12:\n",
        "        exp12_lightweight_transfer()"
      ],
      "metadata": {
        "id": "YWqLAeycw4Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Exp-5: Agreement–cost sweep for lightweight size (Pareto view) ====\n",
        "from time import perf_counter\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "def exp5_agreement_cost(\n",
        "    fractions=(0.20, 0.30, 0.35, 0.40, 0.50),\n",
        "    topk=8,\n",
        "    seed=RANDOM_SEED,\n",
        "    out_png=\"exp5_agreement_pareto.png\",\n",
        "    out_csv=\"exp5_agreement_table.csv\"\n",
        "):\n",
        "    rows = []\n",
        "    for f in fractions:\n",
        "        t0 = perf_counter()\n",
        "        # class-balanced subsample from the train+val pool created earlier as `full_df`\n",
        "        light_df = full_df.groupby(\"target\", group_keys=False).apply(\n",
        "            lambda g: g.sample(max(1, int(len(g)*f)), random_state=seed)\n",
        "        ).reset_index(drop=True)\n",
        "        X_light = light_df[X_df.columns].values\n",
        "        y_light = light_df[\"target\"].values\n",
        "\n",
        "        m = Model(random_state=seed).fit(X_light, y_light)\n",
        "        yhat_val_light = model_outputs(m, scaler.transform(X_val))\n",
        "        cir_light = compute_cir(scaler.transform(X_val), yhat_val_light, X_df.columns.tolist())\n",
        "\n",
        "        # agreement metrics (vs original CIR on the same validation set)\n",
        "        r = spearmanr(cir_orig_val.rank(), cir_light[cir_orig_val.index].rank()).correlation\n",
        "        overlap = len(set(cir_orig_val.head(topk).index) & set(cir_light.head(topk).index)) / topk\n",
        "\n",
        "        runtime = perf_counter() - t0\n",
        "        rows.append({\"fraction\": f, \"spearman\": r, \"topk_overlap\": overlap, \"time_sec\": runtime})\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(\"fraction\")\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"[Exp5] Saved {out_csv}\")\n",
        "\n",
        "    # Pareto scatter: time vs Spearman; marker size encodes top-k overlap\n",
        "    plt.figure(figsize=(7,5))\n",
        "    s = 400 * df[\"topk_overlap\"].values  # scale marker size by overlap fraction\n",
        "    plt.scatter(df[\"time_sec\"], df[\"spearman\"], s=s)\n",
        "    for i, row in df.iterrows():\n",
        "        plt.annotate(f\"{row['fraction']:.2f}\", (row[\"time_sec\"], row[\"spearman\"]), xytext=(5,5), textcoords=\"offset points\")\n",
        "    plt.xlabel(\"Wall-clock time (s)\")\n",
        "    plt.ylabel(\"Spearman rank correlation (CIR: full vs lightweight)\")\n",
        "    plt.title(\"Experiment 5: Agreement–cost Pareto (marker size = top-8 overlap)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=150)\n",
        "    print(f\"[Exp5] Saved {out_png}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "exp5_table = exp5_agreement_cost()\n",
        "\n",
        "# ==== Exp-6: Runtime scaling vs sample fraction (line plot) ====\n",
        "def exp6_runtime_scaling(\n",
        "    table_df=None,\n",
        "    out_png=\"exp6_runtime_scaling.png\"\n",
        "):\n",
        "    df = table_df.copy() if table_df is not None else exp5_table.copy()\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(df[\"fraction\"], df[\"time_sec\"], marker=\"o\")\n",
        "    plt.xlabel(\"Lightweight fraction of train+val kept\")\n",
        "    plt.ylabel(\"Wall-clock time (s)\")\n",
        "    plt.title(\"Experiment 6: Runtime scaling with sample size\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=150)\n",
        "    print(f\"[Exp6] Saved {out_png}\")\n",
        "\n",
        "exp6_runtime_scaling(exp5_table)\n"
      ],
      "metadata": {
        "id": "LffF05U1w-cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ExCIR on Images (sklearn digits) + MI + PFI\n",
        "# ------------------------------------------------------------\n",
        "# Requires: numpy, pandas, matplotlib, scikit-learn\n",
        "# Saves figures/CSVs in the current folder.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from numpy.random import default_rng\n",
        "\n",
        "OUT = Path(\"./\")  # change if you want a different output folder\n",
        "SEED = 42\n",
        "rng = default_rng(SEED)\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def compute_cir_univariate(Xs: np.ndarray, yhat: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    ExCIR per feature (global, univariate), using the 'midpoint' formula.\n",
        "    Inputs:\n",
        "      Xs   : (n, d) standardized features\n",
        "      yhat : (n,) model scores (e.g., proba for a class)\n",
        "    Returns:\n",
        "      (d,) ExCIR values (higher = moves with yhat more strongly)\n",
        "    \"\"\"\n",
        "    n, d = Xs.shape\n",
        "    y_bar = float(yhat.mean())\n",
        "    den_y = float(np.sum((yhat - 0.5*(0 + y_bar))**2))  # not used alone, keep for clarity\n",
        "\n",
        "    out = np.empty(d, dtype=float)\n",
        "    for j in range(d):\n",
        "        f = Xs[:, j]\n",
        "        f_bar = float(f.mean())\n",
        "        m = 0.5*(f_bar + y_bar)\n",
        "        num = n * ((f_bar - m)**2 + (y_bar - m)**2)\n",
        "        den = float(np.sum((f - m)**2) + np.sum((yhat - m)**2))\n",
        "        out[j] = 0.0 if den <= 0 else num / den\n",
        "    return out\n",
        "\n",
        "def to_img(v: np.ndarray, h=8, w=8):\n",
        "    return v.reshape(h, w)\n",
        "\n",
        "def save_heatmap(img, title, fname, cmap=\"viridis\"):\n",
        "    plt.figure(figsize=(6, 4.8))\n",
        "    plt.imshow(img, cmap=cmap)\n",
        "    plt.colorbar(label=\"score\")\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT / fname, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def save_topk_csv(scores, k, fname):\n",
        "    # attach (row, col) for readability\n",
        "    d = scores.size\n",
        "    rows = []\n",
        "    order = np.argsort(scores)[::-1]\n",
        "    for rank, j in enumerate(order[:k], start=1):\n",
        "        r, c = divmod(j, 8)\n",
        "        rows.append([rank, j, r, c, scores[j]])\n",
        "    pd.DataFrame(rows, columns=[\"rank\", \"pixel_index\", \"row\", \"col\", \"score\"]).to_csv(OUT / fname, index=False)\n",
        "\n",
        "def permutation_feature_importance(model, X_test_s, y_test, n_repeats=1, seed=SEED):\n",
        "    \"\"\"\n",
        "    PFI per pixel: accuracy drop when permuting each pixel independently.\n",
        "    Returns drops (d,) and baseline accuracy.\n",
        "    \"\"\"\n",
        "    rng = default_rng(seed)\n",
        "    baseline = accuracy_score(y_test, model.predict(X_test_s))\n",
        "    d = X_test_s.shape[1]\n",
        "    drops = np.zeros(d)\n",
        "    for j in range(d):\n",
        "        accs = []\n",
        "        for _ in range(n_repeats):\n",
        "            Xp = X_test_s.copy()\n",
        "            Xp[:, j] = rng.permutation(Xp[:, j])\n",
        "            accs.append(accuracy_score(y_test, model.predict(Xp)))\n",
        "        drops[j] = baseline - np.mean(accs)\n",
        "    return drops, baseline\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Load data, split, standardize, train\n",
        "# -----------------------------\n",
        "digits = datasets.load_digits()\n",
        "X = digits.data.astype(float)      # (n, 64) pixels 0..16\n",
        "y = digits.target.astype(int)      # 10 classes\n",
        "H = W = 8\n",
        "\n",
        "X_tv, X_test, y_tv, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=SEED)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tv, y_tv, test_size=0.20, stratify=y_tv, random_state=SEED)\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_s = scaler.transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=2000, random_state=SEED\n",
        ").fit(X_train_s, y_train)\n",
        "\n",
        "val_acc  = accuracy_score(y_val,  clf.predict(X_val_s))\n",
        "test_acc = accuracy_score(y_test, clf.predict(X_test_s))\n",
        "print(f\"Val acc:  {val_acc:.4f}\")\n",
        "print(f\"Test acc: {test_acc:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Class-conditioned ExCIR & MI (for two example classes)\n",
        "# -----------------------------\n",
        "P_val = clf.predict_proba(X_val_s)  # (n_val, 10)\n",
        "\n",
        "for cls in [8, 1]:\n",
        "    yhat_c = P_val[:, cls]\n",
        "    # ExCIR per pixel\n",
        "    cir_pix = compute_cir_univariate(X_val_s, yhat_c)\n",
        "    pd.Series(cir_pix).to_csv(OUT / f\"digits_ExCIR_class{cls}.csv\", index=False)\n",
        "    save_heatmap(\n",
        "        to_img(cir_pix, H, W),\n",
        "        title=f\"ExCIR per pixel — class {cls}\",\n",
        "        fname=f\"digits_ExCIR_class{cls}.png\",\n",
        "    )\n",
        "    save_topk_csv(cir_pix, k=10, fname=f\"digits_ExCIR_class{cls}_top10.csv\")\n",
        "\n",
        "    # Mutual information between pixel values and p(class=cls)\n",
        "    mi_pix = mutual_info_regression(X_val_s, yhat_c, random_state=SEED, discrete_features=False)\n",
        "    pd.Series(mi_pix).to_csv(OUT / f\"digits_MI_class{cls}.csv\", index=False)\n",
        "    save_heatmap(\n",
        "        to_img(mi_pix, H, W),\n",
        "        title=f\"Mutual Information (pixel, p_c) — class {cls}\",\n",
        "        fname=f\"digits_MI_class{cls}.png\",\n",
        "        cmap=\"magma\",\n",
        "    )\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Permutation Feature Importance (overall accuracy)\n",
        "# -----------------------------\n",
        "drops, base_acc = permutation_feature_importance(clf, X_test_s, y_test, n_repeats=3, seed=SEED)\n",
        "pd.DataFrame({\"pixel_index\": np.arange(drops.size), \"acc_drop\": drops}).to_csv(\n",
        "    OUT / \"digits_PFI_overall.csv\", index=False\n",
        ")\n",
        "save_heatmap(\n",
        "    to_img(drops, H, W),\n",
        "    title=f\"PFI — accuracy drop per pixel (baseline acc={base_acc:.3f})\",\n",
        "    fname=\"digits_PFI_overall.png\",\n",
        "    cmap=\"plasma\",\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Class mean images (for visual context)\n",
        "# -----------------------------\n",
        "for cls in [8, 1]:\n",
        "    mean_img = X[y == cls].mean(axis=0)\n",
        "    save_heatmap(to_img(mean_img, H, W),\n",
        "                 title=f\"Mean image — class {cls}\",\n",
        "                 fname=f\"digits_mean_class{cls}.png\",\n",
        "                 cmap=\"gray\")\n"
      ],
      "metadata": {
        "id": "wS7F6ONRxFSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ExCIR Benchmarks (Fresh Run): 12 Methods, Correlations & Top-k Tests\n",
        "# ============================================================\n",
        "# - Creates synthetic vehicular dataset\n",
        "# - Trains a GradientBoostingClassifier on standardized features\n",
        "# - Computes global rankings via:\n",
        "#   ExCIR, SHAP*, LIME*, PFI, TreeGain, PDP-variance, MI(pred),\n",
        "#   HSIC(pred), SAGE*, MI(label), Surrogate-LR (mimic), (optional) Surrogate-Ridge\n",
        "#   (* optional; auto-skips if package missing)\n",
        "# - Produces: CSVs + PNGs in ./out/\n",
        "#\n",
        "# Requirements:\n",
        "#   numpy, pandas, matplotlib, scikit-learn, scipy\n",
        "#   Optional: shap, lime, sage-importance, psutil\n",
        "#\n",
        "# Run:\n",
        "#   python excir_benchmarks_full.py\n",
        "# ============================================================\n",
        "\n",
        "import os, time, math, warnings, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import partial_dependence\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
        "\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Optional deps\n",
        "try:\n",
        "    import shap\n",
        "except Exception:\n",
        "    shap = None\n",
        "\n",
        "try:\n",
        "    from lime.lime_tabular import LimeTabularExplainer\n",
        "except Exception:\n",
        "    LimeTabularExplainer = None\n",
        "\n",
        "try:\n",
        "    import sage  # pip install sage-importance\n",
        "except Exception:\n",
        "    sage = None\n",
        "\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "rng = check_random_state(RANDOM_SEED)\n",
        "OUT = Path(\"./out\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Data generation (fresh)\n",
        "# ------------------------------------------------------------\n",
        "def make_synthetic_vehicular(random_state=RANDOM_SEED):\n",
        "    rng = check_random_state(random_state)\n",
        "    n = 6000\n",
        "\n",
        "    feat_names = [\n",
        "        \"speed_kph\",\"rpm\",\"throttle\",\"brake\",\"steering_deg\",\"gear\",\n",
        "        \"accel_long\",\"accel_lat\",\"yaw_rate\",\"road_grade\",\n",
        "        \"ambient_temp\",\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\",\n",
        "        \"engine_load\",\"maf\",\"intake_air_temp\",\"battery_v\",\"fuel_rate\"\n",
        "    ]\n",
        "\n",
        "    speed = np.clip(rng.normal(80, 15, n), 0, None)\n",
        "    throttle = np.clip(rng.beta(2, 2, n), 0, 1)\n",
        "    brake = np.clip(1 - throttle + rng.normal(0, 0.15, n), 0, 1)\n",
        "    steering = rng.normal(0, 10, n)\n",
        "    gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, n), 1, 7)\n",
        "    accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, n)\n",
        "    accel_lat = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, n)\n",
        "    yaw_rate = rng.normal(steering/30 * (speed/60), 0.2, n)\n",
        "    road_grade = rng.normal(0, 2, n)\n",
        "    ambient_temp = rng.normal(20, 8, n)\n",
        "\n",
        "    tire_base = rng.normal(34, 1.0, (n,4))\n",
        "    low_mask = rng.uniform(0,1,n) < 0.15\n",
        "    tire_drop = rng.normal(4, 1.0, (n,4)) * low_mask[:,None]\n",
        "    tires = tire_base - tire_drop\n",
        "\n",
        "    engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, n), 0, 100)\n",
        "    maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,n), 0, None)\n",
        "    intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, n), -10, 80)\n",
        "    battery_v = np.clip(rng.normal(13.8, 0.3, n) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "    fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,n), 0, None)\n",
        "    rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, n), 700, 7000)\n",
        "\n",
        "    X_df = pd.DataFrame({\n",
        "        \"speed_kph\": speed,\n",
        "        \"rpm\": rpm,\n",
        "        \"throttle\": throttle,\n",
        "        \"brake\": brake,\n",
        "        \"steering_deg\": steering,\n",
        "        \"gear\": gear,\n",
        "        \"accel_long\": accel_long,\n",
        "        \"accel_lat\": accel_lat,\n",
        "        \"yaw_rate\": yaw_rate,\n",
        "        \"road_grade\": road_grade,\n",
        "        \"ambient_temp\": ambient_temp,\n",
        "        \"tire_fl\": tires[:,0],\n",
        "        \"tire_fr\": tires[:,1],\n",
        "        \"tire_rl\": tires[:,2],\n",
        "        \"tire_rr\": tires[:,3],\n",
        "        \"engine_load\": engine_load,\n",
        "        \"maf\": maf,\n",
        "        \"intake_air_temp\": intake_air_temp,\n",
        "        \"battery_v\": battery_v,\n",
        "        \"fuel_rate\": fuel_rate,\n",
        "    })\n",
        "\n",
        "    low_tire = (32 - X_df[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "\n",
        "    risk_logit = (\n",
        "        1.2*(X_df[\"speed_kph\"]-110)/20\n",
        "        + 1.1*X_df[\"brake\"]\n",
        "        + 0.9*np.abs(X_df[\"steering_deg\"])/15\n",
        "        + 0.7*np.abs(X_df[\"yaw_rate\"])\n",
        "        + 0.8*(low_tire)\n",
        "        + 0.7*(X_df[\"engine_load\"]/100)\n",
        "        + 0.3*(X_df[\"road_grade\"]/5)\n",
        "        - 0.2*(X_df[\"battery_v\"]-13.5)\n",
        "    )\n",
        "    p = 1/(1+np.exp(-(risk_logit + rng.normal(0, 0.4, n))))\n",
        "    y = (rng.uniform(0,1,n) < p).astype(int)\n",
        "\n",
        "    # Splits\n",
        "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "        X_df, y, test_size=0.20, stratify=y, random_state=RANDOM_SEED\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_full, y_train_full, test_size=0.20, stratify=y_train_full, random_state=RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    # Standardize on train+val (the data the model will see for fitting)\n",
        "    scaler = StandardScaler()\n",
        "    X_fit = pd.concat([X_train, X_val], axis=0)\n",
        "    y_fit = np.concatenate([y_train, y_val], axis=0)\n",
        "    X_fit_s = scaler.fit_transform(X_fit)\n",
        "\n",
        "    X_train_s = scaler.transform(X_train)\n",
        "    X_val_s   = scaler.transform(X_val)\n",
        "    X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "    # Model\n",
        "    model = GradientBoostingClassifier(random_state=RANDOM_SEED)\n",
        "    model.fit(X_fit_s, y_fit)\n",
        "\n",
        "    pack = dict(\n",
        "        X_df=X_df, feat_names=X_df.columns.tolist(),\n",
        "        X_train=X_train, X_val=X_val, X_test=X_test,\n",
        "        y_train=y_train, y_val=y_val, y_test=y_test,\n",
        "        scaler=scaler, model=model,\n",
        "        X_fit=X_fit, y_fit=y_fit,\n",
        "        X_fit_s=X_fit_s, X_train_s=X_train_s, X_val_s=X_val_s, X_test_s=X_test_s\n",
        "    )\n",
        "    return pack\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ExCIR (global, univariate midpoint form)\n",
        "# ------------------------------------------------------------\n",
        "def compute_excir(Xs: np.ndarray, yhat: np.ndarray, names):\n",
        "    n = Xs.shape[0]\n",
        "    y_bar = float(np.mean(yhat))\n",
        "    vals = []\n",
        "    for i in range(Xs.shape[1]):\n",
        "        f = Xs[:, i]\n",
        "        f_bar = float(np.mean(f))\n",
        "        m = 0.5*(f_bar + y_bar)\n",
        "        num = n*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "        den = float(np.sum((f - m)**2) + np.sum((yhat - m)**2))\n",
        "        eta = float(num/den) if den > 0 else 0.0\n",
        "        vals.append(eta)\n",
        "    return pd.Series(vals, index=names).sort_values(ascending=False)\n",
        "\n",
        "# ================================\n",
        "# KDE–MI, Symmetric KL, Wasserstein\n",
        "# ================================\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from scipy.stats import wasserstein_distance\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _std01(a):\n",
        "    a = np.asarray(a).ravel()\n",
        "    m, s = a.mean(), a.std() + 1e-12\n",
        "    return (a - m) / s\n",
        "\n",
        "def _kde_logprob(X, bw):\n",
        "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=bw)\n",
        "    kde.fit(X)\n",
        "    return kde.score_samples(X)  # log p̂ at training points\n",
        "\n",
        "def compute_kde_mi_xy(X_val_s, yhat, feat_names, cap=3000, seed=0):\n",
        "    \"\"\"\n",
        "    I(x; yhat) ≈ (1/N) Σ [ log p̂_xy(x_i,y_i) - log p̂_x(x_i) - log p̂_y(y_i) ],\n",
        "    using KDE with Scott bandwidth on standardized variables.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(X_val_s.shape[0])\n",
        "    if len(idx) > cap:\n",
        "        idx = rng.choice(idx, size=cap, replace=False)\n",
        "\n",
        "    Y = _std01(yhat[idx])[:, None]\n",
        "    # Scott's rule: h ∝ n^{-1/(d+4)}; use scalar bandwidth on standardized data\n",
        "    n = len(idx); d_joint = 2\n",
        "    bw_joint = n ** (-1.0 / (d_joint + 4.0))  # n^{-1/6}\n",
        "    bw_marg  = n ** (-1.0 / (1 + 4.0))        # n^{-1/5} for 1D\n",
        "\n",
        "    out = []\n",
        "    for j, name in enumerate(feat_names):\n",
        "        Xj = _std01(X_val_s[idx, j])[:, None]\n",
        "        XY = np.hstack([Xj, Y])\n",
        "\n",
        "        lp_xy = _kde_logprob(XY, bw_joint)\n",
        "        lp_x  = _kde_logprob(Xj, bw_marg)\n",
        "        lp_y  = _kde_logprob(Y,  bw_marg)\n",
        "\n",
        "        mi = float(np.mean(lp_xy - lp_x - lp_y))\n",
        "        out.append(mi)\n",
        "\n",
        "    return pd.Series(out, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "def _hist_density_1d(a, bins=40, eps=1e-9):\n",
        "    a = np.asarray(a).ravel()\n",
        "    lo, hi = np.min(a), np.max(a)\n",
        "    if not np.isfinite(lo) or not np.isfinite(hi) or lo == hi:\n",
        "        # degenerate → return a single-bin uniform mass\n",
        "        p = np.ones(1) / 1.0\n",
        "        edges = np.array([lo, hi + 1e-6])\n",
        "        return p, edges\n",
        "    p, edges = np.histogram(a, bins=bins, range=(lo, hi), density=True)\n",
        "    p = p + eps\n",
        "    p = p / p.sum()\n",
        "    return p, edges\n",
        "\n",
        "def _sym_kl(p, q, eps=1e-12):\n",
        "    p = np.asarray(p) + eps\n",
        "    q = np.asarray(q) + eps\n",
        "    p = p / p.sum(); q = q / q.sum()\n",
        "    return 0.5 * (np.sum(p * np.log(p / q)) + np.sum(q * np.log(q / p)))\n",
        "\n",
        "def compute_kl_conditional(yhat, X_val_s, feat_names, q=0.3, bins=40):\n",
        "    \"\"\"\n",
        "    Symmetric KL between yhat|X_j in bottom-q vs top-q quantiles.\n",
        "    Higher = stronger distributional shift in predictions due to feature j.\n",
        "    \"\"\"\n",
        "    yh = np.asarray(yhat).ravel()\n",
        "    out = []\n",
        "    for j, name in enumerate(feat_names):\n",
        "        x = X_val_s[:, j]\n",
        "        lo_thr, hi_thr = np.quantile(x, [q, 1.0 - q])\n",
        "        lo = yh[x <= lo_thr]; hi = yh[x >= hi_thr]\n",
        "        if len(lo) < 5 or len(hi) < 5:\n",
        "            out.append(0.0); continue\n",
        "        p, e1 = _hist_density_1d(lo, bins=bins)\n",
        "        qh, e2 = _hist_density_1d(hi, bins=bins)\n",
        "        # If bin edges differ slightly, re-bin onto common bounds\n",
        "        lohi = (min(e1[0], e2[0]), max(e1[-1], e2[-1]))\n",
        "        p, _  = np.histogram(lo, bins=bins, range=lohi, density=True)\n",
        "        qh, _ = np.histogram(hi, bins=bins, range=lohi, density=True)\n",
        "        score = _sym_kl(p, qh)\n",
        "        out.append(float(score))\n",
        "    return pd.Series(out, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "def compute_wasserstein_conditional(yhat, X_val_s, feat_names, q=0.3):\n",
        "    \"\"\"\n",
        "    1D Wasserstein (Earth Mover’s) distance between yhat|low-q and yhat|high-q for each feature.\n",
        "    This is a projection-style embedding distance that is hyperparameter-light.\n",
        "    \"\"\"\n",
        "    yh = np.asarray(yhat).ravel()\n",
        "    out = []\n",
        "    for j, name in enumerate(feat_names):\n",
        "        x = X_val_s[:, j]\n",
        "        lo_thr, hi_thr = np.quantile(x, [q, 1.0 - q])\n",
        "        lo = yh[x <= lo_thr]; hi = yh[x >= hi_thr]\n",
        "        if len(lo) < 5 or len(hi) < 5:\n",
        "            out.append(0.0); continue\n",
        "        w = wasserstein_distance(lo, hi)\n",
        "        out.append(float(w))\n",
        "    return pd.Series(out, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Extra Ranking Methods\n",
        "# ------------------------------------------------------------\n",
        "def compute_shap_global(model, X_val_s, feat_names, seed=RANDOM_SEED, cap=1200):\n",
        "    if shap is None:\n",
        "        return None\n",
        "    try:\n",
        "        idx = np.arange(X_val_s.shape[0])\n",
        "        if len(idx) > cap:\n",
        "            rng = np.random.default_rng(seed)\n",
        "            idx = rng.choice(idx, size=cap, replace=False)\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        sv = explainer.shap_values(X_val_s[idx])\n",
        "        if isinstance(sv, list):  # multiclass guard\n",
        "            sv = sv[-1]\n",
        "        vals = np.abs(sv).mean(0)\n",
        "        return pd.Series(vals, index=feat_names).sort_values(ascending=False)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _lime_basefeat(name: str) -> str:\n",
        "    # LIME strings look like \"rpm <= 1234.5\" or \"steering_deg > -1.2\"\n",
        "    # Take the first token as base feature name.\n",
        "    return name.split(\" \")[0]\n",
        "\n",
        "def compute_lime_global(model, X_train_s, X_val_s, feat_names, seed=RANDOM_SEED, samples=400):\n",
        "    if LimeTabularExplainer is None:\n",
        "        return None\n",
        "    try:\n",
        "        expl = LimeTabularExplainer(\n",
        "            X_train_s,\n",
        "            feature_names=feat_names,\n",
        "            class_names=[\"safe\",\"unsafe\"],\n",
        "            discretize_continuous=True,\n",
        "            mode=\"classification\",\n",
        "            random_state=seed\n",
        "        )\n",
        "        preds = lambda Z: model.predict_proba(Z)\n",
        "        rng = np.random.default_rng(seed)\n",
        "        rows = rng.choice(X_val_s.shape[0], size=min(samples, X_val_s.shape[0]), replace=False)\n",
        "        agg = pd.Series(0.0, index=feat_names)\n",
        "        cnt = pd.Series(0, index=feat_names, dtype=int)\n",
        "        for r in rows:\n",
        "            exp = expl.explain_instance(X_val_s[r], preds, num_features=min(10, len(feat_names)))\n",
        "            for name, w in exp.as_list():\n",
        "                base = _lime_basefeat(name)\n",
        "                if base in agg.index:\n",
        "                    agg[base] += abs(w); cnt[base] += 1\n",
        "        cnt[cnt==0] = 1\n",
        "        return (agg/cnt).sort_values(ascending=False)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def compute_pfi(model, X_test_s, y_test, feat_names, n_repeats=5, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    base = accuracy_score(y_test, model.predict(X_test_s))\n",
        "    scores = []\n",
        "    Xw = X_test_s.copy()\n",
        "    for j in range(Xw.shape[1]):\n",
        "        drops = []\n",
        "        col = Xw[:, j].copy()\n",
        "        for _ in range(n_repeats):\n",
        "            rng.shuffle(Xw[:, j])\n",
        "            drops.append(base - accuracy_score(y_test, model.predict(Xw)))\n",
        "            Xw[:, j] = col\n",
        "        scores.append(np.mean(drops))\n",
        "    return pd.Series(scores, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "def compute_tree_gain(model, feat_names):\n",
        "    try:\n",
        "        imp = model.feature_importances_\n",
        "        return pd.Series(imp, index=feat_names).sort_values(ascending=False)\n",
        "    except Exception:\n",
        "        return pd.Series(0.0, index=feat_names)\n",
        "\n",
        "def compute_pdp_variance(model, X_val_s, feat_names):\n",
        "    vals = []\n",
        "    for j, name in enumerate(feat_names):\n",
        "        try:\n",
        "            pdp = partial_dependence(model, X_val_s, [j], kind=\"average\").average[0]\n",
        "            vals.append(np.var(pdp))\n",
        "        except Exception:\n",
        "            vals.append(0.0)\n",
        "    return pd.Series(vals, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "def compute_mi_with_pred(model, X_val_s, feat_names):\n",
        "    yhat = model.predict_proba(X_val_s)[:,1]\n",
        "    mi = mutual_info_regression(X_val_s, yhat, random_state=0)\n",
        "    return pd.Series(mi, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "def _stdize(X):\n",
        "    m = X.mean(axis=0, keepdims=True); s = X.std(axis=0, keepdims=True) + 1e-12\n",
        "    return (X - m) / s\n",
        "\n",
        "def compute_hsic_pred(model, X_val_s, feat_names, cap=3000, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(X_val_s.shape[0])\n",
        "    if len(idx) > cap:\n",
        "        idx = rng.choice(idx, size=cap, replace=False)\n",
        "    Xs = X_val_s[idx]\n",
        "    y = model.predict_proba(Xs)[:,1:2]\n",
        "    y = _stdize(y)\n",
        "\n",
        "    def rbf(X, sigma=None):\n",
        "        if sigma is None:\n",
        "            d = np.sqrt(((X[None,:,:]-X[:,None,:])**2).sum(-1))\n",
        "            sigma = np.median(d) + 1e-12\n",
        "        G = ((X[None,:,:]-X[:,None,:])**2).sum(-1)\n",
        "        return np.exp(-G/(2*sigma**2))\n",
        "\n",
        "    n = len(idx)\n",
        "    Ky = rbf(y)\n",
        "    H = np.eye(n) - np.ones((n,n))/n\n",
        "    Kyc = H @ Ky @ H\n",
        "\n",
        "    out=[]\n",
        "    for j in range(Xs.shape[1]):\n",
        "        x = _stdize(Xs[:, j:j+1])\n",
        "        Kx = rbf(x)\n",
        "        Kxc = H @ Kx @ H\n",
        "        hsic = np.trace(Kxc @ Kyc) / (n**2)\n",
        "        out.append(hsic)\n",
        "    return pd.Series(out, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "def compute_sage_global(model, X_train_s, X_val_s, feat_names, seed=0, m_samples=2000):\n",
        "    if sage is None:\n",
        "        return None\n",
        "    try:\n",
        "        imputer = sage.PermutationImputer(model, X_train_s, random_state=seed)\n",
        "        estimator = sage.MarginalEstimator(imputer, loss=\"crossentropy\")\n",
        "        idx = np.arange(X_val_s.shape[0])\n",
        "        if len(idx) > m_samples:\n",
        "            rng = np.random.default_rng(seed)\n",
        "            idx = rng.choice(idx, size=m_samples, replace=False)\n",
        "        phi = estimator(X_val_s[idx], y=None)\n",
        "        vals = np.array(phi.values)\n",
        "        return pd.Series(vals, index=feat_names).sort_values(ascending=False)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def compute_mi_with_label(X_val_s, y_val, feat_names):\n",
        "    vals = mutual_info_classif(X_val_s, y_val, random_state=0)\n",
        "    return pd.Series(vals, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "def compute_surrogate_lr(model, X_fit_s, X_val_s, feat_names, C=1.0, seed=RANDOM_SEED):\n",
        "    # Fit LR to mimic model's probability outputs (teacher-student)\n",
        "    ysoft = model.predict_proba(X_fit_s)[:,1]\n",
        "    # Clamp to avoid 0/1 extremes before logit\n",
        "    eps = 1e-5\n",
        "    ysoft = np.clip(ysoft, eps, 1-eps)\n",
        "    z = np.log(ysoft/(1-ysoft))  # logit\n",
        "    lr = Ridge(alpha=1.0/C, random_state=seed).fit(X_fit_s, z)\n",
        "    coefs = np.abs(lr.coef_)\n",
        "    return pd.Series(coefs, index=feat_names).sort_values(ascending=False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Evaluation helpers\n",
        "# ------------------------------------------------------------\n",
        "def train_eval_on_features(feats, X_fit_s, y_fit, X_test_s, y_test, seed=RANDOM_SEED):\n",
        "    idxs = [feat_names.index(c) for c in feats]\n",
        "    m = GradientBoostingClassifier(random_state=seed).fit(X_fit_s[:, idxs], y_fit)\n",
        "    ypred = m.predict(X_test_s[:, idxs])\n",
        "    return accuracy_score(y_test, ypred)\n",
        "\n",
        "def topk_bootstrap_ci(rank_series, ks, pack, B=400, seed=0):\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    feat_names = pack[\"feat_names\"]  # avoid globals\n",
        "    # sanity: ensure ranking index are feature names\n",
        "    unknown = [f for f in rank_series.index if f not in feat_names]\n",
        "    if unknown:\n",
        "        raise ValueError(f\"Ranking contains unknown features (first few): {unknown[:5]}\")\n",
        "\n",
        "    results = []\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    for k in ks:\n",
        "        feats = rank_series.head(k).index.tolist()\n",
        "        idx_feats = [feat_names.index(c) for c in feats]\n",
        "\n",
        "        # fit once on the k features\n",
        "        m = GradientBoostingClassifier(random_state=seed).fit(\n",
        "            pack[\"X_fit_s\"][:, idx_feats], pack[\"y_fit\"]\n",
        "        )\n",
        "        ypred = m.predict(pack[\"X_test_s\"][:, idx_feats])\n",
        "\n",
        "        # make sure we are indexing NumPy arrays (positional)\n",
        "        yt = np.asarray(pack[\"y_test\"])\n",
        "        ypred = np.asarray(ypred)\n",
        "\n",
        "        # point estimate\n",
        "        acc = accuracy_score(yt, ypred)\n",
        "\n",
        "        # bootstrap CI\n",
        "        n = yt.shape[0]\n",
        "        boots = []\n",
        "        for _ in range(B):\n",
        "            b = rng.integers(0, n, n)  # positions, not labels\n",
        "            boots.append(accuracy_score(yt[b], ypred[b]))\n",
        "        lo, hi = np.percentile(boots, [2.5, 97.5])\n",
        "        results.append((k, acc, lo, hi))\n",
        "    return results\n",
        "\n",
        "\n",
        "def rank_to_vector(series, all_feats):\n",
        "    s = series.reindex(all_feats).fillna(0.0)\n",
        "    return s.values\n",
        "\n",
        "def plot_top_bar(series, title, path, topn=10):\n",
        "    s = series.head(topn)[::-1]\n",
        "    plt.figure(figsize=(7, 4.2))\n",
        "    plt.barh(s.index, s.values)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"score\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def plot_corr_heatmap(df_corr, title, path):\n",
        "    plt.figure(figsize=(7.5, 6.5))\n",
        "    im = plt.imshow(df_corr.values, interpolation='nearest')\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.xticks(range(df_corr.shape[1]), df_corr.columns, rotation=45, ha='right')\n",
        "    plt.yticks(range(df_corr.shape[0]), df_corr.index)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Main\n",
        "# ------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    t0 = time.time()\n",
        "    pack = make_synthetic_vehicular()\n",
        "    X_df, feat_names = pack[\"X_df\"], pack[\"feat_names\"]\n",
        "    scaler, model = pack[\"scaler\"], pack[\"model\"]\n",
        "\n",
        "    # --- Predictions for ExCIR ---\n",
        "    yhat_val = model.predict_proba(pack[\"X_val_s\"])[:,1]\n",
        "    rankings[\"KDE-MI(x,yhat)\"]   = compute_kde_mi_xy(pack[\"X_val_s\"], yhat_val, feat_names)\n",
        "    rankings[\"KL(low↔high ŷ)\"]  = compute_kl_conditional(yhat_val, pack[\"X_val_s\"], feat_names, q=0.3, bins=40)\n",
        "    rankings[\"W1(low↔high ŷ)\"]  = compute_wasserstein_conditional(yhat_val, pack[\"X_val_s\"], feat_names, q=0.3)\n",
        "\n",
        "    excir = compute_excir(pack[\"X_val_s\"], yhat_val, feat_names)\n",
        "\n",
        "    # --- Other rankings ---\n",
        "    rankings = {}\n",
        "\n",
        "    rankings[\"ExCIR\"] = excir\n",
        "    rankings[\"SHAP\"] = compute_shap_global(model, pack[\"X_val_s\"], feat_names)\n",
        "    rankings[\"LIME\"] = compute_lime_global(model, pack[\"X_train_s\"], pack[\"X_val_s\"], feat_names)\n",
        "    rankings[\"PFI\"] = compute_pfi(model, pack[\"X_test_s\"], pack[\"y_test\"], feat_names)\n",
        "    rankings[\"TreeGain\"] = compute_tree_gain(model, feat_names)\n",
        "    rankings[\"PDP-var\"] = compute_pdp_variance(model, pack[\"X_val_s\"], feat_names)\n",
        "    rankings[\"MI(pred)\"] = compute_mi_with_pred(model, pack[\"X_val_s\"], feat_names)\n",
        "    rankings[\"HSIC(pred)\"] = compute_hsic_pred(model, pack[\"X_val_s\"], feat_names)\n",
        "    rankings[\"SAGE\"] = compute_sage_global(model, pack[\"X_fit_s\"], pack[\"X_val_s\"], feat_names)\n",
        "    rankings[\"MI(label)\"] = compute_mi_with_label(pack[\"X_val_s\"], pack[\"y_val\"], feat_names)\n",
        "    rankings[\"Surrogate-LR\"] = compute_surrogate_lr(model, pack[\"X_fit_s\"], pack[\"X_val_s\"], feat_names)\n",
        "\n",
        "    # Remove Nones, ensure sorted\n",
        "    rankings = {k:v.sort_values(ascending=False) for k,v in rankings.items() if v is not None}\n",
        "\n",
        "    # --- Save per-method top-10 PNGs + full CSVs ---\n",
        "    for name, series in rankings.items():\n",
        "        series.to_csv(OUT / f\"ranking_{name}.csv\", header=[name])\n",
        "        plot_top_bar(series, f\"{name} — Top 10\", OUT / f\"ranking_{name}_top10.png\", topn=10)\n",
        "\n",
        "    # --- Correlation matrices across methods (Spearman & Kendall) ---\n",
        "    methods = list(rankings.keys())\n",
        "    M = len(methods)\n",
        "    score_mat = np.stack([rank_to_vector(rankings[m], feat_names) for m in methods], axis=1)\n",
        "    df_scores = pd.DataFrame(score_mat, index=feat_names, columns=methods)\n",
        "    df_scores.to_csv(OUT / \"all_method_scores.csv\")\n",
        "\n",
        "    # Spearman\n",
        "    spearman = np.zeros((M, M))\n",
        "    kendall = np.zeros((M, M))\n",
        "    for i in range(M):\n",
        "        for j in range(M):\n",
        "            r_s, _ = spearmanr(df_scores.iloc[:, i], df_scores.iloc[:, j])\n",
        "            r_k, _ = kendalltau(df_scores.iloc[:, i], df_scores.iloc[:, j])\n",
        "            spearman[i, j] = r_s\n",
        "            kendall[i, j] = r_k\n",
        "\n",
        "    df_spear = pd.DataFrame(spearman, index=methods, columns=methods)\n",
        "    df_kendall = pd.DataFrame(kendall, index=methods, columns=methods)\n",
        "    df_spear.to_csv(OUT / \"corr_spearman.csv\")\n",
        "    df_kendall.to_csv(OUT / \"corr_kendall.csv\")\n",
        "\n",
        "    plot_corr_heatmap(df_spear, \"Spearman correlation (scores)\", OUT / \"corr_spearman.png\")\n",
        "    plot_corr_heatmap(df_kendall, \"Kendall correlation (scores)\", OUT / \"corr_kendall.png\")\n",
        "\n",
        "    # --- Top-k subset tests with bootstrap CIs ---\n",
        "    ks = [3, 5, 8, 12]\n",
        "    rows = []\n",
        "    for name, series in rankings.items():\n",
        "        res = topk_bootstrap_ci(series, ks, pack, B=400, seed=RANDOM_SEED)\n",
        "        for (k, acc, lo, hi) in res:\n",
        "            rows.append({\"method\": name, \"k\": k, \"acc\": acc, \"ci_lo\": lo, \"ci_hi\": hi})\n",
        "    df_topk = pd.DataFrame(rows).sort_values([\"k\",\"acc\"], ascending=[True,False])\n",
        "    df_topk.to_csv(OUT / \"topk_bootstrap_acc.csv\", index=False)\n",
        "\n",
        "    # --- Small summary JSON ---\n",
        "    summary = {\n",
        "        \"n_features\": len(feat_names),\n",
        "        \"n_train\": int(pack[\"X_train\"].shape[0]),\n",
        "        \"n_val\": int(pack[\"X_val\"].shape[0]),\n",
        "        \"n_test\": int(pack[\"X_test\"].shape[0]),\n",
        "        \"model\": \"GradientBoostingClassifier\",\n",
        "        \"methods\": list(rankings.keys()),\n",
        "        \"runtime_sec\": round(time.time() - t0, 2),\n",
        "        \"cpu_count\": psutil.cpu_count(logical=True) if psutil else None,\n",
        "    }\n",
        "    with open(OUT / \"summary.json\", \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    # --- Quick console prints ---\n",
        "    print(\"=== Summary ===\")\n",
        "    print(json.dumps(summary, indent=2))\n",
        "    print(\"\\nTop-5 per method:\")\n",
        "    for name, s in rankings.items():\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(s.head(5))\n",
        "\n",
        "    print(\"\\nTop-k bootstrap accuracy (mean & 95% CI):\")\n",
        "    print(df_topk.to_string(index=False))\n",
        "\n",
        "    print(f\"\\nSaved outputs in: {OUT.resolve()}\")\n"
      ],
      "metadata": {
        "id": "Z2B86d7_xICD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_topk_overview.py\n",
        "# One-graph visualization of top-k subset accuracy with 95% CIs\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "IN  = Path(\"out/topk_bootstrap_acc.csv\")\n",
        "OUT = Path(\"out\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(IN)\n",
        "df[\"k\"] = df[\"k\"].astype(int)\n",
        "\n",
        "# Optional: choose which methods to show (keep None to show all)\n",
        "# methods_to_plot = [\"PFI\",\"ExCIR\",\"SHAP\",\"MI(pred)\",\"TreeGain\",\"PDP-var\",\"Surrogate-LR\",\"MI(label)\",\"HSIC(pred)\"]\n",
        "methods_to_plot = None\n",
        "\n",
        "if methods_to_plot is not None:\n",
        "    df = df[df[\"method\"].isin(methods_to_plot)].copy()\n",
        "\n",
        "# Order methods by mean accuracy across ks (descending)\n",
        "order = (\n",
        "    df.groupby(\"method\")[\"acc\"]\n",
        "      .mean()\n",
        "      .sort_values(ascending=False)\n",
        "      .index\n",
        "      .tolist()\n",
        ")\n",
        "\n",
        "ks = sorted(df[\"k\"].unique())\n",
        "\n",
        "plt.figure(figsize=(9.2, 5.2))\n",
        "\n",
        "for m in order:\n",
        "    sub = df[df[\"method\"] == m].sort_values(\"k\")\n",
        "    y  = sub[\"acc\"].values\n",
        "    x  = sub[\"k\"].values\n",
        "    lo = sub[\"ci_lo\"].values\n",
        "    hi = sub[\"ci_hi\"].values\n",
        "    yerr = np.vstack([y - lo, hi - y])\n",
        "\n",
        "    # connected line + CI whiskers\n",
        "    plt.errorbar(x, y, yerr=yerr, marker=\"o\", capsize=3, linewidth=1.8, label=m)\n",
        "\n",
        "# Annotate the winner at each k\n",
        "for k in ks:\n",
        "    block = df[df[\"k\"] == k]\n",
        "    best_row = block.loc[block[\"acc\"].idxmax()]\n",
        "    plt.scatter([k], [best_row[\"acc\"]], s=90, marker=\"*\", zorder=5)\n",
        "    plt.text(k, best_row[\"acc\"] + 0.003, f\"{best_row['method']}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
        "\n",
        "plt.xticks(ks)\n",
        "plt.xlabel(\"Top-k features\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Top-k subset accuracy with 95% CIs (lower is whisker, upper is whisker)\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend(ncol=3, frameon=False, fontsize=9, loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(OUT / \"topk_overview.png\", dpi=200)\n",
        "plt.savefig(OUT / \"topk_overview.pdf\")\n",
        "print(f\"Saved: {OUT / 'topk_overview.png'} and {OUT / 'topk_overview.pdf'}\")\n"
      ],
      "metadata": {
        "id": "Q2XQJiJUxNhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# more_graphs.py\n",
        "# Extra visualizations for ExCIR vs baselines\n",
        "# Requires files produced by your main run:\n",
        "#   out/topk_bootstrap_acc.csv, out/all_method_scores.csv, ranking_*.csv,\n",
        "#   corr_spearman.csv, corr_kendall.csv (optional but recommended)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "OUT = Path(\"out\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def load_topk():\n",
        "    df = pd.read_csv(OUT / \"topk_bootstrap_acc.csv\")\n",
        "    df[\"k\"] = df[\"k\"].astype(int)\n",
        "    return df\n",
        "\n",
        "def stable_ranks(df):\n",
        "    # rank methods by accuracy within each k (1 = best).\n",
        "    rows = []\n",
        "    for k, block in df.groupby(\"k\"):\n",
        "        # sort by acc desc, then method name for deterministic ties\n",
        "        block = block.sort_values([\"acc\",\"method\"], ascending=[False, True]).reset_index(drop=True)\n",
        "        block[\"rank\"] = np.arange(1, len(block)+1)\n",
        "        rows.append(block)\n",
        "    return pd.concat(rows, axis=0, ignore_index=True)\n",
        "\n",
        "def load_rank_series(method):\n",
        "    p = OUT / f\"ranking_{method}.csv\"\n",
        "    if p.exists():\n",
        "        s = pd.read_csv(p, index_col=0, header=0).iloc[:,0]\n",
        "        s = s.sort_values(ascending=False)\n",
        "        s.name = method\n",
        "        return s\n",
        "    return None\n",
        "\n",
        "def get_methods_from_topk(df):\n",
        "    return sorted(df[\"method\"].unique().tolist())\n",
        "\n",
        "def jaccard_topk(methods, k):\n",
        "    # build top-k sets and compute Jaccard similarity\n",
        "    tops = {}\n",
        "    for m in methods:\n",
        "        s = load_rank_series(m)\n",
        "        if s is not None:\n",
        "            tops[m] = set(s.head(k).index.tolist())\n",
        "    methods_ok = sorted(tops.keys())\n",
        "    n = len(methods_ok)\n",
        "    J = np.zeros((n, n))\n",
        "    for i, mi in enumerate(methods_ok):\n",
        "        for j, mj in enumerate(methods_ok):\n",
        "            a, b = tops[mi], tops[mj]\n",
        "            inter = len(a & b)\n",
        "            uni = len(a | b) if len(a | b) > 0 else 1\n",
        "            J[i, j] = inter / uni\n",
        "    return methods_ok, J\n",
        "\n",
        "def presence_matrix(methods, k):\n",
        "    # rows = features (top by frequency), cols = methods, entries = {0,1}\n",
        "    feats = []\n",
        "    cols = []\n",
        "    mats = []\n",
        "    for m in methods:\n",
        "        s = load_rank_series(m)\n",
        "        if s is None:\n",
        "            continue\n",
        "        cols.append(m)\n",
        "        feats.append(s.head(k).index.tolist())\n",
        "    if not cols:\n",
        "        return None, None, None\n",
        "    all_feats = sorted(set(f for lst in feats for f in lst))\n",
        "    M = np.zeros((len(all_feats), len(cols)))\n",
        "    for j, m in enumerate(cols):\n",
        "        s = load_rank_series(m)\n",
        "        topk = set(s.head(k).index.tolist())\n",
        "        for i, f in enumerate(all_feats):\n",
        "            M[i, j] = 1.0 if f in topk else 0.0\n",
        "    freq = M.sum(axis=1)\n",
        "    order = np.argsort(-freq)  # by frequency desc\n",
        "    return [all_feats[i] for i in order], cols, M[order]\n",
        "\n",
        "def _save(figpath):\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figpath, dpi=200)\n",
        "    print(f\"Saved: {figpath}\")\n",
        "\n",
        "# ---------- 1) Bump chart: rank vs k ----------\n",
        "df = load_topk()\n",
        "df_ranked = stable_ranks(df)\n",
        "\n",
        "plt.figure(figsize=(9, 5))\n",
        "methods_order = (df.groupby(\"method\")[\"acc\"].mean().sort_values(ascending=False).index.tolist())\n",
        "ks = sorted(df[\"k\"].unique())\n",
        "for m in methods_order:\n",
        "    sub = df_ranked[df_ranked[\"method\"] == m].sort_values(\"k\")\n",
        "    if sub.empty:\n",
        "        continue\n",
        "    plt.plot(sub[\"k\"].values, sub[\"rank\"].values, marker=\"o\", linewidth=1.5, label=m)\n",
        "plt.gca().invert_yaxis()  # rank 1 at top\n",
        "plt.xticks(ks)\n",
        "plt.yticks(range(1, df[\"method\"].nunique()+1))\n",
        "plt.xlabel(\"Top-k features\")\n",
        "plt.ylabel(\"Rank (1 = best)\")\n",
        "plt.title(\"Bump chart: method rank vs. k\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend(ncol=3, frameon=False, fontsize=9, loc=\"upper right\")\n",
        "_save(OUT / \"bump_chart.png\")\n",
        "\n",
        "# ---------- 2) Jaccard heatmap of top-k sets (k=5) ----------\n",
        "methods = get_methods_from_topk(df)\n",
        "methods5, J5 = jaccard_topk(methods, k=5)\n",
        "plt.figure(figsize=(7.5, 6.2))\n",
        "im = plt.imshow(J5, interpolation=\"nearest\")\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "plt.xticks(range(len(methods5)), methods5, rotation=45, ha=\"right\")\n",
        "plt.yticks(range(len(methods5)), methods5)\n",
        "plt.title(\"Jaccard similarity of top-5 sets (higher = more overlap)\")\n",
        "_save(OUT / \"jaccard_top5.png\")\n",
        "\n",
        "# ---------- 3) Jaccard heatmap of top-k sets (k=12) ----------\n",
        "methods12, J12 = jaccard_topk(methods, k=12)\n",
        "plt.figure(figsize=(7.5, 6.2))\n",
        "im = plt.imshow(J12, interpolation=\"nearest\")\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "plt.xticks(range(len(methods12)), methods12, rotation=45, ha=\"right\")\n",
        "plt.yticks(range(len(methods12)), methods12)\n",
        "plt.title(\"Jaccard similarity of top-12 sets (higher = more overlap)\")\n",
        "_save(OUT / \"jaccard_top12.png\")\n",
        "\n",
        "# ---------- 4) Feature presence heatmap (top features by frequency, k=8) ----------\n",
        "feats, cols, M = presence_matrix(methods, k=8)\n",
        "if feats is not None:\n",
        "    topn = min(15, len(feats))\n",
        "    plt.figure(figsize=(8.6, 6.8))\n",
        "    im = plt.imshow(M[:topn, :], aspect=\"auto\", interpolation=\"nearest\")\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.yticks(range(topn), feats[:topn])\n",
        "    plt.xticks(range(len(cols)), cols, rotation=45, ha=\"right\")\n",
        "    plt.title(\"Presence of features in top-8 sets (1 = present, 0 = absent)\")\n",
        "    _save(OUT / \"feature_presence_top8.png\")\n",
        "\n",
        "# ---------- 5) Correlation heatmaps (reload and label nicely) ----------\n",
        "for name, title in [(\"corr_spearman.csv\", \"Spearman correlation (method scores)\"),\n",
        "                    (\"corr_kendall.csv\",  \"Kendall correlation (method scores)\")]:\n",
        "    p = OUT / name\n",
        "    if p.exists():\n",
        "        C = pd.read_csv(p, index_col=0)\n",
        "        plt.figure(figsize=(7.5, 6.5))\n",
        "        im = plt.imshow(C.values, interpolation=\"nearest\")\n",
        "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "        plt.xticks(range(C.shape[1]), C.columns, rotation=45, ha=\"right\")\n",
        "        plt.yticks(range(C.shape[0]), C.index)\n",
        "        plt.title(title)\n",
        "        _save(OUT / (name.replace(\".csv\", \"_labeled.png\")))\n",
        "\n",
        "# ---------- 6) Radar chart (optional): accuracy normalized per k ----------\n",
        "# Only top 5 methods by mean accuracy to keep it readable\n",
        "methods_sorted = (df.groupby(\"method\")[\"acc\"].mean().sort_values(ascending=False).index.tolist())\n",
        "sel = methods_sorted[:5]\n",
        "ks = sorted(df[\"k\"].unique())\n",
        "angles = np.linspace(0, 2*np.pi, len(ks), endpoint=False)\n",
        "angles = np.concatenate([angles, angles[:1]])  # close the loop\n",
        "\n",
        "plt.figure(figsize=(6.8, 6.8))\n",
        "ax = plt.subplot(111, polar=True)\n",
        "for m in sel:\n",
        "    sub = df[df[\"method\"] == m].sort_values(\"k\")\n",
        "    # min-max normalize accuracy across k (so polygons are comparable in shape)\n",
        "    a = sub[\"acc\"].values.astype(float)\n",
        "    a_norm = (a - a.min()) / (a.max() - a.min() + 1e-12)\n",
        "    vals = np.concatenate([a_norm, a_norm[:1]])\n",
        "    ax.plot(angles, vals, marker=\"o\", linewidth=1.5, label=m)\n",
        "ax.set_thetagrids(angles[:-1] * 180/np.pi, [f\"k={k}\" for k in ks])\n",
        "ax.set_title(\"Normalized accuracy per k (top 5 methods)\")\n",
        "ax.set_rlim(0.0, 1.0)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.25, 1.05), frameon=False, fontsize=9)\n",
        "_save(OUT / \"radar_top5.png\")\n"
      ],
      "metadata": {
        "id": "AhT-908PxQVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sota_plus_analysis.py\n",
        "# Extra graphs for SOTA comparison on the synthetic vehicular run (dataset-agnostic).\n",
        "# Reads the CSVs your main script already created.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "OUT = Path(\"out\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------------\n",
        "# helpers\n",
        "# ---------------------\n",
        "def load_topk():\n",
        "    df = pd.read_csv(OUT / \"topk_bootstrap_acc.csv\")\n",
        "    df[\"k\"] = df[\"k\"].astype(int)\n",
        "    return df\n",
        "\n",
        "def load_rank_series(method):\n",
        "    p = OUT / f\"ranking_{method}.csv\"\n",
        "    if not p.exists():\n",
        "        return None\n",
        "    s = pd.read_csv(p, index_col=0, header=0).iloc[:, 0]\n",
        "    return s.sort_values(ascending=False)\n",
        "\n",
        "def methods_available():\n",
        "    df = load_topk()\n",
        "    ms = sorted(df[\"method\"].unique().tolist())\n",
        "    # keep only methods that have ranking CSVs (needed for some plots)\n",
        "    ms_csv = [m for m in ms if (OUT / f\"ranking_{m}.csv\").exists()]\n",
        "    return ms, ms_csv\n",
        "\n",
        "def _save(figpath):\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figpath, dpi=200)\n",
        "    print(f\"Saved: {figpath}\")\n",
        "\n",
        "# ---------------------\n",
        "# 1) Delta-to-best line plot (gap to best accuracy at each k)\n",
        "# ---------------------\n",
        "def plot_delta_to_best():\n",
        "    df = load_topk()\n",
        "    ks = sorted(df[\"k\"].unique())\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "\n",
        "    for m in sorted(df[\"method\"].unique()):\n",
        "        sub = df[df[\"method\"] == m].sort_values(\"k\")\n",
        "        gaps = []\n",
        "        for k in ks:\n",
        "            block = df[df[\"k\"] == k]\n",
        "            best = block[\"acc\"].max()\n",
        "            val = float(block[block[\"method\"] == m][\"acc\"])\n",
        "            gaps.append(best - val)\n",
        "        plt.plot(ks, gaps, marker=\"o\", linewidth=1.8, label=m)\n",
        "\n",
        "    plt.xticks(ks)\n",
        "    plt.xlabel(\"Top-k features\")\n",
        "    plt.ylabel(\"Accuracy gap to best\")\n",
        "    plt.title(\"Gap to best accuracy at each k (lower is better)\")\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.legend(ncol=3, frameon=False, fontsize=9, loc=\"upper left\")\n",
        "    _save(OUT / \"delta_to_best.png\")\n",
        "\n",
        "# ---------------------\n",
        "# 2) Win/tie counts across k (bar chart)\n",
        "# ---------------------\n",
        "def plot_win_counts():\n",
        "    df = load_topk()\n",
        "    ks = sorted(df[\"k\"].unique())\n",
        "    wins = dict((m, 0.0) for m in df[\"method\"].unique())\n",
        "    for k in ks:\n",
        "        block = df[df[\"k\"] == k]\n",
        "        best = block[\"acc\"].max()\n",
        "        winners = block[np.isclose(block[\"acc\"], best)][\"method\"].tolist()\n",
        "        share = 1.0 / len(winners)\n",
        "        for m in winners:\n",
        "            wins[m] += share\n",
        "\n",
        "    methods = sorted(wins.keys(), key=lambda m: -wins[m])\n",
        "    vals = [wins[m] for m in methods]\n",
        "\n",
        "    plt.figure(figsize=(8.5, 5.0))\n",
        "    y = np.arange(len(methods))\n",
        "    plt.barh(y, vals)\n",
        "    plt.yticks(y, methods)\n",
        "    plt.xlabel(\"Wins (ties split equally) over k\")\n",
        "    plt.title(\"Win/tie counts across feature budgets\")\n",
        "    _save(OUT / \"win_counts.png\")\n",
        "\n",
        "# ---------------------\n",
        "# 3) Oracle enrichment curve (recall of “true” factors vs k)\n",
        "#    Define an oracle set for this synthetic vehicular generator.\n",
        "#    (Works for other datasets if you redefine ORACLE accordingly.)\n",
        "# ---------------------\n",
        "ORACLE = {\n",
        "    \"speed_kph\", \"brake\", \"steering_deg\", \"yaw_rate\",\n",
        "    \"engine_load\", \"road_grade\", \"battery_v\",\n",
        "    \"tire_fl\", \"tire_fr\", \"tire_rl\", \"tire_rr\"\n",
        "}\n",
        "\n",
        "def plot_oracle_enrichment(kmax=12):\n",
        "    _, ms_csv = methods_available()\n",
        "    if not ms_csv:\n",
        "        return\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    ks = list(range(1, kmax + 1))\n",
        "    for m in ms_csv:\n",
        "        s = load_rank_series(m)\n",
        "        if s is None:\n",
        "            continue\n",
        "        recalls = []\n",
        "        for k in ks:\n",
        "            topk = set(s.head(k).index.tolist())\n",
        "            recalls.append(len(topk & ORACLE) / max(1, len(ORACLE)))\n",
        "        plt.plot(ks, recalls, marker=\"o\", linewidth=1.8, label=m)\n",
        "    plt.xticks(ks)\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    plt.xlabel(\"Top-k features\")\n",
        "    plt.ylabel(\"Recall of oracle set\")\n",
        "    plt.title(\"Oracle enrichment: recall of ground-truth factors vs k\")\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.legend(ncol=3, frameon=False, fontsize=9, loc=\"lower right\")\n",
        "    _save(OUT / \"oracle_enrichment.png\")\n",
        "\n",
        "# ---------------------\n",
        "# 4) Consensus-rank heatmap (features × methods, lower rank = darker)\n",
        "#    Shows rank agreement on the most influential features.\n",
        "# ---------------------\n",
        "def plot_consensus_rank_heatmap(topn_features=15):\n",
        "    _, ms_csv = methods_available()\n",
        "    if not ms_csv:\n",
        "        return\n",
        "    # build rank table\n",
        "    frames = []\n",
        "    for m in ms_csv:\n",
        "        s = load_rank_series(m)\n",
        "        if s is None:\n",
        "            continue\n",
        "        # ranks: 1 = best\n",
        "        r = pd.Series(np.arange(1, len(s) + 1, dtype=float), index=s.index, name=m)\n",
        "        frames.append(r)\n",
        "    R = pd.concat(frames, axis=1)\n",
        "    # average rank to pick prominent features\n",
        "    mean_rank = R.mean(axis=1)\n",
        "    top_feats = mean_rank.nsmallest(min(topn_features, len(mean_rank))).index.tolist()\n",
        "    Rtop = R.loc[top_feats]\n",
        "\n",
        "    plt.figure(figsize=(8.8, 6.8))\n",
        "    im = plt.imshow(Rtop.values, aspect=\"auto\", interpolation=\"nearest\")\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.yticks(range(Rtop.shape[0]), Rtop.index)\n",
        "    plt.xticks(range(Rtop.shape[1]), Rtop.columns, rotation=45, ha=\"right\")\n",
        "    plt.title(\"Consensus ranks (1 = top) across methods\")\n",
        "    _save(OUT / \"consensus_rank_heatmap.png\")\n",
        "\n",
        "# ---------------------\n",
        "# 5) ExCIR-vs-others overlap curve (Jaccard vs k)\n",
        "# ---------------------\n",
        "def plot_excir_overlap(kmax=12):\n",
        "    _, ms_csv = methods_available()\n",
        "    if \"ExCIR\" not in ms_csv:\n",
        "        return\n",
        "    s_ex = load_rank_series(\"ExCIR\")\n",
        "    if s_ex is None:\n",
        "        return\n",
        "    ks = list(range(1, kmax + 1))\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    for m in ms_csv:\n",
        "        if m == \"ExCIR\":\n",
        "            continue\n",
        "        s = load_rank_series(m)\n",
        "        if s is None:\n",
        "            continue\n",
        "        jac = []\n",
        "        for k in ks:\n",
        "            a = set(s_ex.head(k).index.tolist())\n",
        "            b = set(s.head(k).index.tolist())\n",
        "            inter = len(a & b)\n",
        "            uni = len(a | b) if len(a | b) > 0 else 1\n",
        "            jac.append(inter / uni)\n",
        "        plt.plot(ks, jac, marker=\"o\", linewidth=1.8, label=m)\n",
        "    plt.xticks(ks)\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    plt.xlabel(\"Top-k features\")\n",
        "    plt.ylabel(\"Jaccard w.r.t. ExCIR\")\n",
        "    plt.title(\"ExCIR overlap with other methods (higher = more similar)\")\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.legend(ncol=3, frameon=False, fontsize=9, loc=\"lower right\")\n",
        "    _save(OUT / \"excir_overlap.png\")\n",
        "\n",
        "# ---------------------\n",
        "# 6) Violin-esque distribution per k (reconstruct from CI by sampling)\n",
        "#     Note: This is illustrative (reconstructs bootstrap distributions from mean+CI).\n",
        "# ---------------------\n",
        "def plot_violin_like_from_ci():\n",
        "    df = load_topk().copy()\n",
        "    ks = sorted(df[\"k\"].unique())\n",
        "    # Sample synthetic distributions per (method, k) matching mean and 95% CI width.\n",
        "    # Assumes approx normal; for illustration only.\n",
        "    rows = []\n",
        "    for (_, row) in df.iterrows():\n",
        "        m, k, acc, lo, hi = row[\"method\"], int(row[\"k\"]), row[\"acc\"], row[\"ci_lo\"], row[\"ci_hi\"]\n",
        "        sigma = (hi - lo) / (2 * 1.96 + 1e-12)\n",
        "        samples = np.random.normal(loc=acc, scale=max(sigma, 1e-6), size=400)\n",
        "        for s in samples:\n",
        "            rows.append({\"method\": m, \"k\": k, \"acc_samp\": s})\n",
        "    sim = pd.DataFrame(rows)\n",
        "\n",
        "    for k in ks:\n",
        "        block = sim[sim[\"k\"] == k]\n",
        "        methods = sorted(block[\"method\"].unique())\n",
        "        plt.figure(figsize=(9.2, 5.2))\n",
        "        positions = np.arange(1, len(methods) + 1)\n",
        "        data = [block[block[\"method\"] == m][\"acc_samp\"].values for m in methods]\n",
        "        plt.violinplot(data, positions=positions, showmeans=True, showextrema=False)\n",
        "        plt.xticks(positions, methods, rotation=45, ha=\"right\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.title(f\"Synthetic bootstrap distributions per method (k={k})\")\n",
        "        plt.grid(True, axis=\"y\", alpha=0.25)\n",
        "        _save(OUT / f\"boot_violin_k{k}.png\")\n",
        "\n",
        "# ---------------------\n",
        "# run all\n",
        "# ---------------------\n",
        "if __name__ == \"__main__\":\n",
        "    plot_delta_to_best()\n",
        "    plot_win_counts()\n",
        "    plot_oracle_enrichment(kmax=12)\n",
        "    plot_consensus_rank_heatmap(topn_features=15)\n",
        "    plot_excir_overlap(kmax=12)\n",
        "    plot_violin_like_from_ci()\n"
      ],
      "metadata": {
        "id": "XgxXJf-mxTts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# excir_cats_dogs.py\n",
        "# ExCIR experiments on TensorFlow Datasets \"cats_vs_dogs\"\n",
        "# - Trains a compact CNN (binary: cat vs dog)\n",
        "# - Computes class-conditioned ExCIR at pixel level (grayscale, 64x64)\n",
        "# - Optional baselines: MI(pred) (pixel-level), PFI (patch-level 8x8)\n",
        "# - AOPC-style insertion/deletion curves using ExCIR ranking\n",
        "# - Saves figures to ./out\n",
        "\n",
        "import os, math, json, time, warnings, argparse\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--img_size\", type=int, default=64, help=\"square side for resizing images\")\n",
        "parser.add_argument(\"--batch\", type=int, default=64)\n",
        "parser.add_argument(\"--epochs\", type=int, default=3)\n",
        "parser.add_argument(\"--fast_train_cap\", type=int, default=12000, help=\"maximum training images (None for all)\")\n",
        "parser.add_argument(\"--fast_val_cap\", type=int, default=4000, help=\"maximum validation images\")\n",
        "parser.add_argument(\"--fast_test_cap\", type=int, default=4000, help=\"maximum test images\")\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "parser.add_argument(\"--compute_mi\", action=\"store_true\", help=\"compute MI(pred) per pixel (can take a few minutes)\")\n",
        "parser.add_argument(\"--compute_pfi\", action=\"store_true\", help=\"compute patch-level PFI (slowish but OK)\")\n",
        "args = parser.parse_args() if \"__file__\" in globals() else parser.parse_args([])\n",
        "\n",
        "OUTDIR = \"out\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Imports (TF, TFDS, numpy, etc.)\n",
        "# -----------------------------\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_selection import mutual_info_regression\n",
        "    HAVE_SKLEARN_MI = True\n",
        "except Exception:\n",
        "    HAVE_SKLEARN_MI = False\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "tf.random.set_seed(args.seed)\n",
        "\n",
        "# -----------------------------\n",
        "# Data: cats_vs_dogs (TFDS)\n",
        "# Splits: 80/10/10\n",
        "# Preprocess: resize -> grayscale -> [0,1]\n",
        "# -----------------------------\n",
        "IMG = args.img_size\n",
        "\n",
        "def _preprocess(ex):\n",
        "    img = tf.image.resize(ex[\"image\"], [IMG, IMG])\n",
        "    img = tf.image.rgb_to_grayscale(img)  # (H,W,1)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    y = tf.cast(ex[\"label\"], tf.int32)  # 0=cat, 1=dog\n",
        "    return img, y\n",
        "\n",
        "print(\"Loading TFDS cats_vs_dogs...\")\n",
        "ds_train = tfds.load(\"cats_vs_dogs\", split=\"train[:80%]\", as_supervised=False)\n",
        "ds_val   = tfds.load(\"cats_vs_dogs\", split=\"train[80%:90%]\", as_supervised=False)\n",
        "ds_test  = tfds.load(\"cats_vs_dogs\", split=\"train[90%:]\", as_supervised=False)\n",
        "\n",
        "ds_train = ds_train.map(_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_val   = ds_val.map(_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test  = ds_test.map(_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "if args.fast_train_cap:\n",
        "    ds_train = ds_train.take(args.fast_train_cap)\n",
        "if args.fast_val_cap:\n",
        "    ds_val = ds_val.take(args.fast_val_cap)\n",
        "if args.fast_test_cap:\n",
        "    ds_test = ds_test.take(args.fast_test_cap)\n",
        "\n",
        "ds_train = ds_train.shuffle(8192, seed=args.seed, reshuffle_each_iteration=True).batch(args.batch).prefetch(tf.data.AUTOTUNE)\n",
        "ds_val_b = ds_val.batch(args.batch).prefetch(tf.data.AUTOTUNE)\n",
        "ds_test_b= ds_test.batch(args.batch).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# -----------------------------\n",
        "# Build a compact CNN\n",
        "# -----------------------------\n",
        "def build_model(img_size):\n",
        "    inputs = tf.keras.Input(shape=(img_size, img_size, 1))\n",
        "    x = tf.keras.layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
        "    x = tf.keras.layers.MaxPool2D()(x)\n",
        "    x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = tf.keras.layers.MaxPool2D()(x)\n",
        "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)  # dog prob\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = build_model(IMG)\n",
        "model.summary()\n",
        "\n",
        "# -----------------------------\n",
        "# Train\n",
        "# -----------------------------\n",
        "print(\"Training...\")\n",
        "t0 = time.time()\n",
        "hist = model.fit(ds_train, validation_data=ds_val_b, epochs=args.epochs, verbose=1)\n",
        "train_time = time.time()-t0\n",
        "print(f\"Train time: {train_time:.1f}s\")\n",
        "\n",
        "# -----------------------------\n",
        "# Collect arrays for val/test\n",
        "# -----------------------------\n",
        "def collect(ds_batched):\n",
        "    Xs, ys = [], []\n",
        "    for xb, yb in ds_batched:\n",
        "        Xs.append(xb.numpy())  # (B,H,W,1)\n",
        "        ys.append(yb.numpy())\n",
        "    X = np.concatenate(Xs, axis=0)\n",
        "    y = np.concatenate(ys, axis=0).astype(np.int32)\n",
        "    return X, y\n",
        "\n",
        "X_val, y_val = collect(ds_val_b)\n",
        "X_test, y_test = collect(ds_test_b)\n",
        "\n",
        "# Predict probabilities\n",
        "p_val = model.predict(X_val, verbose=0).ravel()  # prob(dog)\n",
        "p_test= model.predict(X_test, verbose=0).ravel()\n",
        "\n",
        "def bin_acc(y_true, p, thr=0.5):\n",
        "    return float(np.mean((p>=thr).astype(int)==y_true))\n",
        "\n",
        "print(f\"Val acc={bin_acc(y_val,p_val):.3f}, AUROC={roc_auc_score(y_val,p_val):.3f}\")\n",
        "print(f\"Test acc={bin_acc(y_test,p_test):.3f}, AUROC={roc_auc_score(y_test,p_test):.3f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# ExCIR (pixel-level) on validation set\n",
        "# Use TRAIN stats to standardize pixels (as in our tabular setup)\n",
        "# -----------------------------\n",
        "def train_pixel_stats(ds_batched):\n",
        "    # One pass to accumulate per-pixel mean/var\n",
        "    cnt = 0\n",
        "    m = np.zeros((IMG,IMG,1), dtype=np.float64)\n",
        "    m2= np.zeros((IMG,IMG,1), dtype=np.float64)\n",
        "    for xb, _ in ds_batched:\n",
        "        xb_np = xb.numpy()  # (B,H,W,1) in [0,1]\n",
        "        cnt += xb_np.shape[0]\n",
        "        m  += xb_np.sum(axis=0)\n",
        "        m2 += (xb_np**2).sum(axis=0)\n",
        "    mean = m / cnt\n",
        "    var  = np.maximum(m2/cnt - mean**2, 1e-12)\n",
        "    std  = np.sqrt(var)\n",
        "    return mean.astype(np.float32), std.astype(np.float32)\n",
        "\n",
        "train_mean, train_std = train_pixel_stats(ds_train.unbatch().batch(args.batch))\n",
        "\n",
        "Xv = X_val.copy()\n",
        "Xv_std = (Xv - train_mean) / train_std  # shape (n,H,W,1)\n",
        "n_val = Xv_std.shape[0]\n",
        "\n",
        "# Flatten pixels -> features (n, d)\n",
        "Xv_flat = Xv_std.reshape(n_val, -1)  # d = IMG*IMG\n",
        "\n",
        "def excir_from_features(X_flat, p):\n",
        "    \"\"\"\n",
        "    Compute ExCIR per feature against scalar p (prob of dog).\n",
        "    Using pooled-mean ANOVA-style ratio (closed form), vectorized.\n",
        "    \"\"\"\n",
        "    n = X_flat.shape[0]\n",
        "    mean_x = X_flat.mean(axis=0)            # (d,)\n",
        "    var_x  = X_flat.var(axis=0)             # (d,)\n",
        "    mean_p = float(p.mean())\n",
        "    var_p  = float(p.var())\n",
        "    # Between-term: n/2 * (mean_x - mean_p)^2\n",
        "    num = 0.5 * (mean_x - mean_p)**2\n",
        "    den = var_x + var_p + num\n",
        "    cir = num / den\n",
        "    return cir  # (d,)\n",
        "\n",
        "print(\"Computing ExCIR (pixel-level)...\")\n",
        "excir_pix = excir_from_features(Xv_flat, p_val)  # shape (H*W,)\n",
        "excir_map = excir_pix.reshape(IMG,IMG)\n",
        "\n",
        "# Save heatmap\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(excir_map, vmin=0, vmax=excir_map.max())\n",
        "plt.title(\"ExCIR heatmap (prob(dog))\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"excir_heatmap_dog.png\"), dpi=160)\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Optional baseline: MI(pred) per pixel\n",
        "# -----------------------------\n",
        "mi_map = None\n",
        "if args.compute_mi:\n",
        "    if not HAVE_SKLEARN_MI:\n",
        "        print(\"sklearn not available; skipping MI.\")\n",
        "    else:\n",
        "        print(\"Computing MI(pred) per pixel (may take a few minutes)...\")\n",
        "        # sklearn's MI expects finite float arrays\n",
        "        mi = mutual_info_regression(Xv_flat, p_val, random_state=args.seed)\n",
        "        mi_map = mi.reshape(IMG,IMG)\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(mi_map, vmin=0, vmax=mi_map.max())\n",
        "        plt.title(\"MI(pred) heatmap (prob(dog))\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTDIR, \"mi_heatmap_dog.png\"), dpi=160)\n",
        "        plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Optional baseline: patch-level PFI (8x8 grid)\n",
        "# Permute each patch across images, measure drop in accuracy on VAL\n",
        "# -----------------------------\n",
        "def patch_slices(H, W, gh=8, gw=8):\n",
        "    hs, ws = H//gh, W//gw\n",
        "    regions=[]\n",
        "    for i in range(gh):\n",
        "        for j in range(gw):\n",
        "            r = slice(i*hs, (i+1)*hs), slice(j*ws, (j+1)*ws)\n",
        "            regions.append(r)\n",
        "    return regions\n",
        "\n",
        "pfi_grid = None\n",
        "if args.compute_pfi:\n",
        "    print(\"Computing patch-level PFI on validation set...\")\n",
        "    regions = patch_slices(IMG, IMG, gh=8, gw=8)  # 64 patches\n",
        "    base_acc = bin_acc(y_val, p_val)\n",
        "    pfi_scores = []\n",
        "    Xv_copy = X_val.copy()\n",
        "    for (rs, cs) in regions:\n",
        "        X_perm = Xv_copy.copy()\n",
        "        # Permute this patch across images (shuffle along batch)\n",
        "        idx = np.arange(n_val)\n",
        "        np.random.shuffle(idx)\n",
        "        X_perm[:, rs, cs, 0] = X_perm[idx][:, rs, cs, 0]\n",
        "        p_perm = model.predict(X_perm, verbose=0).ravel()\n",
        "        drop = base_acc - bin_acc(y_val, p_perm)\n",
        "        pfi_scores.append(max(0.0, drop))\n",
        "    pfi_grid = np.array(pfi_scores).reshape(8,8)\n",
        "\n",
        "    # Upsample to image size for visualization\n",
        "    pfi_big = np.kron(pfi_grid, np.ones((IMG//8, IMG//8)))\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(pfi_big, vmin=0, vmax=pfi_big.max())\n",
        "    plt.title(\"PFI (patch-level, 8x8)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTDIR, \"pfi_patch_heatmap.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# AOPC-style insertion/deletion using ExCIR ranking\n",
        "# We keep/remove top-q% pixels and evaluate on TEST\n",
        "# -----------------------------\n",
        "print(\"Computing AOPC insertion/deletion curves (ExCIR ranking)...\")\n",
        "# Rank pixels by ExCIR (desc)\n",
        "rank = np.argsort(-excir_pix)  # indices of H*W\n",
        "def mask_images(X, keep_frac, rank_idx):\n",
        "    \"\"\"Keep the top keep_frac of pixels; others set to mean gray.\"\"\"\n",
        "    B,H,W,1 == X.shape\n",
        "    k = int(keep_frac * H*W)\n",
        "    keep_set = set(rank_idx[:k].tolist())\n",
        "    X_out = X.copy()\n",
        "    mean_gray = float(X_out.mean())  # dataset mean as baseline\n",
        "    # Create mask flattened then reshape for vectorized apply\n",
        "    mask_flat = np.zeros(H*W, dtype=bool)\n",
        "    if k>0:\n",
        "        mask_flat[list(keep_set)] = True\n",
        "    mask = mask_flat.reshape(H,W)\n",
        "    # Broadcast: where mask==False, set to mean_gray\n",
        "    X_out[:, ~mask, 0] = mean_gray  # fancy indexing not directly ok; do per image\n",
        "    # Safer apply per-batch for clarity\n",
        "    X2 = np.empty_like(X_out)\n",
        "    for i in range(B):\n",
        "        Xi = X_out[i].copy()\n",
        "        Xi[~mask, 0] = mean_gray\n",
        "        X2[i] = Xi\n",
        "    return X2\n",
        "\n",
        "fractions = [0.0, 0.02, 0.05, 0.1, 0.2, 0.35, 0.5, 0.75, 1.0]\n",
        "ins_acc, del_acc = [], []\n",
        "\n",
        "# Insertion: start from mean-gray everywhere, then reveal top pixels\n",
        "H,W = IMG, IMG\n",
        "mean_gray = float(X_test.mean())\n",
        "base_bg = np.full_like(X_test, mean_gray)\n",
        "\n",
        "for f in fractions:\n",
        "    # insertion: copy background, paste top-f pixels from the real image\n",
        "    k = int(f * H*W)\n",
        "    keep = set(rank[:k].tolist())\n",
        "    mask_flat = np.zeros(H*W, dtype=bool)\n",
        "    if k>0:\n",
        "        mask_flat[list(keep)] = True\n",
        "    mask = mask_flat.reshape(H,W)\n",
        "\n",
        "    X_ins = base_bg.copy()\n",
        "    for i in range(X_ins.shape[0]):\n",
        "        Xi = X_ins[i]\n",
        "        Xi[mask, 0] = X_test[i][mask, 0]\n",
        "        X_ins[i] = Xi\n",
        "\n",
        "    p_ins = model.predict(X_ins, verbose=0).ravel()\n",
        "    ins_acc.append(bin_acc(y_test, p_ins))\n",
        "\n",
        "    # deletion: copy real image, remove top-f pixels to mean-gray\n",
        "    X_del = X_test.copy()\n",
        "    for i in range(X_del.shape[0]):\n",
        "        Xi = X_del[i]\n",
        "        Xi[mask, 0] = mean_gray\n",
        "        X_del[i] = Xi\n",
        "    p_del = model.predict(X_del, verbose=0).ravel()\n",
        "    del_acc.append(bin_acc(y_test, p_del))\n",
        "\n",
        "# Plot curves\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot([int(100*f) for f in fractions], ins_acc, marker=\"o\", label=\"Insertion (keep top-% by ExCIR)\")\n",
        "plt.plot([int(100*f) for f in fractions], del_acc, marker=\"o\", label=\"Deletion (zero top-% by ExCIR)\")\n",
        "plt.xlabel(\"Revealed / removed top-% pixels (by ExCIR)\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.title(\"AOPC-style curves (ExCIR ranking)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"aopc_excir.png\"), dpi=160)\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Save quick montage & stats\n",
        "# -----------------------------\n",
        "def save_montage():\n",
        "    # One random val image, raw + ExCIR map overlay\n",
        "    i = np.random.randint(0, X_val.shape[0])\n",
        "    raw = X_val[i, ..., 0]\n",
        "    circ = excir_map\n",
        "    # Normalize circ to [0,1] for overlay\n",
        "    cm = circ / (circ.max() + 1e-9)\n",
        "    # Simple overlay: 70% raw, 30% circ\n",
        "    overlay = 0.7*raw + 0.3*cm\n",
        "    fig, ax = plt.subplots(1,3, figsize=(9,3))\n",
        "    ax[0].imshow(raw, cmap=\"gray\")\n",
        "    ax[0].set_title(\"val image\")\n",
        "    ax[0].axis(\"off\")\n",
        "    ax[1].imshow(circ, cmap=\"viridis\")\n",
        "    ax[1].set_title(\"ExCIR map\")\n",
        "    ax[1].axis(\"off\")\n",
        "    ax[2].imshow(overlay, cmap=\"gray\")\n",
        "    ax[2].set_title(\"overlay\")\n",
        "    ax[2].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(os.path.join(OUTDIR, \"montage_excir.png\"), dpi=160)\n",
        "    plt.close(fig)\n",
        "\n",
        "save_montage()\n",
        "\n",
        "summary = {\n",
        "    \"img_size\": IMG,\n",
        "    \"epochs\": args.epochs,\n",
        "    \"val_acc\": round(bin_acc(y_val,p_val), 4),\n",
        "    \"val_auc\": round(float(roc_auc_score(y_val,p_val)), 4),\n",
        "    \"test_acc\": round(bin_acc(y_test,p_test), 4),\n",
        "    \"test_auc\": round(float(roc_auc_score(y_test,p_test)), 4),\n",
        "    \"train_time_sec\": round(train_time, 2),\n",
        "    \"mi_computed\": bool(mi_map is not None),\n",
        "    \"pfi_computed\": bool(pfi_grid is not None),\n",
        "    \"out_dir\": os.path.abspath(OUTDIR),\n",
        "}\n",
        "with open(os.path.join(OUTDIR, \"summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\n=== SUMMARY ===\")\n",
        "print(json.dumps(summary, indent=2))\n",
        "print(f\"Saved: {OUTDIR}/excir_heatmap_dog.png, {OUTDIR}/aopc_excir.png, {OUTDIR}/montage_excir.png\")\n",
        "if mi_map is not None:\n",
        "    print(f\"Saved: {OUTDIR}/mi_heatmap_dog.png\")\n",
        "if pfi_grid is not None:\n",
        "    print(f\"Saved: {OUTDIR}/pfi_patch_heatmap.png\")\n"
      ],
      "metadata": {
        "id": "TuE5_sD1xaHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ExCIR vs. MI vs. PFI heatmaps (and AOPC curves) for images\n",
        "# Works with KERAS/TensorFlow or PyTorch\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0) CHOOSE YOUR BACKEND + PROVIDE MODEL & VALIDATION ARRAYS\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "USE_KERAS = True   # set False if you're using PyTorch\n",
        "\n",
        "# --- TODO(you): put your trained model here -----------------\n",
        "# model = ...\n",
        "# For Keras: model(x) or model.predict(x) -> probabilities (N x C)\n",
        "# For PyTorch: model(x) -> logits (N x C) or prob (N x C)\n",
        "\n",
        "# --- TODO(you): provide validation arrays -------------------\n",
        "# val_imgs: numpy array with images, NHWC (N,H,W,C) for Keras (preferred)\n",
        "#           If grayscale, shape can be (N,H,W) or (N,H,W,1)\n",
        "# y_val   : integer class labels, shape (N,)  (optional, only needed for AOPC/accuracy)\n",
        "# Example:\n",
        "# val_imgs = ...  # float32 in [0,1] or your model's scale\n",
        "# y_val    = ...  # int64 class ids, optional\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) PREDICTOR ABSTRACTION\n",
        "# ------------------------------------------------------------\n",
        "if USE_KERAS:\n",
        "    import tensorflow as tf\n",
        "\n",
        "    def predict_proba(imgs: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Return NxC numpy probabilities from a Keras model.\"\"\"\n",
        "        p = model.predict(imgs, verbose=0)\n",
        "        p = np.asarray(p)\n",
        "        if p.ndim == 1:  # binary: shape (N,)\n",
        "            p = np.column_stack([1.0 - p, p])\n",
        "        return p.astype('float32')\n",
        "\n",
        "else:\n",
        "    import torch\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device).eval()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_proba(imgs: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        imgs: NHWC (preferred) or NCHW; we’ll convert to NCHW if needed.\n",
        "        Returns NxC probabilities.\n",
        "        \"\"\"\n",
        "        arr = imgs\n",
        "        if arr.ndim == 3:                 # (N,H,W) -> (N,1,H,W)\n",
        "            arr = arr[:, None, ...]\n",
        "        elif arr.ndim == 4 and arr.shape[1] not in (1,3):  # NHWC -> NCHW\n",
        "            arr = np.transpose(arr, (0,3,1,2))\n",
        "        tens = torch.from_numpy(arr).to(device)\n",
        "        logits = model(tens)\n",
        "        if logits.ndim == 1:              # (N,)\n",
        "            probs1 = torch.sigmoid(logits).unsqueeze(1)\n",
        "            probs  = torch.cat([1.0 - probs1, probs1], dim=1)\n",
        "        else:\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "        return probs.cpu().numpy().astype('float32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) UTILS\n",
        "# ------------------------------------------------------------\n",
        "def to_gray_cube(val_imgs: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns X as (N,H,W) float32 for per-pixel stats.\n",
        "    If RGB, converts to luma (Y = 0.299R + 0.587G + 0.114B).\n",
        "    \"\"\"\n",
        "    X = val_imgs\n",
        "    if X.ndim == 4:  # NHWC\n",
        "        C = X.shape[-1]\n",
        "        if C == 3:\n",
        "            R,G,B = X[...,0], X[...,1], X[...,2]\n",
        "            X = 0.299*R + 0.587*G + 0.114*B\n",
        "        else:\n",
        "            X = X[...,0]\n",
        "    return X.astype('float32')\n",
        "\n",
        "def class_probs(val_imgs: np.ndarray, c_idx: int) -> np.ndarray:\n",
        "    P = predict_proba(val_imgs)           # (N,C)\n",
        "    return P[:, c_idx].astype('float32')  # (N,)\n",
        "\n",
        "def ensure_nhwc(arr: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Make sure images are NHWC for editing; returns a copy in NHWC.\"\"\"\n",
        "    if arr.ndim == 3:\n",
        "        return arr[..., None]  # (N,H,W) -> (N,H,W,1)\n",
        "    if arr.ndim == 4 and arr.shape[-1] in (1,3):\n",
        "        return arr\n",
        "    # assume NCHW -> NHWC\n",
        "    return np.transpose(arr, (0,2,3,1))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) ExCIR HEATMAP (global, class-conditioned)\n",
        "# ------------------------------------------------------------\n",
        "def excir_heatmap(val_imgs: np.ndarray, c_idx: int) -> np.ndarray:\n",
        "    X = to_gray_cube(val_imgs)        # (N,H,W)\n",
        "    N,H,W = X.shape\n",
        "    p = class_probs(val_imgs, c_idx)  # (N,)\n",
        "\n",
        "    mx = X.mean(axis=0)               # (H,W)\n",
        "    vx = X.var(axis=0)\n",
        "\n",
        "    mp = float(p.mean())\n",
        "    vp = float(p.var())\n",
        "\n",
        "    diff2 = (mx - mp)**2\n",
        "    num = 0.5 * diff2\n",
        "    den = vx + vp + 0.5 * diff2\n",
        "    cir = (num / (den + 1e-12)).astype('float32')\n",
        "    return cir\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) MUTUAL INFORMATION HEATMAP (per pixel)\n",
        "# ------------------------------------------------------------\n",
        "def mi_heatmap(val_imgs: np.ndarray, c_idx: int, n_neighbors: int = 3, random_state: int = 0) -> np.ndarray:\n",
        "    X = to_gray_cube(val_imgs)        # (N,H,W)\n",
        "    N,H,W = X.shape\n",
        "    p = class_probs(val_imgs, c_idx)  # (N,)\n",
        "\n",
        "    Xf = X.reshape(N, -1)             # (N, H*W)\n",
        "    mi = mutual_info_regression(\n",
        "        Xf, p, discrete_features=False,\n",
        "        n_neighbors=n_neighbors, random_state=random_state\n",
        "    )\n",
        "    return mi.reshape(H, W).astype('float32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) PFI-STYLE PROBABILITY-SHIFT HEATMAP (patch permutation)\n",
        "# ------------------------------------------------------------\n",
        "def pfi_probshift_heatmap(val_imgs: np.ndarray, c_idx: int, patch: int = 4, rng: int | np.random.Generator = 0) -> np.ndarray:\n",
        "    rng = np.random.default_rng(rng) if not isinstance(rng, np.random.Generator) else rng\n",
        "    Xg = to_gray_cube(val_imgs)       # (N,H,W)\n",
        "    N,H,W = Xg.shape\n",
        "\n",
        "    p0 = class_probs(val_imgs, c_idx) # (N,)\n",
        "    out = np.zeros((H, W), dtype='float32')\n",
        "\n",
        "    Hs = (H // patch) * patch\n",
        "    Ws = (W // patch) * patch\n",
        "\n",
        "    for r in range(0, Hs, patch):\n",
        "        for c in range(0, Ws, patch):\n",
        "            Xp = Xg.copy()\n",
        "            perm = rng.permutation(N)\n",
        "            Xp[:, r:r+patch, c:c+patch] = Xp[perm, r:r+patch, c:c+patch]\n",
        "\n",
        "            # rebuild NHWC to pass through the model\n",
        "            X4 = ensure_nhwc(val_imgs.copy())\n",
        "            # replace ALL channels by the permuted gray patch for a strong perturbation\n",
        "            for ch in range(X4.shape[-1]):\n",
        "                X4[:, r:r+patch, c:c+patch, ch] = Xp[:, r:r+patch, c:c+patch]\n",
        "\n",
        "            p1 = class_probs(X4, c_idx)\n",
        "            delta = np.mean(np.abs(p1 - p0))\n",
        "            out[r:r+patch, c:c+patch] = delta\n",
        "\n",
        "    # normalize for display\n",
        "    m, M = float(out.min()), float(out.max()) + 1e-12\n",
        "    return ((out - m) / (M - m)).astype('float32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) VISUALIZATION\n",
        "# ------------------------------------------------------------\n",
        "def show_heatmaps(ex_map, mi_map, pfi_map, cmap='viridis'):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for k, (m, t) in enumerate([(ex_map,'ExCIR'), (mi_map,'Mutual Information'), (pfi_map,'PFI (prob shift)')], 1):\n",
        "        plt.subplot(1,3,k)\n",
        "        plt.imshow(m, cmap=cmap)\n",
        "        plt.axis('off')\n",
        "        plt.title(t)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def overlay_on_image(val_imgs: np.ndarray, heat: np.ndarray, idx: int = 0, alpha: float = 0.35, cmap='viridis'):\n",
        "    \"\"\"Overlay a heatmap on image idx (simple grayscale visualization).\"\"\"\n",
        "    img = ensure_nhwc(val_imgs)[idx]      # (H,W,C)\n",
        "    if img.shape[-1] == 3:\n",
        "        gray = (0.299*img[...,0] + 0.587*img[...,1] + 0.114*img[...,2])\n",
        "    else:\n",
        "        gray = img[...,0]\n",
        "    H,W = gray.shape\n",
        "    heat = (heat - heat.min()) / (heat.max() - heat.min() + 1e-12)\n",
        "\n",
        "    fig, axs = plt.subplots(1,3, figsize=(14,4))\n",
        "    axs[0].imshow(gray, cmap='gray');   axs[0].set_title('val image'); axs[0].axis('off')\n",
        "    im = axs[1].imshow(heat, cmap=cmap);axs[1].set_title('ExCIR map'); axs[1].axis('off')\n",
        "    axs[2].imshow(gray, cmap='gray')\n",
        "    axs[2].imshow(heat, cmap=cmap, alpha=alpha)\n",
        "    axs[2].set_title('overlay'); axs[2].axis('off')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7) AOPC-STYLE INSERTION/DELETION CURVES (optional; needs y_val)\n",
        "# ------------------------------------------------------------\n",
        "def ranked_masks_global(score_map: np.ndarray, percents=(0,1,2,5,10,20,35,50,75,100)):\n",
        "    \"\"\"Return binary masks (H,W) where 1 means 'top-% pixels' according to score_map.\"\"\"\n",
        "    H,W = score_map.shape\n",
        "    flat = score_map.ravel()\n",
        "    order = np.argsort(-flat)  # descending\n",
        "    masks = []\n",
        "    for pct in percents:\n",
        "        k = int(np.round(pct/100.0 * flat.size))\n",
        "        m = np.zeros_like(flat, dtype=bool)\n",
        "        if k > 0:\n",
        "            m[order[:k]] = True\n",
        "        masks.append(m.reshape(H,W))\n",
        "    return percents, masks\n",
        "\n",
        "def apply_mask(images, mask, mode='insert', baseline=None):\n",
        "    \"\"\"\n",
        "    mode='insert': start from baseline and copy pixels where mask=1\n",
        "    mode='delete': start from image and set pixels where mask=1 to baseline\n",
        "    images: NHWC\n",
        "    \"\"\"\n",
        "    X = ensure_nhwc(images)\n",
        "    H,W = mask.shape\n",
        "    assert X.shape[1:3] == (H,W)\n",
        "    if baseline is None:\n",
        "        baseline = np.zeros_like(X)  # black\n",
        "    out = baseline.copy() if mode=='insert' else X.copy()\n",
        "    if mode == 'insert':\n",
        "        out[:, mask, :] = X[:, mask, :]\n",
        "    else:  # delete\n",
        "        out[:, mask, :] = baseline[:, mask, :]\n",
        "    return out\n",
        "\n",
        "def accuracy_from_probs(P, y):\n",
        "    preds = P.argmax(axis=1)\n",
        "    return float((preds == y).mean())\n",
        "\n",
        "def aopc_curves(val_imgs, y_val, c_idx, score_map, percents=(0,1,2,5,10,20,35,50,75,100)):\n",
        "    \"\"\"Compute insertion/deletion accuracy curves using a global ExCIR ranking.\"\"\"\n",
        "    assert y_val is not None, \"y_val is required to compute accuracy curves.\"\n",
        "    X = ensure_nhwc(val_imgs)\n",
        "    base = np.zeros_like(X)  # black baseline\n",
        "    percents, masks = ranked_masks_global(score_map, percents=percents)\n",
        "\n",
        "    ins_acc, del_acc = [], []\n",
        "    for m in masks:\n",
        "        # insertion: keep top-% pixels, others baseline\n",
        "        Xin = apply_mask(X, m, mode='insert', baseline=base)\n",
        "        Pin = predict_proba(Xin)\n",
        "        ins_acc.append(accuracy_from_probs(Pin, y_val))\n",
        "\n",
        "        # deletion: zero the top-% pixels, keep others\n",
        "        Xdel = apply_mask(X, m, mode='delete', baseline=base)\n",
        "        Pdel = predict_proba(Xdel)\n",
        "        del_acc.append(accuracy_from_probs(Pdel, y_val))\n",
        "\n",
        "    return np.array(percents), np.array(ins_acc), np.array(del_acc)\n",
        "\n",
        "def plot_aopc(percents, ins_acc, del_acc, title='AOPC-style curves (ExCIR ranking)'):\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(percents, ins_acc, 'o-', label='Insertion (keep top-% by ExCIR)')\n",
        "    plt.plot(percents, del_acc, 'o-', label='Deletion (zero top-% by ExCIR)')\n",
        "    plt.xlabel('Revealed / removed top-% pixels (by ExCIR)')\n",
        "    plt.ylabel('Test accuracy')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# # ------------------------------------------------------------\n",
        "# 8) EXAMPLE USAGE (uncomment when val_imgs + model are available)\n",
        "# ------------------------------------------------------------\n",
        "c_idx = 1  # e.g., class index for \"dog\" in cats-vs-dogs\n",
        "\n",
        "# Heatmaps\n",
        "ex_map  = excir_heatmap(val_imgs, c_idx)\n",
        "mi_map  = mi_heatmap(val_imgs, c_idx)\n",
        "pfi_map = pfi_probshift_heatmap(val_imgs, c_idx, patch=4)\n",
        "show_heatmaps(ex_map, mi_map, pfi_map)\n",
        "\n",
        "# Overlay ExCIR on a validation image\n",
        "overlay_on_image(val_imgs, ex_map, idx=0, alpha=0.35)\n",
        "\n",
        "# AOPC curves (needs labels)\n",
        "if 'y_val' in globals() and y_val is not None:\n",
        "    perc, ins, dele = aopc_curves(val_imgs, y_val, c_idx, ex_map)\n",
        "    plot_aopc(perc, ins, dele)\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "cS_nOl0jxi1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# excir_cats_dogs_gnn.py\n",
        "# GNN on Cats vs Dogs via superpixel graphs + ExCIR analysis\n",
        "# ---------------------------------------------------------\n",
        "# What it does:\n",
        "# 1) Load TFDS cats_vs_dogs, resize to IMG x IMG, grayscale in [0,1]\n",
        "# 2) Build per-image SLIC superpixel graphs (nodes = superpixels; edges = adjacency)\n",
        "# 3) Compute node features: mean intensity, std, centroid (x,y), area, sobel mean\n",
        "# 4) Train a small GCN for graph-level classification (dog vs cat)\n",
        "# 5) Compute class-conditioned ExCIR on feature channels across images\n",
        "# 6) Compute canonical direction w* and per-image node saliency = x_node · w*\n",
        "# 7) Save overlays and summary\n",
        "#\n",
        "# Quick install (Colab/local):\n",
        "#   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # choose your CUDA/CPU\n",
        "#   pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
        "#   pip install tensorflow-datasets scikit-image matplotlib numpy\n",
        "#\n",
        "# Run:\n",
        "#   python excir_cats_dogs_gnn.py --epochs 5 --img_size 96 --segments 200 --compute_mi\n",
        "#\n",
        "import os, time, json, argparse, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# TFDS for raw images\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Image utils\n",
        "from skimage.segmentation import slic\n",
        "from skimage.filters import sobel\n",
        "\n",
        "# Torch + PyG\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    from torch_geometric.data import Data, Dataset\n",
        "    from torch_geometric.loader import DataLoader\n",
        "    from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "    HAVE_PYG = True\n",
        "except Exception as e:\n",
        "    HAVE_PYG = False\n",
        "    PYG_ERR = str(e)\n",
        "\n",
        "# Optional MI baseline\n",
        "try:\n",
        "    from sklearn.feature_selection import mutual_info_regression\n",
        "    HAVE_SK_MI = True\n",
        "except Exception:\n",
        "    HAVE_SK_MI = False\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--img_size\", type=int, default=96, help=\"image side (resized)\")\n",
        "parser.add_argument(\"--segments\", type=int, default=200, help=\"SLIC superpixels per image\")\n",
        "parser.add_argument(\"--compactness\", type=float, default=10.0, help=\"SLIC compactness\")\n",
        "parser.add_argument(\"--epochs\", type=int, default=4)\n",
        "parser.add_argument(\"--batch\", type=int, default=32)\n",
        "parser.add_argument(\"--fast_train_cap\", type=int, default=4000)\n",
        "parser.add_argument(\"--fast_val_cap\", type=int, default=1000)\n",
        "parser.add_argument(\"--fast_test_cap\", type=int, default=1000)\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "parser.add_argument(\"--compute_mi\", action=\"store_true\", help=\"compute MI(pred) for feature channels\")\n",
        "args = parser.parse_args() if \"__file__\" in globals() else parser.parse_args([])\n",
        "\n",
        "OUTDIR = \"out_gnn\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "if not HAVE_PYG:\n",
        "    raise RuntimeError(\n",
        "        \"PyTorch Geometric not found. Please install torch-geometric and friends.\\n\"\n",
        "        f\"Import error: {PYG_ERR}\"\n",
        "    )\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "tf.random.set_seed(args.seed)\n",
        "\n",
        "# -----------------------------\n",
        "# Load Cats vs Dogs\n",
        "# -----------------------------\n",
        "IMG = args.img_size\n",
        "\n",
        "def _prep(ex):\n",
        "    img = tf.image.resize(ex[\"image\"], [IMG, IMG])\n",
        "    img = tf.image.rgb_to_grayscale(img)  # H,W,1\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    y = tf.cast(ex[\"label\"], tf.int32)    # 0 cat, 1 dog\n",
        "    return img, y\n",
        "\n",
        "print(\"Loading TFDS cats_vs_dogs...\")\n",
        "ds_train = tfds.load(\"cats_vs_dogs\", split=\"train[:80%]\", as_supervised=False)\n",
        "ds_val   = tfds.load(\"cats_vs_dogs\", split=\"train[80%:90%]\", as_supervised=False)\n",
        "ds_test  = tfds.load(\"cats_vs_dogs\", split=\"train[90%:]\", as_supervised=False)\n",
        "ds_train = ds_train.map(_prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_val   = ds_val.map(_prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test  = ds_test.map(_prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "if args.fast_train_cap: ds_train = ds_train.take(args.fast_train_cap)\n",
        "if args.fast_val_cap:   ds_val   = ds_val.take(args.fast_val_cap)\n",
        "if args.fast_test_cap:  ds_test  = ds_test.take(args.fast_test_cap)\n",
        "\n",
        "def to_numpy(ds):\n",
        "    Xs, ys = [], []\n",
        "    for x, y in ds:\n",
        "        Xs.append(x.numpy())  # (H,W,1)\n",
        "        ys.append(y.numpy())\n",
        "    X = np.stack(Xs, axis=0)\n",
        "    y = np.array(ys).astype(np.int64)\n",
        "    return X, y\n",
        "\n",
        "print(\"Materializing splits (this may take a minute)...\")\n",
        "Xtr, ytr = to_numpy(ds_train)\n",
        "Xva, yva = to_numpy(ds_val)\n",
        "Xte, yte = to_numpy(ds_test)\n",
        "\n",
        "# -----------------------------\n",
        "# Superpixel Graph Builder\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class GraphExample:\n",
        "    data: Data\n",
        "    raw: np.ndarray        # (H,W) grayscale\n",
        "    seg: np.ndarray        # (H,W) labels\n",
        "    label: int\n",
        "\n",
        "def build_graph_from_image(img01, label, segments=200, compactness=10.0):\n",
        "    \"\"\"\n",
        "    img01: (H,W,1) numpy in [0,1]\n",
        "    Returns a PyG Data with:\n",
        "      x: (nodes, d)\n",
        "      edge_index: (2, E)\n",
        "      y: graph label (0/1)\n",
        "      pos: (nodes, 2) normalized centroids\n",
        "    Also returns segmentation and raw image for visualization.\n",
        "    \"\"\"\n",
        "    H, W, _ = img01.shape\n",
        "    im = img01[..., 0]\n",
        "    seg = slic(im, n_segments=segments, compactness=compactness, start_label=0)\n",
        "    n_nodes = seg.max() + 1\n",
        "\n",
        "    # adjacency from pixel neighbors (4-neighborhood)\n",
        "    edges_set = set()\n",
        "    # right & down neighbors to avoid duplicates\n",
        "    for i in range(H-1):\n",
        "        for j in range(W-1):\n",
        "            a = seg[i, j]; b = seg[i, j+1]; c = seg[i+1, j]\n",
        "            if a != b: edges_set.add((min(a,b), max(a,b)))\n",
        "            if a != c: edges_set.add((min(a,c), max(a,c)))\n",
        "    # convert to undirected edge_index\n",
        "    e_list = list(edges_set)\n",
        "    if len(e_list)==0:\n",
        "        # degenerate case: connect sequentially\n",
        "        e_list = [(k, (k+1)%n_nodes) for k in range(n_nodes)]\n",
        "    e_src = [u for (u,v) in e_list] + [v for (u,v) in e_list]\n",
        "    e_dst = [v for (u,v) in e_list] + [u for (u,v) in e_list]\n",
        "    edge_index = torch.tensor([e_src, e_dst], dtype=torch.long)\n",
        "\n",
        "    # node features: mean, std, centroid_x, centroid_y, area_frac, sobel_mean\n",
        "    x_feats = []\n",
        "    pos = []\n",
        "    sob = sobel(im)\n",
        "    for k in range(n_nodes):\n",
        "        mask = (seg==k)\n",
        "        vals = im[mask]\n",
        "        if vals.size == 0:\n",
        "            mean = 0.0; std = 0.0; area = 0\n",
        "            cx = 0.5; cy = 0.5; sob_mean = 0.0\n",
        "        else:\n",
        "            mean = float(vals.mean())\n",
        "            std  = float(vals.std())\n",
        "            area = float(vals.size) / (H*W)\n",
        "            ys, xs = np.where(mask)\n",
        "            cx = float(xs.mean() / (W-1))\n",
        "            cy = float(ys.mean() / (H-1))\n",
        "            sob_mean = float(sob[mask].mean())\n",
        "        x_feats.append([mean, std, cx, cy, area, sob_mean])\n",
        "        pos.append([cx, cy])\n",
        "    x = torch.tensor(np.array(x_feats, dtype=np.float32))\n",
        "    pos = torch.tensor(np.array(pos, dtype=np.float32))\n",
        "    y_t = torch.tensor([label], dtype=torch.long)\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, y=y_t, pos=pos)\n",
        "    return GraphExample(data=data, raw=im, seg=seg, label=int(label))\n",
        "\n",
        "def build_graph_dataset(X, y, segments, compactness):\n",
        "    out = []\n",
        "    for i in range(len(X)):\n",
        "        out.append(build_graph_from_image(X[i], int(y[i]),\n",
        "                                          segments=segments, compactness=compactness))\n",
        "    return out\n",
        "\n",
        "print(\"Building superpixel graphs...\")\n",
        "train_graphs = build_graph_dataset(Xtr, ytr, args.segments, args.compactness)\n",
        "val_graphs   = build_graph_dataset(Xva, yva, args.segments, args.compactness)\n",
        "test_graphs  = build_graph_dataset(Xte, yte, args.segments, args.compactness)\n",
        "\n",
        "# PyG Dataset wrapper\n",
        "class ListGraphDataset(Dataset):\n",
        "    def __init__(self, items):\n",
        "        super().__init__(None)\n",
        "        self.items = items\n",
        "    def len(self): return len(self.items)\n",
        "    def get(self, idx): return self.items[idx].data\n",
        "\n",
        "ds_train = ListGraphDataset(train_graphs)\n",
        "ds_val   = ListGraphDataset(val_graphs)\n",
        "ds_test  = ListGraphDataset(test_graphs)\n",
        "\n",
        "train_loader = DataLoader(ds_train, batch_size=args.batch, shuffle=True)\n",
        "val_loader   = DataLoader(ds_val,   batch_size=args.batch, shuffle=False)\n",
        "test_loader  = DataLoader(ds_test,  batch_size=args.batch, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Simple GCN model (graph-level)\n",
        "# -----------------------------\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_dim=6, hidden=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_dim, hidden)\n",
        "        self.conv2 = GCNConv(hidden, hidden)\n",
        "        self.lin1  = nn.Linear(hidden, 64)\n",
        "        self.lin2  = nn.Linear(64, 1)  # sigmoid later\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        g = global_mean_pool(x, batch)\n",
        "        g = F.relu(self.lin1(g))\n",
        "        logit = self.lin2(g).view(-1)\n",
        "        return logit\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GCN(in_dim=train_loader.dataset.get(0).x.shape[1]).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    y_true, y_prob = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            logit = model(data)\n",
        "            prob = torch.sigmoid(logit)\n",
        "            y_true.append(data.y.float().view(-1).cpu().numpy())\n",
        "            y_prob.append(prob.cpu().numpy())\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_prob = np.concatenate(y_prob)\n",
        "    acc = float(np.mean((y_prob>=0.5)==y_true))\n",
        "    return acc, y_prob, y_true\n",
        "\n",
        "print(\"Training GCN...\")\n",
        "t0 = time.time()\n",
        "for epoch in range(1, args.epochs+1):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        opt.zero_grad()\n",
        "        logit = model(data)\n",
        "        y = data.y.float().view(-1)\n",
        "        loss = bce(logit, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        losses.append(float(loss.item()))\n",
        "    va_acc, va_prob, va_true = eval_epoch(val_loader)\n",
        "    print(f\"Epoch {epoch:02d} | loss {np.mean(losses):.4f} | val acc {va_acc:.3f}\")\n",
        "train_time = time.time()-t0\n",
        "\n",
        "te_acc, te_prob, te_true = eval_epoch(test_loader)\n",
        "print(f\"Test acc={te_acc:.3f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# ExCIR on feature channels (global, across images)\n",
        "# We'll use feature MEANS per graph: mu_i = mean over nodes (after train-std)\n",
        "# -----------------------------\n",
        "# 1) Collect all node features from TRAIN to get per-channel mean/std\n",
        "all_nodes = []\n",
        "for ge in train_graphs:\n",
        "    all_nodes.append(ge.data.x.numpy())\n",
        "all_nodes = np.vstack(all_nodes)  # (sum_nodes, d)\n",
        "mu_ch = all_nodes.mean(axis=0)\n",
        "sd_ch = all_nodes.std(axis=0) + 1e-12\n",
        "\n",
        "def graph_channel_means(graphs):\n",
        "    mus = []\n",
        "    for ge in graphs:\n",
        "        X = ge.data.x.numpy()\n",
        "        Xs = (X - mu_ch)/sd_ch\n",
        "        mus.append(Xs.mean(axis=0))  # (d,)\n",
        "    return np.vstack(mus)  # (N_graphs, d)\n",
        "\n",
        "Mu_val = graph_channel_means(val_graphs)     # standardized\n",
        "p_val  = va_prob                              # prob(dog)\n",
        "Mu_test= graph_channel_means(test_graphs)\n",
        "p_test = te_prob\n",
        "\n",
        "def excir_channels(Mu, p):\n",
        "    # Mu: (N, d), p: (N,)\n",
        "    mean_f = Mu.mean(axis=0)                 # (d,)\n",
        "    var_f  = Mu.var(axis=0)                  # (d,)\n",
        "    mean_p = float(p.mean())\n",
        "    var_p  = float(p.var())\n",
        "    num = 0.5*(mean_f - mean_p)**2\n",
        "    den = var_f + var_p + num\n",
        "    cir = num/den\n",
        "    return cir  # (d,)\n",
        "\n",
        "excir_ch = excir_channels(Mu_val, p_val)     # per-channel ExCIR\n",
        "rank_idx = np.argsort(-excir_ch)\n",
        "\n",
        "# -----------------------------\n",
        "# Canonical direction w* for per-node heatmaps\n",
        "# w* ∝ Σ^{-1} Cov(Mu, p)\n",
        "# -----------------------------\n",
        "Mu_c = Mu_val - Mu_val.mean(axis=0, keepdims=True)\n",
        "p_c  = p_val - p_val.mean()\n",
        "Sigma = (Mu_c.T @ Mu_c)/max(1, Mu_c.shape[0]-1) + 1e-6*np.eye(Mu_c.shape[1])\n",
        "cov   = (Mu_c.T @ p_c.reshape(-1,1))/max(1, Mu_c.shape[0]-1)  # (d,1)\n",
        "w_star = np.linalg.solve(Sigma, cov).reshape(-1)  # (d,)\n",
        "# scale-invariant: normalize\n",
        "w_star /= (np.linalg.norm(w_star)+1e-12)\n",
        "\n",
        "# -----------------------------\n",
        "# Visualize per-image node saliency = <x_node_std, w*>\n",
        "# -----------------------------\n",
        "def node_scores_for_image(ge: GraphExample):\n",
        "    X = ge.data.x.numpy()\n",
        "    Xs = (X - mu_ch)/sd_ch\n",
        "    s = Xs @ w_star  # (nodes,)\n",
        "    # min-max to [0,1] for colormap\n",
        "    s_norm = (s - s.min())/(s.max()-s.min() + 1e-9)\n",
        "    return s_norm  # (nodes,)\n",
        "\n",
        "def overlay_superpixel_map(ge: GraphExample, node_scores, fname):\n",
        "    # color each superpixel by node_scores\n",
        "    im = ge.raw  # (H,W)\n",
        "    seg = ge.seg\n",
        "    H,W = im.shape\n",
        "    heat = np.zeros_like(im)\n",
        "    for k in range(node_scores.shape[0]):\n",
        "        heat[seg==k] = node_scores[k]\n",
        "    # overlay\n",
        "    overlay = 0.65*im + 0.35*heat  # simple blend (both in [0,1])\n",
        "    plt.figure(figsize=(7,3))\n",
        "    plt.subplot(1,3,1); plt.imshow(im, cmap=\"gray\"); plt.title(\"image\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,2); plt.imshow(heat, cmap=\"viridis\"); plt.title(\"node saliency\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,3); plt.imshow(overlay, cmap=\"gray\"); plt.title(\"overlay\"); plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTDIR, fname), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# Save a few random overlays from VAL\n",
        "np.random.seed(args.seed)\n",
        "idxs = np.random.choice(len(val_graphs), size=min(6, len(val_graphs)), replace=False)\n",
        "for i, idx in enumerate(idxs):\n",
        "    ge = val_graphs[idx]\n",
        "    scores = node_scores_for_image(ge)\n",
        "    overlay_superpixel_map(ge, scores, f\"gnn_excir_overlay_val_{i}.png\")\n",
        "\n",
        "# -----------------------------\n",
        "# Optional MI baseline on channels\n",
        "# -----------------------------\n",
        "mi_ch = None\n",
        "if args.compute_mi and HAVE_SK_MI:\n",
        "    print(\"Computing MI(pred) for feature channels...\")\n",
        "    mi_ch = mutual_info_regression(Mu_val, p_val, random_state=args.seed)\n",
        "\n",
        "# -----------------------------\n",
        "# Bar chart of channel importances\n",
        "# -----------------------------\n",
        "names = [\"mean\",\"std\",\"cx\",\"cy\",\"area\",\"sobel\"]\n",
        "assert len(names)==excir_ch.shape[0]\n",
        "plt.figure(figsize=(6,3.2))\n",
        "plt.bar(np.arange(len(names))-0.15, excir_ch, width=0.3, label=\"ExCIR\")\n",
        "if mi_ch is not None:\n",
        "    plt.bar(np.arange(len(names))+0.15, mi_ch/ (mi_ch.max()+1e-12), width=0.3, label=\"MI (scaled)\")\n",
        "plt.xticks(np.arange(len(names)), names, rotation=0)\n",
        "plt.ylabel(\"score (unitless)\")\n",
        "plt.title(\"Channel importance (val set)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"gnn_channel_importance.png\"), dpi=160)\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Small insertion/deletion on nodes (per-image, using w*)\n",
        "# Insertion: start from mean-node graph, reveal top-q% nodes\n",
        "# Deletion: zero out top-q% nodes\n",
        "# -----------------------------\n",
        "def predict_probs_graphs(graphs):\n",
        "    model.eval()\n",
        "    probs=[]\n",
        "    with torch.no_grad():\n",
        "        for g in graphs:\n",
        "            data = g.data\n",
        "            data = Data(x=data.x, edge_index=data.edge_index, y=data.y)\n",
        "            data.batch = torch.zeros(data.x.shape[0], dtype=torch.long)  # single graph\n",
        "            data = data.to(device)\n",
        "            probs.append(torch.sigmoid(model(data)).item())\n",
        "    return np.array(probs)\n",
        "\n",
        "def aopc_curves(graphs, fractions=(0.0,0.05,0.1,0.2,0.35,0.5,0.75,1.0)):\n",
        "    base_probs = predict_probs_graphs(graphs)\n",
        "    base_acc = float(np.mean((base_probs>=0.5)==np.array([g.label for g in graphs])))\n",
        "\n",
        "    ins_acc, del_acc = [], []\n",
        "    # Precompute dataset mean node feature (per-channel)\n",
        "    allX = np.vstack([ge.data.x.numpy() for ge in graphs])\n",
        "    xbar = allX.mean(axis=0, keepdims=True)\n",
        "\n",
        "    for f in fractions:\n",
        "        probs_ins, probs_del = [], []\n",
        "        for ge in graphs:\n",
        "            X = ge.data.x.numpy()\n",
        "            Xs = (X - mu_ch)/sd_ch\n",
        "            s = (Xs @ w_star)  # node scores\n",
        "            n = len(s)\n",
        "            k = int(max(1, np.round(f*n))) if f>0 else 0\n",
        "            order = np.argsort(-s)  # descending\n",
        "\n",
        "            # insertion: start from mean nodes, copy top-k nodes from original\n",
        "            Xin = np.repeat(xbar, repeats=X.shape[0], axis=0)\n",
        "            if k>0:\n",
        "                Xin[order[:k], :] = X[order[:k], :]\n",
        "\n",
        "            # deletion: copy original, zero top-k to mean\n",
        "            Xdel = X.copy()\n",
        "            if k>0:\n",
        "                Xdel[order[:k], :] = xbar[0]\n",
        "\n",
        "            def pred_for(Xnew):\n",
        "                data = Data(x=torch.tensor(Xnew, dtype=torch.float32),\n",
        "                            edge_index=ge.data.edge_index,\n",
        "                            y=torch.tensor([ge.label], dtype=torch.long))\n",
        "                data.batch = torch.zeros(Xnew.shape[0], dtype=torch.long)\n",
        "                data = data.to(device)\n",
        "                with torch.no_grad():\n",
        "                    return torch.sigmoid(model(data)).item()\n",
        "\n",
        "            probs_ins.append(pred_for(Xin))\n",
        "            probs_del.append(pred_for(Xdel))\n",
        "\n",
        "        ytrue = np.array([ge.label for ge in graphs])\n",
        "        ins_acc.append(float(np.mean((np.array(probs_ins)>=0.5)==ytrue)))\n",
        "        del_acc.append(float(np.mean((np.array(probs_del)>=0.5)==ytrue)))\n",
        "\n",
        "    return np.array(fractions), np.array(ins_acc), np.array(del_acc), base_acc\n",
        "\n",
        "print(\"Computing AOPC curves on a small VAL subset...\")\n",
        "subN = min(150, len(val_graphs))\n",
        "fractions, ins_acc, del_acc, base_acc = aopc_curves(val_graphs[:subN])\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fractions*100, ins_acc, marker=\"o\", label=\"Insertion (keep top-% nodes)\")\n",
        "plt.plot(fractions*100, del_acc, marker=\"o\", label=\"Deletion (remove top-% nodes)\")\n",
        "plt.axhline(base_acc, ls=\"--\", lw=1, label=f\"Base acc={base_acc:.3f}\")\n",
        "plt.xlabel(\"Top-% nodes by canonical score\")\n",
        "plt.ylabel(\"Accuracy (VAL subset)\")\n",
        "plt.title(\"AOPC-style curves (GNN + ExCIR node ranking)\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"gnn_aopc_nodes.png\"), dpi=160)\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Save summary\n",
        "# -----------------------------\n",
        "summary = {\n",
        "    \"img_size\": IMG,\n",
        "    \"segments\": args.segments,\n",
        "    \"epochs\": args.epochs,\n",
        "    \"val_acc\": float(np.mean((va_prob>=0.5)==va_true)),\n",
        "    \"test_acc\": te_acc,\n",
        "    \"train_time_sec\": round(train_time,2),\n",
        "    \"channels\": [\"mean\",\"std\",\"cx\",\"cy\",\"area\",\"sobel\"],\n",
        "    \"excir_channels\": [float(v) for v in excir_ch.tolist()],\n",
        "    \"mi_channels\": ([float(v) for v in mi_ch.tolist()] if (args.compute_mi and HAVE_SK_MI) else None),\n",
        "    \"out_dir\": os.path.abspath(OUTDIR)\n",
        "}\n",
        "with open(os.path.join(OUTDIR, \"gnn_summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\n=== GNN + ExCIR SUMMARY ===\")\n",
        "print(json.dumps(summary, indent=2))\n",
        "print(f\"Saved overlays & plots under: {OUTDIR}\")\n"
      ],
      "metadata": {
        "id": "MZu8SDeexjlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run_benchmarks.py\n",
        "import os, json, math, time, pathlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_openml, fetch_20newsgroups\n",
        "\n",
        "# ------------------\n",
        "# Utilities\n",
        "# ------------------\n",
        "def standardize_fit(Xtr, Xva, Xte):\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    Xtr_s = ss.fit_transform(Xtr)\n",
        "    Xva_s = ss.transform(Xva)\n",
        "    Xte_s = ss.transform(Xte)\n",
        "    return Xtr_s, Xva_s, Xte_s\n",
        "\n",
        "def excir_scores(X, p):  # global ExCIR against predicted prob/logit vector p (on same rows)\n",
        "    # X: (n, d) standardized; p: (n,) standardized or raw (we standardize inside)\n",
        "    n, d = X.shape\n",
        "    ps = (p - p.mean()) / (p.std() + 1e-12)\n",
        "    res = np.zeros(d)\n",
        "    var_p = np.var(ps, ddof=0)\n",
        "    for j in range(d):\n",
        "        xj = X[:, j]\n",
        "        xj = (xj - xj.mean()) / (xj.std() + 1e-12)\n",
        "        # pooled mean m = (mean xj + mean p)/2 = 0 by std.; formula reduces to rho^2/(1+rho^2)\n",
        "        rho = np.corrcoef(xj, ps)[0,1]\n",
        "        rho2 = 0.0 if np.isnan(rho) else rho*rho\n",
        "        res[j] = rho2 / (1.0 + rho2)\n",
        "    return res\n",
        "\n",
        "def topk_sufficiency_eval(Xtr, ytr, Xva, yva, Xte, yte, method_scores, ks, task='clf'):\n",
        "    ranks = np.argsort(-method_scores)  # descending\n",
        "    out = []\n",
        "    for k in ks:\n",
        "        keep = ranks[:k]\n",
        "        if task == 'clf':\n",
        "            model = GradientBoostingClassifier(random_state=0)\n",
        "        else:\n",
        "            model = GradientBoostingRegressor(random_state=0)\n",
        "        Xtr_k = np.concatenate([Xtr, Xva], axis=0)[:, keep]\n",
        "        ytr_k = np.concatenate([ytr, yva], axis=0)\n",
        "        model.fit(Xtr_k, ytr_k)\n",
        "        yhat = model.predict(Xte[:, keep])\n",
        "        if task == 'clf':\n",
        "            metric = accuracy_score(yte, yhat)\n",
        "        else:\n",
        "            metric = r2_score(yte, yhat)\n",
        "        out.append((k, metric))\n",
        "    return out\n",
        "\n",
        "def aopc_curves(Xte, model, scores, task='clf', steps=10):\n",
        "    idx = np.argsort(-scores)\n",
        "    n, d = Xte.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    # base predictions\n",
        "    if task == 'clf':\n",
        "        base = model.predict(Xte)\n",
        "        base_acc = accuracy_score(yte_global, base)\n",
        "    else:\n",
        "        base_r2 = r2_score(yte_global, model.predict(Xte))\n",
        "    ins_y = []\n",
        "    del_y = []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f * d))\n",
        "        keep = idx[:k]\n",
        "        mask_keep = np.zeros(d, dtype=bool); mask_keep[keep] = True\n",
        "        # insertion: keep only top-k, zero the rest\n",
        "        X_ins = np.where(mask_keep, Xte, 0.0)\n",
        "        # deletion: zero top-k, keep the rest\n",
        "        X_del = np.where(mask_keep, 0.0, Xte)\n",
        "        if task == 'clf':\n",
        "            ins_y.append(accuracy_score(yte_global, model.predict(X_ins)))\n",
        "            del_y.append(accuracy_score(yte_global, model.predict(X_del)))\n",
        "        else:\n",
        "            ins_y.append(r2_score(yte_global, model.predict(X_ins)))\n",
        "            del_y.append(r2_score(yte_global, model.predict(X_del)))\n",
        "    return fracs, np.array(ins_y), np.array(del_y)\n",
        "\n",
        "# ------------------\n",
        "# Tabular: Adult (binary) and California Housing (regression)\n",
        "# ------------------\n",
        "def load_adult():\n",
        "    ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "    X = ds.data.select_dtypes(include=[np.number]).to_numpy()\n",
        "    y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=0)\n",
        "    Xtr_s, Xva_s, Xte_s = standardize_fit(Xtr, Xva, Xte)\n",
        "    return Xtr_s, ytr, Xva_s, yva, Xte_s, yte, 'clf'\n",
        "\n",
        "def load_california():\n",
        "    ds = fetch_openml(\"california_housing\", version=1, as_frame=True)\n",
        "    X = ds.data.to_numpy()\n",
        "    y = ds.target.to_numpy()\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=0)\n",
        "    Xtr_s, Xva_s, Xte_s = standardize_fit(Xtr, Xva, Xte)\n",
        "    return Xtr_s, ytr, Xva_s, yva, Xte_s, yte, 'reg'\n",
        "\n",
        "# ------------------\n",
        "# Text: 20 Newsgroups (binary or small 4-way)\n",
        "# ------------------\n",
        "def load_20ng_binary():\n",
        "    cats = ['comp.graphics', 'sci.space']\n",
        "    tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "    te = fetch_20newsgroups(subset='test', categories=cats, remove=('headers','footers','quotes'))\n",
        "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
        "    Xtr = tfidf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "    Xte = tfidf.transform(te.data).astype(np.float32).toarray()\n",
        "    ytr = tr.target; yte = te.target\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=0)\n",
        "    # Already standardized-ish; leave as is for TF-IDF\n",
        "    return Xtr, ytr, Xva, yva, Xte, yte, 'clf'\n",
        "\n",
        "# ------------------\n",
        "# Signals: UCI HAR (light wrapper via OpenML copy)\n",
        "# ------------------\n",
        "def load_har():\n",
        "    ds = fetch_openml(\"HAR\", version=1, as_frame=True)  # may fall back to a mirror\n",
        "    X = ds.data.to_numpy(dtype=np.float32)\n",
        "    y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=0)\n",
        "    Xtr_s, Xva_s, Xte_s = standardize_fit(Xtr, Xva, Xte)\n",
        "    return Xtr_s, ytr, Xva_s, yva, Xte_s, yte, 'clf'\n",
        "\n",
        "# ------------------\n",
        "# Main runner\n",
        "# ------------------\n",
        "DATASETS = {\n",
        "    \"adult\": load_adult,\n",
        "    \"california\": load_california,\n",
        "    \"20ng_bin\": load_20ng_binary,\n",
        "    \"har\": load_har,\n",
        "    # For images/graphs, add separate scripts or wrappers (CNN/GCN training) and then compute ExCIR over flattened features or patches.\n",
        "}\n",
        "\n",
        "RESULTS = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(\"out\", exist_ok=True)\n",
        "    ks = [3,5,8,12]\n",
        "    for name, loader in DATASETS.items():\n",
        "        print(f\"\\n=== {name} ===\")\n",
        "        Xtr, ytr, Xva, yva, Xte, yte, task = loader()\n",
        "        # model for base predictions on validation\n",
        "        if task == 'clf':\n",
        "            model = GradientBoostingClassifier(random_state=0)\n",
        "        else:\n",
        "            model = GradientBoostingRegressor(random_state=0)\n",
        "        model.fit(np.concatenate([Xtr,Xva],0), np.concatenate([ytr,yva],0))\n",
        "        # validation probabilities/logits for ExCIR (use prob of positive or mean prediction)\n",
        "        if task == 'clf':\n",
        "            p_va = model.predict_proba(Xva)[:, -1]\n",
        "        else:\n",
        "            p_va = model.predict(Xva)\n",
        "        # compute scores\n",
        "        s_excir = excir_scores(Xva.copy(), p_va.copy())\n",
        "\n",
        "        # baselines (PFI on val)\n",
        "        if task == 'clf':\n",
        "            imp = permutation_importance(model, Xva, yva, n_repeats=10, random_state=0)\n",
        "            s_pfi = imp.importances_mean\n",
        "        else:\n",
        "            imp = permutation_importance(model, Xva, yva, n_repeats=10, random_state=0)\n",
        "            s_pfi = imp.importances_mean\n",
        "\n",
        "        # TreeGain if model is tree-based\n",
        "        try:\n",
        "            s_gain = model.feature_importances_\n",
        "        except Exception:\n",
        "            s_gain = np.zeros(Xva.shape[1])\n",
        "\n",
        "        # top-k sufficiency\n",
        "        global yte_global\n",
        "        yte_global = yte\n",
        "        for label, s in [(\"ExCIR\", s_excir), (\"PFI\", s_pfi), (\"TreeGain\", s_gain)]:\n",
        "            res = topk_sufficiency_eval(Xtr, ytr, Xva, yva, Xte, yte, s, ks, task=task)\n",
        "            for k, metric in res:\n",
        "                RESULTS.append({\"dataset\": name, \"method\": label, \"k\": k,\n",
        "                                \"metric\": float(metric), \"task\": task})\n",
        "\n",
        "        # AOPC on test (optional quick look)\n",
        "        fracs, ins, dele = aopc_curves(Xte, model, s_excir, task=task, steps=9)\n",
        "        plt.figure(figsize=(6,4.2))\n",
        "        plt.plot(100*fracs, ins, marker='o', label='Insertion (keep top-% by ExCIR)')\n",
        "        plt.plot(100*fracs, dele, marker='o', label='Deletion (zero top-% by ExCIR)')\n",
        "        plt.xlabel('Revealed / removed top-% features')\n",
        "        plt.ylabel('Test ' + ('Accuracy' if task=='clf' else 'R$^2$'))\n",
        "        plt.title(f'AOPC (ExCIR) — {name}')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"out/aopc_{name}.png\", dpi=160); plt.close()\n",
        "\n",
        "    # save summary\n",
        "    with open(\"out/summary.json\", \"w\") as f:\n",
        "        json.dump(RESULTS, f, indent=2)\n",
        "    print(\"\\nSaved: out/summary.json and aopc_* plots.\")\n"
      ],
      "metadata": {
        "id": "RDtedJgvxm99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "ExCIR Benchmarks (tabular, text, signals, images) + lightweight-vs-original agreement/time\n",
        "------------------------------------------------------------------------------------------\n",
        "Outputs (in ./out):\n",
        "  - summary_topk.csv           : Top-k sufficiency (per dataset, per method)\n",
        "  - aopc_<dataset>.png         : Insertion/Deletion curves using ExCIR ranking\n",
        "  - pareto_<dataset>.png       : Agreement–cost Pareto for lightweight fractions\n",
        "  - runtime_<dataset>.png      : End-to-end runtime vs fraction (lightweight)\n",
        "  - stability_<dataset>.png    : Rank stability under small noise (ExCIR)\n",
        "  - overlap_corr_<dataset>.png : Method Jaccard@k and Spearman correlation heatmaps (when computed)\n",
        "  - *_class_heatmaps.png       : Image (digits) class-conditioned ExCIR maps (optional)\n",
        "  - log.txt                    : A brief run log\n",
        "\n",
        "Notes:\n",
        "  • SHAP and HSIC are optional (skip if packages missing / dimensionality too large).\n",
        "  • Image experiments use sklearn 'digits' (8x8) to keep dependencies light and runnable everywhere.\n",
        "  • CIFAR-10 / Cats-vs-Dogs and graphs can be added by installing torchvision / torch-geometric.\n",
        "\"\"\"\n",
        "\n",
        "import os, sys, time, json, math, warnings, argparse, pathlib, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, List\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from sklearn.datasets import fetch_openml, load_digits\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Optional imports\n",
        "try:\n",
        "    import shap\n",
        "    HAVE_SHAP = True\n",
        "except Exception:\n",
        "    HAVE_SHAP = False\n",
        "\n",
        "# -------------------------\n",
        "# Utility helpers\n",
        "# -------------------------\n",
        "def set_seed(seed: int = 0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def ensure_out():\n",
        "    os.makedirs(\"out\", exist_ok=True)\n",
        "\n",
        "def log(msg: str):\n",
        "    print(msg, flush=True)\n",
        "    with open(\"out/log.txt\", \"a\") as f:\n",
        "        f.write(msg + \"\\n\")\n",
        "\n",
        "def standardize_fit(Xtr, Xva, Xte):\n",
        "    \"\"\"Fit scaler on train only, transform val/test.\"\"\"\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    Xtr_s = ss.fit_transform(Xtr)\n",
        "    Xva_s = ss.transform(Xva)\n",
        "    Xte_s = ss.transform(Xte)\n",
        "    return Xtr_s, Xva_s, Xte_s\n",
        "\n",
        "def train_gboost(task, Xtr, ytr, Xva, yva, seed=0):\n",
        "    Xall = np.vstack([Xtr, Xva]); yall = np.concatenate([ytr, yva])\n",
        "    if task == 'clf':\n",
        "        model = GradientBoostingClassifier(random_state=seed)\n",
        "    else:\n",
        "        model = GradientBoostingRegressor(random_state=seed)\n",
        "    model.fit(Xall, yall)\n",
        "    return model\n",
        "\n",
        "def get_pred_score_vector(task, model, X):\n",
        "    \"\"\"Return a 1D score vector per sample for ExCIR: prob of positive class or regression output.\"\"\"\n",
        "    if task == 'clf':\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            p = model.predict_proba(X)[:, -1]\n",
        "        else:\n",
        "            # fallback to decision_function then logistic squash\n",
        "            s = model.decision_function(X)\n",
        "            p = 1/(1+np.exp(-s))\n",
        "        return p\n",
        "    else:\n",
        "        return model.predict(X)\n",
        "\n",
        "def spearman_safe(a, b):\n",
        "    v = spearmanr(a, b).correlation\n",
        "    if np.isnan(v):\n",
        "        return 0.0\n",
        "    return float(v)\n",
        "\n",
        "# -------------------------\n",
        "# ExCIR & baselines\n",
        "# -------------------------\n",
        "def excir_scores(X, p):\n",
        "    \"\"\"\n",
        "    Global ExCIR per-feature vs. a 1D score vector p.\n",
        "    Uses the standardized correlation shortcut:\n",
        "        CIR = rho^2 / (1 + rho^2)\n",
        "    \"\"\"\n",
        "    X = np.asarray(X)\n",
        "    p = np.asarray(p)\n",
        "    n, d = X.shape\n",
        "    ps = (p - p.mean()) / (p.std() + 1e-12)\n",
        "    res = np.zeros(d, dtype=float)\n",
        "    for j in range(d):\n",
        "        xj = X[:, j]\n",
        "        xj = (xj - xj.mean()) / (xj.std() + 1e-12)\n",
        "        rho = np.corrcoef(xj, ps)[0, 1]\n",
        "        if np.isnan(rho): rho = 0.0\n",
        "        r2 = rho * rho\n",
        "        res[j] = r2 / (1.0 + r2)\n",
        "    return res\n",
        "\n",
        "def surrogate_lr_scores(X, p, task='clf'):\n",
        "    \"\"\"Fit a linear surrogate to mimic teacher scores; return |coef| normalized.\"\"\"\n",
        "    X = np.asarray(X); p = np.asarray(p)\n",
        "    if task == 'clf':\n",
        "        # mimic logit if possible\n",
        "        p = np.clip(p, 1e-6, 1-1e-6)\n",
        "        ysur = np.log(p/(1-p))\n",
        "        reg = Ridge(alpha=1.0, random_state=0)\n",
        "    else:\n",
        "        ysur = p\n",
        "        reg = Ridge(alpha=1.0, random_state=0)\n",
        "    reg.fit(X, ysur)\n",
        "    w = np.abs(reg.coef_)\n",
        "    if w.ndim > 1:\n",
        "        w = np.linalg.norm(w, axis=0)\n",
        "    return (w + 1e-12) / (w.sum() + 1e-12)\n",
        "\n",
        "def pfi_scores(model, X, y, task='clf', n_repeats=10, seed=0):\n",
        "    imp = permutation_importance(model, X, y, n_repeats=n_repeats, random_state=seed, scoring=None)\n",
        "    return imp.importances_mean\n",
        "\n",
        "def tree_gain_scores(model, d):\n",
        "    try:\n",
        "        w = model.feature_importances_\n",
        "        if w is None or len(w) != d:\n",
        "            return np.zeros(d)\n",
        "        return w\n",
        "    except Exception:\n",
        "        return np.zeros(d)\n",
        "\n",
        "def pdp_var_scores(model, X, task='clf', grid_points=20, random_state=0):\n",
        "    \"\"\"\n",
        "    Simple PDP variance per feature using sklearn's partial dependence.\n",
        "    Note: PDP for non-tree models can be slower; we keep it for GBM.\n",
        "    \"\"\"\n",
        "    from sklearn.inspection import partial_dependence\n",
        "    d = X.shape[1]\n",
        "    scores = np.zeros(d)\n",
        "    rs = check_random_state(random_state)\n",
        "    # sample at most 2000 rows for PDP to keep it light\n",
        "    idx = rs.choice(np.arange(X.shape[0]), size=min(2000, X.shape[0]), replace=False)\n",
        "    Xs = X[idx]\n",
        "    for j in range(d):\n",
        "        try:\n",
        "            pd = partial_dependence(model, Xs, features=[j], kind='average', grid_resolution=grid_points)\n",
        "            curve = pd.average[0]  # shape (grid_points,)\n",
        "            scores[j] = np.var(curve)\n",
        "        except Exception:\n",
        "            scores[j] = 0.0\n",
        "    return scores\n",
        "\n",
        "def mi_pred_scores(X, p, task='clf'):\n",
        "    \"\"\"\n",
        "    Mutual information between each feature and the predicted score (binned).\n",
        "    For MI we discretize p via median split for classification; for regression, quantile bins.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X)\n",
        "    p = np.asarray(p)\n",
        "    if task == 'clf':\n",
        "        z = (p > np.median(p)).astype(int)\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "    else:\n",
        "        # 3-bin discretization for regression scores\n",
        "        z = np.digitize(p, np.quantile(p, [1/3, 2/3]))\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "\n",
        "def mi_label_scores(X, y, task='clf'):\n",
        "    if task == 'clf':\n",
        "        return mutual_info_classif(X, y, random_state=0)\n",
        "    else:\n",
        "        return mutual_info_regression(X, y, random_state=0)\n",
        "\n",
        "def hsic_pred_scores(X, p, max_d_for_hsic=512):\n",
        "    \"\"\"\n",
        "    Unbiased HSIC with RBF kernels (median heuristic).\n",
        "    For high-d feature spaces (e.g., TF-IDF), skip to avoid O(n^2 d) blow-up.\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    if d > max_d_for_hsic:\n",
        "        warnings.warn(f\"HSIC skipped (d={d} > {max_d_for_hsic}).\")\n",
        "        return np.zeros(d)\n",
        "    p = p.reshape(-1, 1)\n",
        "    # RBF kernel on p\n",
        "    def rbf_gram(v):\n",
        "        D = ((v - v.T) ** 2)\n",
        "        sigma2 = np.median(D[D > 0]) + 1e-12\n",
        "        K = np.exp(-D / (2.0 * sigma2))\n",
        "        return K, sigma2\n",
        "    L, _ = rbf_gram(p)\n",
        "    H = np.eye(n) - np.ones((n, n))/n\n",
        "    HLH = H @ L @ H\n",
        "    scores = np.zeros(d)\n",
        "    for j in range(d):\n",
        "        xj = X[:, j:j+1]\n",
        "        K, _ = rbf_gram(xj)\n",
        "        KH = K @ H\n",
        "        scores[j] = np.trace(KH @ HLH) / ((n-1)**2 + 1e-12)\n",
        "    return scores\n",
        "\n",
        "# -------------------------\n",
        "# Evaluation helpers\n",
        "# -------------------------\n",
        "def rank_from_scores(scores):\n",
        "    return np.argsort(-scores)\n",
        "\n",
        "def topk_sufficiency_eval(method_name, scores, task, Xtr, ytr, Xva, yva, Xte, yte, ks, seed=0):\n",
        "    ranks = rank_from_scores(scores)\n",
        "    metrics = []\n",
        "    for k in ks:\n",
        "        keep = ranks[:k]\n",
        "        if task == 'clf':\n",
        "            model_k = GradientBoostingClassifier(random_state=seed)\n",
        "        else:\n",
        "            model_k = GradientBoostingRegressor(random_state=seed)\n",
        "        Xtr_k = np.vstack([Xtr, Xva])[:, keep]\n",
        "        ytr_k = np.concatenate([ytr, yva])\n",
        "        model_k.fit(Xtr_k, ytr_k)\n",
        "        yhat = model_k.predict(Xte[:, keep])\n",
        "        if task == 'clf':\n",
        "            metric = accuracy_score(yte, yhat)\n",
        "        else:\n",
        "            metric = r2_score(yte, yhat)\n",
        "        metrics.append((k, metric))\n",
        "    return {\"method\": method_name, \"metrics\": metrics}\n",
        "\n",
        "def aopc_curves(task, model, Xte, yte, scores, steps=11):\n",
        "    idx = rank_from_scores(scores)\n",
        "    n, d = Xte.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f * d))\n",
        "        keep = idx[:k]\n",
        "        mask = np.zeros(d, dtype=bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0)\n",
        "        X_del = np.where(mask, 0.0, Xte)\n",
        "        if task == 'clf':\n",
        "            ins.append(accuracy_score(yte, model.predict(X_ins)))\n",
        "            dele.append(accuracy_score(yte, model.predict(X_del)))\n",
        "        else:\n",
        "            ins.append(r2_score(yte, model.predict(X_ins)))\n",
        "            dele.append(r2_score(yte, model.predict(X_del)))\n",
        "    return fracs, np.array(ins), np.array(dele)\n",
        "\n",
        "def stability_noise_eval(scores_fn, Xva, p_va, repeats=20, sigma=0.01):\n",
        "    base = scores_fn(Xva, p_va)\n",
        "    spearmans, jacc8 = [], []\n",
        "    for _ in range(repeats):\n",
        "        noise = np.random.normal(0, sigma, size=Xva.shape)\n",
        "        s_noisy = scores_fn(Xva + noise, p_va)\n",
        "        spearmans.append(spearman_safe(base, s_noisy))\n",
        "        jacc8.append(jaccard_topk(base, s_noisy, k=min(8, Xva.shape[1])))\n",
        "    return np.array(spearmans), np.array(jacc8)\n",
        "\n",
        "def jaccard_topk(a_scores, b_scores, k=8):\n",
        "    A = set(rank_from_scores(a_scores)[:k])\n",
        "    B = set(rank_from_scores(b_scores)[:k])\n",
        "    inter = len(A & B)\n",
        "    union = len(A | B) + 1e-12\n",
        "    return inter / union\n",
        "\n",
        "# -------------------------\n",
        "# Lightweight vs original (agreement & time)\n",
        "# -------------------------\n",
        "def lightweight_sweep(task, full_model, Xtr, ytr, Xva, yva, fractions=(0.2, 0.3, 0.4, 0.5, 0.75, 1.0), seed=0):\n",
        "    \"\"\"Train on a fraction of train+val and compare ExCIR ranks on the SAME validation split.\"\"\"\n",
        "    rng = check_random_state(seed)\n",
        "    Xbig = np.vstack([Xtr, Xva]); ybig = np.concatenate([ytr, yva])\n",
        "    # Full-ranking on validation:\n",
        "    p_full = get_pred_score_vector(task, full_model, Xva)\n",
        "    s_full = excir_scores(Xva, p_full)\n",
        "\n",
        "    times, sp_corrs, jac8 = [], [], []\n",
        "    for f in fractions:\n",
        "        t0 = time.time()\n",
        "        n_rows = int(f * Xbig.shape[0])\n",
        "        idx = rng.choice(np.arange(Xbig.shape[0]), size=n_rows, replace=False)\n",
        "        Xsub, ysub = Xbig[idx], ybig[idx]\n",
        "        if task == 'clf':\n",
        "            model = GradientBoostingClassifier(random_state=seed)\n",
        "        else:\n",
        "            model = GradientBoostingRegressor(random_state=seed)\n",
        "        model.fit(Xsub, ysub)\n",
        "        p_sub = get_pred_score_vector(task, model, Xva)\n",
        "        s_sub = excir_scores(Xva, p_sub)\n",
        "        t1 = time.time() - t0\n",
        "        times.append(t1)\n",
        "        sp_corrs.append(spearman_safe(s_full, s_sub))\n",
        "        jac8.append(jaccard_topk(s_full, s_sub, k=min(8, Xva.shape[1])))\n",
        "    return np.array(fractions), np.array(times), np.array(sp_corrs), np.array(jac8)\n",
        "\n",
        "# -------------------------\n",
        "# Datasets\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str\n",
        "    task: str\n",
        "    Xtr: np.ndarray\n",
        "    ytr: np.ndarray\n",
        "    Xva: np.ndarray\n",
        "    yva: np.ndarray\n",
        "    Xte: np.ndarray\n",
        "    yte: np.ndarray\n",
        "    feature_names: List[str]\n",
        "\n",
        "def load_adult(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "    dfX = ds.data.select_dtypes(include=[np.number]).copy()\n",
        "    y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "    X = dfX.to_numpy(dtype=float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"adult\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, list(dfX.columns))\n",
        "\n",
        "def load_california(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"california_housing\", version=1, as_frame=True)\n",
        "    X = ds.data.to_numpy(dtype=float)\n",
        "    y = ds.target.to_numpy(dtype=float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"california\", \"reg\", Xtr, ytr, Xva, yva, Xte, yte, list(ds.data.columns))\n",
        "\n",
        "def load_20ng_binary(seed=0) -> DatasetPack:\n",
        "    cats = ['comp.graphics', 'sci.space']\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "    te = fetch_20newsgroups(subset='test', categories=cats, remove=('headers','footers','quotes'))\n",
        "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
        "    Xtr_full = tfidf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "    Xte = tfidf.transform(te.data).astype(np.float32).toarray()\n",
        "    ytr_full = tr.target; yte = te.target\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2, stratify=ytr_full, random_state=seed)\n",
        "    # leave TF-IDF as-is (already standardized-like)\n",
        "    return DatasetPack(\"20ng_bin\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "def load_har(seed=0) -> DatasetPack:\n",
        "    # Some mirrors name it \"HAR\"; if unavailable, try \"Human Activity Recognition Using Smartphones\" via other sources\n",
        "    ds = fetch_openml(\"HAR\", version=1, as_frame=True)\n",
        "    X = ds.data.to_numpy(dtype=float)\n",
        "    y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"har6\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "def load_digits8x8(seed=0) -> DatasetPack:\n",
        "    digits = load_digits()  # 1797 samples, 8x8 grayscale\n",
        "    X = digits.data.astype(float) / 16.0\n",
        "    y = digits.target\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    # Standardize per-feature (pixel)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"digits8x8\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"pix{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "# -------------------------\n",
        "# Main run for one dataset\n",
        "# -------------------------\n",
        "def run_on_dataset(pack: DatasetPack, ks=(3,5,8,12), seed=0, do_shap=False, do_hsic=True):\n",
        "    ensure_out()\n",
        "    log(f\"\\n=== Dataset: {pack.name} (task={pack.task}) ===\")\n",
        "    # 1) Train base model on train+val\n",
        "    base_model = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    p_va = get_pred_score_vector(pack.task, base_model, pack.Xva)\n",
        "\n",
        "    # 2) Global scores on validation\n",
        "    scores = {}\n",
        "    t0 = time.time(); scores['ExCIR'] = excir_scores(pack.Xva, p_va); t1 = time.time()\n",
        "    log(f\"ExCIR computed in {t1-t0:.3f}s, d={pack.Xva.shape[1]}\")\n",
        "    # Baselines\n",
        "    t0 = time.time(); scores['PFI'] = pfi_scores(base_model, pack.Xva, pack.yva, task=pack.task, n_repeats=10, seed=seed); log(f\"PFI {time.time()-t0:.3f}s\")\n",
        "    scores['TreeGain'] = tree_gain_scores(base_model, pack.Xva.shape[1])\n",
        "    t0 = time.time(); scores['PDP-var'] = pdp_var_scores(base_model, pack.Xva, task=pack.task); log(f\"PDP-var {time.time()-t0:.3f}s\")\n",
        "    scores['MI(pred)'] = mi_pred_scores(pack.Xva, p_va, task=pack.task)\n",
        "    scores['MI(label)'] = mi_label_scores(pack.Xva, pack.yva, task=pack.task)\n",
        "    scores['Surrogate-LR'] = surrogate_lr_scores(pack.Xva, p_va, task=pack.task)\n",
        "\n",
        "    if do_hsic:\n",
        "        try:\n",
        "            t0 = time.time(); scores['HSIC(pred)'] = hsic_pred_scores(pack.Xva, p_va, max_d_for_hsic=512); log(f\"HSIC(pred) {time.time()-t0:.3f}s\")\n",
        "        except Exception as e:\n",
        "            log(f\"HSIC skipped: {e}\")\n",
        "\n",
        "    if do_shap and HAVE_SHAP:\n",
        "        try:\n",
        "            expl = shap.TreeExplainer(base_model)\n",
        "            phi = expl.shap_values(pack.Xva)\n",
        "            if isinstance(phi, list):  # multiclass -> average magnitude across classes\n",
        "                m = np.mean([np.mean(np.abs(p), axis=0) for p in phi], axis=0)\n",
        "            else:\n",
        "                m = np.mean(np.abs(phi), axis=0)\n",
        "            scores['SHAP'] = m\n",
        "        except Exception as e:\n",
        "            log(f\"SHAP failed: {e}\")\n",
        "\n",
        "    # 3) Top-k sufficiency (retrain) for a subset of methods to keep runtime moderate\n",
        "    methods_for_topk = [m for m in ['ExCIR','PFI','TreeGain','MI(pred)','SHAP','Surrogate-LR','PDP-var','MI(label)'] if m in scores]\n",
        "    rows = []\n",
        "    for m in methods_for_topk:\n",
        "        res = topk_sufficiency_eval(m, scores[m], pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, pack.Xte, pack.yte, ks, seed=seed)\n",
        "        for k, metric in res['metrics']:\n",
        "            rows.append((pack.name, m, k, metric))\n",
        "    # Append to CSV\n",
        "    import csv\n",
        "    with open(\"out/summary_topk.csv\", \"a\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"dataset\",\"method\",\"k\",\"metric\"])\n",
        "        for r in rows: writer.writerow(r)\n",
        "\n",
        "    # 4) AOPC insertion/deletion using ExCIR\n",
        "    fracs, ins, dele = aopc_curves(pack.task, base_model, pack.Xte, pack.yte, scores['ExCIR'], steps=9)\n",
        "    plt.figure(figsize=(6.5,4.3))\n",
        "    plt.plot(100*fracs, ins, marker='o', label='Insertion (keep top-% by ExCIR)')\n",
        "    plt.plot(100*fracs, dele, marker='o', label='Deletion (zero top-% by ExCIR)')\n",
        "    plt.xlabel('Revealed / removed top-% features'); plt.ylabel('Test ' + ('Accuracy' if pack.task=='clf' else 'R$^2$'))\n",
        "    plt.title(f'AOPC (ExCIR) — {pack.name}')\n",
        "    plt.legend(); plt.tight_layout(); plt.savefig(f\"out/aopc_{pack.name}.png\", dpi=160); plt.close()\n",
        "\n",
        "    # 5) Stability under small noise (ExCIR)\n",
        "    sp, j8 = stability_noise_eval(lambda X, p: excir_scores(X, p), pack.Xva, p_va, repeats=20, sigma=0.01)\n",
        "    plt.figure(figsize=(6.0,4.0))\n",
        "    plt.subplot(1,2,1); plt.hist(sp, bins=10); plt.title('Spearman under noise'); plt.xlabel('ρ'); plt.ylabel('count')\n",
        "    plt.subplot(1,2,2); plt.hist(j8, bins=10); plt.title('Jaccard@8 under noise'); plt.xlabel('overlap'); plt.tight_layout()\n",
        "    plt.savefig(f\"out/stability_{pack.name}.png\", dpi=160); plt.close()\n",
        "\n",
        "    # 6) Lightweight vs original agreement/time (ExCIR)  — PARETO\n",
        "fracs = (0.2, 0.3, 0.4, 0.5, 0.75, 1.0)\n",
        "F, T, C, J, _ = lightweight_sweep(pack.task, base_model,\n",
        "                                  pack.Xtr, pack.ytr, pack.Xva, pack.yva,\n",
        "                                  fractions=fracs, seed=seed)\n",
        "# Pareto: time vs agreement; marker size = Jaccard@8\n",
        "plt.figure(figsize=(6.0,4.5))\n",
        "plt.scatter(T, C, s=200*np.maximum(J, 0.05), alpha=0.8)\n",
        "for i, f in enumerate(F):\n",
        "    plt.text(T[i], C[i], f\"{int(100*f)}%\", ha='center', va='bottom', fontsize=9)\n",
        "plt.xlabel('Wall time (s)')\n",
        "plt.ylabel('CIR\\u03c1 (full vs light)')  # <-- y-axis is CIRρ\n",
        "plt.title(f'Agreement–cost (ExCIR) — {pack.name}')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"out/pareto_{pack.name}.png\", dpi=160)\n",
        "plt.close()\n",
        "\n",
        "# Runtime vs fraction stays the same\n",
        "plt.figure(figsize=(6.0,4.0))\n",
        "plt.plot([int(100*f) for f in F], T, marker='o')\n",
        "plt.xlabel('Fraction of rows kept (%)')\n",
        "plt.ylabel('Wall time (s)')\n",
        "plt.title(f'Runtime vs fraction — {pack.name}')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"out/runtime_{pack.name}.png\", dpi=160)\n",
        "plt.close()\n",
        "\n",
        "    # 7) Optional: correlation & Jaccard matrices across methods (if not too many features)\n",
        "    try:\n",
        "        meths = list(scores.keys())\n",
        "        # Spearman matrix\n",
        "        M = np.zeros((len(meths), len(meths)))\n",
        "        for i, a in enumerate(meths):\n",
        "            for j, b in enumerate(meths):\n",
        "                M[i,j] = spearman_safe(scores[a], scores[b])\n",
        "        plt.figure(figsize=(6.2,5.2))\n",
        "        plt.imshow(M, vmin=0, vmax=1, cmap='viridis'); plt.colorbar()\n",
        "        plt.xticks(range(len(meths)), meths, rotation=45, ha='right'); plt.yticks(range(len(meths)), meths)\n",
        "        plt.title(f\"Spearman of score vectors — {pack.name}\")\n",
        "        plt.tight_layout(); plt.savefig(f\"out/overlap_corr_{pack.name}.png\", dpi=160); plt.close()\n",
        "    except Exception as e:\n",
        "        log(f\"Skipped correlation heatmap: {e}\")\n",
        "\n",
        "    # 8) If digits image: make class-conditioned ExCIR maps & AOPC by pixels\n",
        "    if pack.name == \"digits8x8\":\n",
        "        # Train multinomial logistic regression for fast per-class probs\n",
        "        clf = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs', random_state=seed)\n",
        "        clf.fit(np.vstack([pack.Xtr, pack.Xva]), np.concatenate([pack.ytr, pack.yva]))\n",
        "        Pva = clf.predict_proba(pack.Xva)  # (n_va, 10)\n",
        "        # ExCIR per pixel for class '3' (as an example)\n",
        "        c = 3\n",
        "        s_c = excir_scores(pack.Xva, Pva[:, c])\n",
        "        # Heatmap (reshape 8x8)\n",
        "        H = s_c.reshape(8,8)\n",
        "        plt.figure(figsize=(4.5,4.5)); plt.imshow(H, cmap='viridis'); plt.colorbar()\n",
        "        plt.title('Class-conditioned ExCIR (digit 3)'); plt.tight_layout()\n",
        "        plt.savefig(\"out/digits_excir_class3.png\", dpi=160); plt.close()\n",
        "        # AOPC by pixels for class 3 uses the same aopc_curves with scores = s_c on test\n",
        "        yte_pred_model = clf  # reuse\n",
        "        fracs, ins, dele = aopc_curves(\"clf\", yte_pred_model, pack.Xte, pack.yte, s_c, steps=9)\n",
        "        plt.figure(figsize=(6.2,4.2))\n",
        "        plt.plot(100*fracs, ins, marker='o', label='Insertion (keep top-% pixels)')\n",
        "        plt.plot(100*fracs, dele, marker='o', label='Deletion (zero top-% pixels)')\n",
        "        plt.xlabel('Revealed / removed top-% pixels'); plt.ylabel('Test Accuracy'); plt.title('AOPC — digits (class 3 map)')\n",
        "        plt.legend(); plt.tight_layout(); plt.savefig(\"out/digits_aopc_class3.png\", dpi=160); plt.close()\n",
        "\n",
        "# -------------------------\n",
        "# Orchestrate all datasets\n",
        "# -------------------------\n",
        "ALL_DATASETS = {\n",
        "    \"adult\": load_adult,\n",
        "    \"california\": load_california,\n",
        "    \"20ng_bin\": load_20ng_binary,\n",
        "    \"har6\": load_har,\n",
        "    \"digits8x8\": load_digits8x8,\n",
        "}\n",
        "\n",
        "def main(argv=None):\n",
        "    parser = argparse.ArgumentParser(description=\"ExCIR cross-domain benchmarks\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--datasets\", type=str, default=\"adult,california,20ng_bin,har6,digits8x8\",\n",
        "                        help=\"comma-separated subset of datasets to run\")\n",
        "    parser.add_argument(\"--ks\", type=str, default=\"3,5,8,12\",\n",
        "                        help=\"top-k list for sufficiency, comma-separated\")\n",
        "    parser.add_argument(\"--no-shap\", action=\"store_true\", help=\"disable SHAP\")\n",
        "    parser.add_argument(\"--no-hsic\", action=\"store_true\", help=\"disable HSIC(pred)\")\n",
        "\n",
        "    # If argv is None (normal script run), accept unknown Jupyter flags; else parse the provided list.\n",
        "    if argv is None:\n",
        "        args, _ = parser.parse_known_args()\n",
        "    else:\n",
        "        args = parser.parse_args(argv)\n",
        "\n",
        "    ensure_out()\n",
        "    set_seed(args.seed)\n",
        "    with open(\"out/log.txt\", \"w\") as f:\n",
        "        f.write(\"ExCIR benchmark log\\n\")\n",
        "\n",
        "    ks = tuple(int(s) for s in args.ks.split(\",\"))\n",
        "    names = [s.strip() for s in args.datasets.split(\",\") if s.strip()]\n",
        "\n",
        "    # prepare CSV header\n",
        "    with open(\"out/summary_topk.csv\", \"w\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for nm in names:\n",
        "        if nm not in ALL_DATASETS:\n",
        "            log(f\"Unknown dataset key: {nm} (skip)\")\n",
        "            continue\n",
        "        try:\n",
        "            pack = ALL_DATASETS[nm](seed=args.seed)\n",
        "            run_on_dataset(pack, ks=ks, seed=args.seed,\n",
        "                           do_shap=(not args.no_shap and HAVE_SHAP),\n",
        "                           do_hsic=(not args.no_hsic))\n",
        "            agreement_graphs_for_dataset(pack, seed=args.seed)\n",
        "        except Exception as e:\n",
        "            log(f\"[{nm}] ERROR: {e}\")\n",
        "\n",
        "    log(\"\\nDone. See the 'out/' folder for CSVs and figures.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "nrxEGChZxpw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k heatmap from summary_topk.csv\n",
        "# ---------------------------------------------------------\n",
        "# Expects columns: dataset, method, k, metric\n",
        "# Creates: /mnt/data/topk_heatmap_all.png\n",
        "# ---------------------------------------------------------\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "csv_path = \"/out/summary_topk.csv\"  # change if your file is elsewhere\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Drop any repeated header rows embedded in the CSV\n",
        "mask_header_rows = (df[\"dataset\"].astype(str).str.lower() == \"dataset\") | (\n",
        "    df[\"method\"].astype(str).str.lower() == \"method\"\n",
        ")\n",
        "df = df[~mask_header_rows].copy()\n",
        "\n",
        "# Coerce types\n",
        "df[\"k\"] = pd.to_numeric(df[\"k\"], errors=\"coerce\")\n",
        "df[\"metric\"] = pd.to_numeric(df[\"metric\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"k\", \"metric\"])\n",
        "df[\"k\"] = df[\"k\"].astype(int)\n",
        "\n",
        "# Pivot: rows=(dataset, method), cols=k, values=metric\n",
        "pivot = (\n",
        "    df.pivot_table(index=[\"dataset\", \"method\"], columns=\"k\", values=\"metric\", aggfunc=\"mean\")\n",
        "    .sort_index(level=[0,1])\n",
        ")\n",
        "# Keep columns in 3,5,8,12 order if present\n",
        "col_order = [k for k in [3,5,8,12] if k in pivot.columns]\n",
        "pivot = pivot[col_order]\n",
        "\n",
        "# Build the heatmap with matplotlib only\n",
        "data = pivot.values\n",
        "n_rows, n_cols = data.shape\n",
        "\n",
        "height_per_row = 0.35\n",
        "fig_h = max(4, height_per_row * n_rows + 2)\n",
        "fig_w = 8\n",
        "fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
        "\n",
        "im = ax.imshow(data, aspect=\"auto\")  # default colormap\n",
        "\n",
        "# Ticks and labels\n",
        "ax.set_xticks(np.arange(n_cols))\n",
        "ax.set_xticklabels([str(c) for c in pivot.columns])\n",
        "row_labels = [f\"{idx[0]} — {idx[1]}\" for idx in pivot.index]\n",
        "ax.set_yticks(np.arange(n_rows))\n",
        "ax.set_yticklabels(row_labels)\n",
        "\n",
        "# Draw horizontal lines to separate datasets\n",
        "dataset_boundaries = []\n",
        "last_ds = None\n",
        "for r, (ds, _) in enumerate(pivot.index):\n",
        "    if last_ds is None:\n",
        "        last_ds = ds\n",
        "        continue\n",
        "    if ds != last_ds:\n",
        "        dataset_boundaries.append(r - 0.5)\n",
        "        last_ds = ds\n",
        "for y in dataset_boundaries:\n",
        "    ax.axhline(y=y, linewidth=2, alpha=0.6)\n",
        "\n",
        "# Annotate cells\n",
        "for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "        v = data[i, j]\n",
        "        text = \"—\" if np.isnan(v) else f\"{v:.3f}\"\n",
        "        ax.text(j, i, text, ha=\"center\", va=\"center\", fontsize=8)\n",
        "\n",
        "ax.set_xlabel(\"k (top-k features)\")\n",
        "ax.set_title(\"Top-k Accuracy Heatmap — datasets × methods\")\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "cbar.set_label(\"metric (accuracy)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "out_path = \"/mnt/data/topk_heatmap_all.png\"\n",
        "plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
        "print(f\"Saved: {out_path}\")\n"
      ],
      "metadata": {
        "id": "jW1QncrSxsSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Agreement-beyond-ranks: Spearman + Projection residual + KL(KDE)\n",
        "# (Drop-in addendum; no external deps beyond scipy/numpy/matplotlib)\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _projection_residual(y_full, y_light):\n",
        "    \"\"\"\n",
        "    Best affine alignment Y_full ≈ Y_light @ A + 1 b^T and its normalized residual.\n",
        "    Works for 1D or q-D outputs. Returns (residual, A, b).\n",
        "    \"\"\"\n",
        "    Y  = np.asarray(y_full)\n",
        "    Yp = np.asarray(y_light)\n",
        "\n",
        "    # Ensure 2D (n, q)\n",
        "    if Y.ndim == 1:  Y  = Y.reshape(-1, 1)\n",
        "    if Yp.ndim == 1: Yp = Yp.reshape(-1, 1)\n",
        "\n",
        "    n = Y.shape[0]\n",
        "    Phi = np.hstack([Yp, np.ones((n, 1))])          # (n, q+1)\n",
        "    Theta, *_ = np.linalg.lstsq(Phi, Y, rcond=None) # ((q+1) x q)\n",
        "    A = Theta[:-1, :]\n",
        "    b = Theta[-1, :].ravel()\n",
        "\n",
        "    Yhat = Phi @ Theta\n",
        "    resid = np.linalg.norm(Y - Yhat, ord='fro') / (np.linalg.norm(Y, ord='fro') + 1e-12)\n",
        "    return float(resid), A, b\n",
        "\n",
        "def _kl_kde_1d(y_full, y_light, grid_points=400):\n",
        "    \"\"\"\n",
        "    Symmetric KL between 1D KDEs (standardized outputs). Returns (KL_sym, KL(p||q), KL(q||p)).\n",
        "    Uses scipy.stats.gaussian_kde but imports locally to avoid touching your global imports.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy.stats import gaussian_kde\n",
        "\n",
        "    y  = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "\n",
        "    ys  = (y  - y.mean())  / (y.std()  + 1e-12)\n",
        "    yps = (yp - yp.mean()) / (yp.std() + 1e-12)\n",
        "\n",
        "    kde_p = gaussian_kde(ys)\n",
        "    kde_q = gaussian_kde(yps)\n",
        "\n",
        "    both = np.concatenate([ys, yps])\n",
        "    lo = np.percentile(both, 0.5)\n",
        "    hi = np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "\n",
        "    p = kde_p(xs); q = kde_q(xs)\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, None)\n",
        "    q = np.clip(q, eps, None)\n",
        "\n",
        "    dx = (hi - lo) / max(grid_points - 1, 1)\n",
        "    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))) * dx)\n",
        "    return 0.5 * (kl_pq + kl_qp), kl_pq, kl_qp\n",
        "\n",
        "def agreement_graphs_for_dataset(pack, seed=0, fractions=(0.2, 0.3, 0.4, 0.5, 0.75, 1.0)):\n",
        "    \"\"\"\n",
        "    Trains full model on train+val, then for each fraction re-trains on a subsample of rows;\n",
        "    computes ExCIR on the SAME validation split; collects:\n",
        "      - Spearman(full vs light) of ExCIR vectors (rank agreement)\n",
        "      - Projection residual between full and light predicted outputs (shape agreement)\n",
        "      - Symmetric KL between full and light outputs via KDE (calibration/distributional match)\n",
        "    Saves:\n",
        "      out/agreement_bundle_<dataset>.png  (3-panel line plot)\n",
        "      out/agreement_map_<dataset>.png     (scatter: Spearman vs residual, color=KL, size=Jaccard@8)\n",
        "    \"\"\"\n",
        "    ensure_out()\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    # Train FULL model on train+val\n",
        "    base_model = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    p_full = get_pred_score_vector(pack.task, base_model, pack.Xva)\n",
        "    s_full = excir_scores(pack.Xva, p_full)\n",
        "\n",
        "    Xbig = np.vstack([pack.Xtr, pack.Xva])\n",
        "    ybig = np.concatenate([pack.ytr, pack.yva])\n",
        "\n",
        "    F_list, T, SPEAR, JACC8, DPROJ, KLSYM = [], [], [], [], [], []\n",
        "\n",
        "    for f in fractions:\n",
        "        t0 = time.time()\n",
        "        n_rows = int(f * Xbig.shape[0])\n",
        "        idx = rng.choice(np.arange(Xbig.shape[0]), size=n_rows, replace=False)\n",
        "        Xsub, ysub = Xbig[idx], ybig[idx]\n",
        "\n",
        "        model = GradientBoostingClassifier(random_state=seed) if pack.task == 'clf' else GradientBoostingRegressor(random_state=seed)\n",
        "        model.fit(Xsub, ysub)\n",
        "        p_sub = get_pred_score_vector(pack.task, model, pack.Xva)\n",
        "        s_sub = excir_scores(pack.Xva, p_sub)\n",
        "\n",
        "        # Collect metrics\n",
        "        T.append(time.time() - t0)\n",
        "        SPEAR.append(spearman_safe(s_full, s_sub))\n",
        "        JACC8.append(jaccard_topk(s_full, s_sub, k=min(8, pack.Xva.shape[1])))\n",
        "\n",
        "        dproj, _, _ = _projection_residual(p_full, p_sub)\n",
        "        DPROJ.append(dproj)\n",
        "\n",
        "        kls, _, _ = _kl_kde_1d(p_full, p_sub)\n",
        "        KLSYM.append(kls)\n",
        "\n",
        "        F_list.append(f)\n",
        "\n",
        "    F = np.array(F_list)\n",
        "    T = np.array(T)\n",
        "    SPEAR = np.array(SPEAR)\n",
        "    JACC8 = np.array(JACC8)\n",
        "    DPROJ = np.array(DPROJ)\n",
        "    KLSYM = np.array(KLSYM)\n",
        "\n",
        "    # --- Figure 1: 3-panel line plot vs fraction ---\n",
        "    plt.figure(figsize=(9.4, 3.6))\n",
        "\n",
        "    ax1 = plt.subplot(1,3,1)\n",
        "    ax1.plot([int(100*f) for f in F], SPEAR, marker='o')\n",
        "    ax1.set_ylim(0, 1.0)\n",
        "    ax1.set_xlabel('% rows kept'); ax1.set_ylabel('Spearman (rank)')\n",
        "    ax1.set_title('Rank agreement')\n",
        "\n",
        "    ax2 = plt.subplot(1,3,2)\n",
        "    ax2.plot([int(100*f) for f in F], DPROJ, marker='o')\n",
        "    ax2.set_xlabel('% rows kept'); ax2.set_ylabel('Projection residual')\n",
        "    ax2.set_title('Shape agreement (affine residual)')\n",
        "\n",
        "    ax3 = plt.subplot(1,3,3)\n",
        "    ax3.plot([int(100*f) for f in F], KLSYM, marker='o')\n",
        "    ax3.set_xlabel('% rows kept'); ax3.set_ylabel('Sym. KL (KDE)')\n",
        "    ax3.set_title('Distribution match')\n",
        "\n",
        "    plt.suptitle(f'Agreement beyond ranks — {pack.name}', y=1.02, fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"out/agreement_bundle_{pack.name}.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    # --- Figure 2: Spearman vs residual scatter, color=KL, size=Jaccard@8 ---\n",
        "    plt.figure(figsize=(5.6, 4.8))\n",
        "    sc = plt.scatter(SPEAR, DPROJ, c=KLSYM, s=200*np.maximum(JACC8, 0.05), alpha=0.85)\n",
        "    for i, f in enumerate(F):\n",
        "        plt.text(SPEAR[i], DPROJ[i], f\"{int(100*f)}%\", ha='center', va='bottom', fontsize=9)\n",
        "    plt.xlabel('Spearman (rank)')\n",
        "    plt.ylabel('Projection residual (shape)')\n",
        "    cbar = plt.colorbar(sc); cbar.set_label('Symmetric KL (KDE)')\n",
        "    plt.title(f'Agreement map — {pack.name}')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"out/agreement_map_{pack.name}.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    # Optional: log quick summary\n",
        "    log(f\"[{pack.name}] Agreement beyond ranks:\")\n",
        "    log(f\"  Spearman @ {F}: {np.round(SPEAR, 3)}\")\n",
        "    log(f\"  Proj residual @ {F}: {np.round(DPROJ, 4)}\")\n",
        "    log(f\"  Sym KL (KDE) @ {F}: {np.round(KLSYM, 4)}\")\n",
        "    log(f\"  Jaccard@8 @ {F}: {np.round(JACC8, 3)}\")\n"
      ],
      "metadata": {
        "id": "4RIn9O69xuij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "ExCIR Benchmarks (tabular, text, signals, images) using the traditional pooled mid-mean CIR\n",
        "------------------------------------------------------------------------------------------\n",
        "Outputs (in ./out):\n",
        "  - summary_topk.csv               : Top-k sufficiency (per dataset, per method)\n",
        "  - aopc_<dataset>.png             : Insertion/Deletion curves using ExCIR ranking\n",
        "  - pareto_<dataset>.png           : Agreement–cost Pareto for lightweight fractions\n",
        "  - runtime_<dataset>.png          : End-to-end runtime vs fraction (lightweight)\n",
        "  - stability_<dataset>.png        : Rank stability under small noise (ExCIR)\n",
        "  - overlap_corr_<dataset>.png     : Method Spearman heatmaps (optional, for context)\n",
        "  - agreement_bundle_<dataset>.png : CIR (full vs light) + projection residual + KL (KDE)\n",
        "  - agreement_map_<dataset>.png    : Scatter: CIR vs projection residual, color=KL, size=Jaccard@8\n",
        "  - *_class_heatmaps.png           : Image (digits) class-conditioned ExCIR maps (optional)\n",
        "  - log.txt                        : A brief run log\n",
        "\n",
        "Notes:\n",
        "  • SHAP and HSIC are optional (skip if packages missing / dimensionality too large).\n",
        "  • Image experiments use sklearn 'digits' (8x8) to keep dependencies light and runnable everywhere.\n",
        "  • You can add CIFAR-10 / Cats-vs-Dogs / graph datasets if you install torchvision / torch-geometric.\n",
        "\"\"\"\n",
        "\n",
        "import os, sys, time, json, math, warnings, argparse, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, List\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from sklearn.datasets import fetch_openml, load_digits\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Optional imports\n",
        "try:\n",
        "    import shap\n",
        "    HAVE_SHAP = True\n",
        "except Exception:\n",
        "    HAVE_SHAP = False\n",
        "\n",
        "# -------------------------\n",
        "# Utility helpers\n",
        "# -------------------------\n",
        "def set_seed(seed: int = 0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def ensure_out():\n",
        "    os.makedirs(\"out\", exist_ok=True)\n",
        "\n",
        "def log(msg: str):\n",
        "    print(msg, flush=True)\n",
        "    with open(\"out/log.txt\", \"a\") as f:\n",
        "        f.write(msg + \"\\n\")\n",
        "\n",
        "def standardize_fit(Xtr, Xva, Xte):\n",
        "    \"\"\"Fit scaler on train only, transform val/test.\"\"\"\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    return ss.fit_transform(Xtr), ss.transform(Xva), ss.transform(Xte)\n",
        "\n",
        "def train_gboost(task, Xtr, ytr, Xva, yva, seed=0):\n",
        "    Xall = np.vstack([Xtr, Xva]); yall = np.concatenate([ytr, yva])\n",
        "    model = GradientBoostingClassifier(random_state=seed) if task=='clf' else GradientBoostingRegressor(random_state=seed)\n",
        "    model.fit(Xall, yall)\n",
        "    return model\n",
        "\n",
        "def get_pred_score_vector(task, model, X):\n",
        "    \"\"\"Return a 1D score vector per sample for ExCIR: prob of positive class or regression output.\"\"\"\n",
        "    if task == 'clf':\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            return model.predict_proba(X)[:, -1]\n",
        "        s = model.decision_function(X)\n",
        "        return 1/(1+np.exp(-s))\n",
        "    return model.predict(X)\n",
        "\n",
        "def spearman_safe(a, b):\n",
        "    v = spearmanr(a, b).correlation\n",
        "    return 0.0 if np.isnan(v) else float(v)\n",
        "\n",
        "# -------------------------\n",
        "# CIR: traditional pooled mid-mean\n",
        "# -------------------------\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    \"\"\"\n",
        "    Traditional CIR between two 1D signals z, s (length n):\n",
        "        m = (mean(z) + mean(s)) / 2\n",
        "        SS_between = n * [ (mean(z) - m)^2 + (mean(s) - m)^2 ]\n",
        "        SS_total   = sum_i (z_i - m)^2 + sum_i (s_i - m)^2\n",
        "        CIR        = SS_between / (SS_total + eps)\n",
        "    Returns a number in [0, 1].\n",
        "    \"\"\"\n",
        "    z = np.asarray(z).ravel()\n",
        "    s = np.asarray(s).ravel()\n",
        "    n = z.shape[0]\n",
        "    mz = z.mean(); ms = s.mean()\n",
        "    m = 0.5 * (mz + ms)\n",
        "    ss_between = n * ((mz - m)**2 + (ms - m)**2)\n",
        "    ss_total   = np.sum((z - m)**2) + np.sum((s - m)**2)\n",
        "    return float(ss_between / (ss_total + eps))\n",
        "\n",
        "def excir_scores_traditional(X: np.ndarray, p: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    ExCIR per-feature using the traditional pooled mid-mean CIR.\n",
        "    For each feature column x_j in X, compute CIR(x_j, p).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X); p = np.asarray(p).ravel()\n",
        "    d = X.shape[1]\n",
        "    out = np.zeros(d, dtype=float)\n",
        "    for j in range(d):\n",
        "        out[j] = cir_pair(X[:, j], p)\n",
        "    return out\n",
        "\n",
        "# -------------------------\n",
        "# Baselines\n",
        "# -------------------------\n",
        "def surrogate_lr_scores(X, p, task='clf'):\n",
        "    \"\"\"Fit a linear surrogate to mimic teacher scores; return |coef| normalized.\"\"\"\n",
        "    X = np.asarray(X); p = np.asarray(p)\n",
        "    if task == 'clf':\n",
        "        p = np.clip(p, 1e-6, 1-1e-6)\n",
        "        ysur = np.log(p/(1-p))\n",
        "        reg = Ridge(alpha=1.0, random_state=0)\n",
        "    else:\n",
        "        ysur = p\n",
        "        reg = Ridge(alpha=1.0, random_state=0)\n",
        "    reg.fit(X, ysur)\n",
        "    w = np.abs(reg.coef_)\n",
        "    if w.ndim > 1: w = np.linalg.norm(w, axis=0)\n",
        "    return (w + 1e-12) / (w.sum() + 1e-12)\n",
        "\n",
        "def pfi_scores(model, X, y, task='clf', n_repeats=10, seed=0):\n",
        "    imp = permutation_importance(model, X, y, n_repeats=n_repeats, random_state=seed, scoring=None)\n",
        "    return imp.importances_mean\n",
        "\n",
        "def tree_gain_scores(model, d):\n",
        "    try:\n",
        "        w = model.feature_importances_\n",
        "        return w if (w is not None and len(w)==d) else np.zeros(d)\n",
        "    except Exception:\n",
        "        return np.zeros(d)\n",
        "\n",
        "def pdp_var_scores(model, X, grid_points=20, random_state=0):\n",
        "    \"\"\"\n",
        "    Simple PDP variance per feature using sklearn's partial dependence (average PDP).\n",
        "    Use with tree-based GBM for speed.\n",
        "    \"\"\"\n",
        "    from sklearn.inspection import partial_dependence\n",
        "    d = X.shape[1]\n",
        "    scores = np.zeros(d)\n",
        "    rs = check_random_state(random_state)\n",
        "    idx = rs.choice(np.arange(X.shape[0]), size=min(2000, X.shape[0]), replace=False)\n",
        "    Xs = X[idx]\n",
        "    for j in range(d):\n",
        "        try:\n",
        "            pd = partial_dependence(model, Xs, features=[j], kind='average', grid_resolution=grid_points)\n",
        "            curve = pd.average[0]\n",
        "            scores[j] = np.var(curve)\n",
        "        except Exception:\n",
        "            scores[j] = 0.0\n",
        "    return scores\n",
        "\n",
        "def mi_pred_scores(X, p, task='clf'):\n",
        "    \"\"\"\n",
        "    Mutual information between each feature and the predicted score (binned).\n",
        "    For classification, bin p by median; for regression, 3-quantile bins.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X); p = np.asarray(p)\n",
        "    if task == 'clf':\n",
        "        z = (p > np.median(p)).astype(int)\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "    z = np.digitize(p, np.quantile(p, [1/3, 2/3]))\n",
        "    return mutual_info_classif(X, z, random_state=0)\n",
        "\n",
        "def mi_label_scores(X, y, task='clf'):\n",
        "    return mutual_info_classif(X, y, random_state=0) if task=='clf' else mutual_info_regression(X, y, random_state=0)\n",
        "\n",
        "# -------------------------\n",
        "# Evaluation helpers\n",
        "# -------------------------\n",
        "def rank_from_scores(scores):\n",
        "    return np.argsort(-scores)\n",
        "\n",
        "def topk_sufficiency_eval(method_name, scores, task, Xtr, ytr, Xva, yva, Xte, yte, ks, seed=0):\n",
        "    ranks = rank_from_scores(scores)\n",
        "    metrics = []\n",
        "    for k in ks:\n",
        "        keep = ranks[:k]\n",
        "        model_k = GradientBoostingClassifier(random_state=seed) if task=='clf' else GradientBoostingRegressor(random_state=seed)\n",
        "        Xtr_k = np.vstack([Xtr, Xva])[:, keep]\n",
        "        ytr_k = np.concatenate([ytr, yva])\n",
        "        model_k.fit(Xtr_k, ytr_k)\n",
        "        yhat = model_k.predict(Xte[:, keep])\n",
        "        metric = accuracy_score(yte, yhat) if task=='clf' else r2_score(yte, yhat)\n",
        "        metrics.append((k, metric))\n",
        "    return {\"method\": method_name, \"metrics\": metrics}\n",
        "\n",
        "def aopc_curves(task, model, Xte, yte, scores, steps=11):\n",
        "    idx = rank_from_scores(scores)\n",
        "    n, d = Xte.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f * d))\n",
        "        keep = idx[:k]\n",
        "        mask = np.zeros(d, dtype=bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0)\n",
        "        X_del = np.where(mask, 0.0, Xte)\n",
        "        if task == 'clf':\n",
        "            ins.append(accuracy_score(yte, model.predict(X_ins)))\n",
        "            dele.append(accuracy_score(yte, model.predict(X_del)))\n",
        "        else:\n",
        "            ins.append(r2_score(yte, model.predict(X_ins)))\n",
        "            dele.append(r2_score(yte, model.predict(X_del)))\n",
        "    return fracs, np.array(ins), np.array(dele)\n",
        "\n",
        "def stability_noise_eval(scores_fn, Xva, p_va, repeats=20, sigma=0.01):\n",
        "    base = scores_fn(Xva, p_va)\n",
        "    spearmans, jacc8 = [], []\n",
        "    for _ in range(repeats):\n",
        "        noise = np.random.normal(0, sigma, size=Xva.shape)\n",
        "        s_noisy = scores_fn(Xva + noise, p_va)\n",
        "        spearmans.append(spearman_safe(base, s_noisy))\n",
        "        jacc8.append(jaccard_topk(base, s_noisy, k=min(8, Xva.shape[1])))\n",
        "    return np.array(spearmans), np.array(jacc8)\n",
        "\n",
        "def jaccard_topk(a_scores, b_scores, k=8):\n",
        "    A = set(rank_from_scores(a_scores)[:k])\n",
        "    B = set(rank_from_scores(b_scores)[:k])\n",
        "    inter = len(A & B)\n",
        "    union = len(A | B) + 1e-12\n",
        "    return inter / union\n",
        "\n",
        "# -------------------------\n",
        "# Lightweight vs original: agreement & time\n",
        "# -------------------------\n",
        "def lightweight_sweep(task, full_model, Xtr, ytr, Xva, yva,\n",
        "                      fractions=(0.2, 0.3, 0.4, 0.5, 0.75, 1.0), seed=0):\n",
        "    rng = check_random_state(seed)\n",
        "    Xbig = np.vstack([Xtr, Xva]); ybig = np.concatenate([ytr, yva])\n",
        "\n",
        "    # Full outputs and full ExCIR ranking on validation:\n",
        "    p_full = get_pred_score_vector(task, full_model, Xva)\n",
        "    s_full = excir_scores_traditional(Xva, p_full)\n",
        "\n",
        "    times, cir_agree, jac8 = [], [], []\n",
        "    for f in fractions:\n",
        "        t0 = time.time()\n",
        "        n_rows = int(f * Xbig.shape[0])\n",
        "        idx = rng.choice(np.arange(Xbig.shape[0]), size=n_rows, replace=False)\n",
        "        Xsub, ysub = Xbig[idx], ybig[idx]\n",
        "        model = GradientBoostingClassifier(random_state=seed) if task=='clf' else GradientBoostingRegressor(random_state=seed)\n",
        "        model.fit(Xsub, ysub)\n",
        "        p_sub = get_pred_score_vector(task, model, Xva)\n",
        "        s_sub = excir_scores_traditional(Xva, p_sub)\n",
        "\n",
        "        times.append(time.time() - t0)\n",
        "        # >>> FIX: correlation-based CIR for output agreement <<<\n",
        "        cir_agree.append(cir_agree_rho(p_full, p_sub))   # or cir_agree_spearman(...)\n",
        "        jac8.append(jaccard_topk(s_full, s_sub, k=min(8, Xva.shape[1])))\n",
        "\n",
        "    return np.array(fractions), np.array(times), np.array(cir_agree), np.array(jac8), p_full\n",
        "\n",
        "# -------------------------\n",
        "# Agreement beyond ranks: CIR + Projection residual + KL(KDE)\n",
        "# -------------------------\n",
        "def _projection_residual(y_full, y_light):\n",
        "    \"\"\"\n",
        "    Best affine alignment Y_full ≈ Y_light @ A + 1 b^T and its normalized residual.\n",
        "    Works for 1D or q-D outputs. Returns (residual, A, b).\n",
        "    \"\"\"\n",
        "    Y  = np.asarray(y_full)\n",
        "    Yp = np.asarray(y_light)\n",
        "    if Y.ndim == 1:  Y  = Y.reshape(-1, 1)\n",
        "    if Yp.ndim == 1: Yp = Yp.reshape(-1, 1)\n",
        "    n = Y.shape[0]\n",
        "    Phi = np.hstack([Yp, np.ones((n, 1))])          # (n, q+1)\n",
        "    Theta, *_ = np.linalg.lstsq(Phi, Y, rcond=None) # ((q+1) x q)\n",
        "    A = Theta[:-1, :]\n",
        "    b = Theta[-1, :].ravel()\n",
        "    Yhat = Phi @ Theta\n",
        "    resid = np.linalg.norm(Y - Yhat, ord='fro') / (np.linalg.norm(Y, ord='fro') + 1e-12)\n",
        "    return float(resid), A, b\n",
        "\n",
        "def _kl_kde_1d(y_full, y_light, grid_points=400):\n",
        "    \"\"\"\n",
        "    Symmetric KL between 1D KDEs (standardized outputs). Returns (KL_sym, KL(p||q), KL(q||p)).\n",
        "    \"\"\"\n",
        "    from scipy.stats import gaussian_kde\n",
        "    y  = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "    ys  = (y  - y.mean())  / (y.std()  + 1e-12)\n",
        "    yps = (yp - yp.mean()) / (yp.std() + 1e-12)\n",
        "    kde_p = gaussian_kde(ys)\n",
        "    kde_q = gaussian_kde(yps)\n",
        "    both = np.concatenate([ys, yps])\n",
        "    lo = np.percentile(both, 0.5); hi = np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = kde_p(xs); q = kde_q(xs)\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, None); q = np.clip(q, eps, None)\n",
        "    dx = (hi - lo) / max(grid_points - 1, 1)\n",
        "    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))) * dx)\n",
        "    return 0.5 * (kl_pq + kl_qp), kl_pq, kl_qp\n",
        "\n",
        "def agreement_graphs_for_dataset(pack, seed=0, fractions=(0.2, 0.3, 0.4, 0.5, 0.75, 1.0)):\n",
        "    \"\"\"\n",
        "    Trains full model on train+val, then for each fraction re-trains on a subsample of rows;\n",
        "    computes ExCIR on the SAME validation split; collects:\n",
        "      - CIR(full vs light) of outputs (primary agreement)\n",
        "      - Projection residual between full and light predicted outputs (shape agreement)\n",
        "      - Symmetric KL between full and light outputs via KDE (calibration/distributional match)\n",
        "    Saves:\n",
        "      out/agreement_bundle_<dataset>.png  (3-panel line plot)\n",
        "      out/agreement_map_<dataset>.png     (scatter: CIR vs residual, color=KL, size=Jaccard@8)\n",
        "    \"\"\"\n",
        "    ensure_out()\n",
        "    # Train FULL model and compute outputs + full ExCIR\n",
        "    base_model = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    F, T, CIRs, JACC8, p_full = lightweight_sweep(pack.task, base_model,\n",
        "                                                  pack.Xtr, pack.ytr, pack.Xva, pack.yva,\n",
        "                                                  fractions=fractions, seed=seed)\n",
        "\n",
        "    # For residual & KL we need the light outputs again (retrain once more for clarity)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    Xbig = np.vstack([pack.Xtr, pack.Xva]); ybig = np.concatenate([pack.ytr, pack.yva])\n",
        "    DPROJ, KLSYM = [], []\n",
        "    for f in F:\n",
        "        n_rows = int(f * Xbig.shape[0])\n",
        "        idx = rng.choice(np.arange(Xbig.shape[0]), size=n_rows, replace=False)\n",
        "        model = GradientBoostingClassifier(random_state=seed) if pack.task=='clf' else GradientBoostingRegressor(random_state=seed)\n",
        "        model.fit(Xbig[idx], ybig[idx])\n",
        "        p_sub = get_pred_score_vector(pack.task, model, pack.Xva)\n",
        "        dproj, _, _ = _projection_residual(p_full, p_sub)\n",
        "        DPROJ.append(dproj)\n",
        "        kls, _, _ = _kl_kde_1d(p_full, p_sub)\n",
        "        KLSYM.append(kls)\n",
        "    DPROJ = np.array(DPROJ); KLSYM = np.array(KLSYM)\n",
        "\n",
        "    # --- Figure 1: 3-panel vs fraction ---\n",
        "    plt.figure(figsize=(9.4, 3.6))\n",
        "    ax1 = plt.subplot(1,3,1)\n",
        "    ax1.plot([int(100*f) for f in F], CIRs, marker='o')\n",
        "    ax1.set_ylim(0, 1.0)\n",
        "    ax1.set_xlabel('% rows kept'); #ax1.set_ylabel('CIR(full, light)')\n",
        "    #ax1.set_title('Agreement (CIR)')\n",
        "    ax1.set_ylabel('CIRρ(full, light)')\n",
        "    ax1.set_title('Agreement (CIRρ)')\n",
        "\n",
        "\n",
        "    ax2 = plt.subplot(1,3,2)\n",
        "    ax2.plot([int(100*f) for f in F], DPROJ, marker='o')\n",
        "    ax2.set_xlabel('% rows kept'); ax2.set_ylabel('Projection residual')\n",
        "    ax2.set_title('Shape agreement')\n",
        "\n",
        "    ax3 = plt.subplot(1,3,3)\n",
        "    ax3.plot([int(100*f) for f in F], KLSYM, marker='o')\n",
        "    ax3.set_xlabel('% rows kept'); ax3.set_ylabel('Sym. KL (KDE)')\n",
        "    ax3.set_title('Distribution match')\n",
        "\n",
        "    plt.suptitle(f'Agreement beyond ranks — {pack.name}', y=1.02, fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"out/agreement_bundle_{pack.name}.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    # --- Figure 2: CIR vs residual scatter, color=KL, size=Jaccard@8 ---\n",
        "    plt.figure(figsize=(5.8, 4.8))\n",
        "    sc = plt.scatter(CIRs, DPROJ, c=KLSYM, s=200*np.maximum(JACC8, 0.05), alpha=0.85)\n",
        "    for i, f in enumerate(F):\n",
        "        plt.text(CIRs[i], DPROJ[i], f\"{int(100*f)}%\", ha='center', va='bottom', fontsize=9)\n",
        "    plt.xlabel('CIR(full, light)')\n",
        "    plt.ylabel('Projection residual')\n",
        "    cbar = plt.colorbar(sc); cbar.set_label('Symmetric KL (KDE)')\n",
        "    plt.title(f'Agreement map — {pack.name}')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"out/agreement_map_{pack.name}.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    log(f\"[{pack.name}] Agreement beyond ranks (CIR primary):\")\n",
        "    log(f\"  CIR(full,light) @ {F}: {np.round(CIRs, 3)}\")\n",
        "    log(f\"  Proj residual   @ {F}: {np.round(DPROJ, 4)}\")\n",
        "    log(f\"  Sym KL (KDE)    @ {F}: {np.round(KLSYM, 4)}\")\n",
        "    log(f\"  Jaccard@8       @ {F}: {np.round(JACC8, 3)}\")\n",
        "\n",
        "# -------------------------\n",
        "# Datasets\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str\n",
        "    task: str\n",
        "    Xtr: np.ndarray\n",
        "    ytr: np.ndarray\n",
        "    Xva: np.ndarray\n",
        "    yva: np.ndarray\n",
        "    Xte: np.ndarray\n",
        "    yte: np.ndarray\n",
        "    feature_names: List[str]\n",
        "\n",
        "def load_adult(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "    dfX = ds.data.select_dtypes(include=[np.number]).copy()\n",
        "    y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "    X = dfX.to_numpy(dtype=float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"adult\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, list(dfX.columns))\n",
        "\n",
        "def load_california(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"california_housing\", version=1, as_frame=True)\n",
        "    X = ds.data.to_numpy(dtype=float)\n",
        "    y = ds.target.to_numpy(dtype=float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"california\", \"reg\", Xtr, ytr, Xva, yva, Xte, yte, list(ds.data.columns))\n",
        "\n",
        "def load_20ng_binary(seed=0) -> DatasetPack:\n",
        "    cats = ['comp.graphics', 'sci.space']\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "    te = fetch_20newsgroups(subset='test',  categories=cats, remove=('headers','footers','quotes'))\n",
        "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
        "    Xtr_full = tfidf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "    Xte = tfidf.transform(te.data).astype(np.float32).toarray()\n",
        "    ytr_full = tr.target; yte = te.target\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2, stratify=ytr_full, random_state=seed)\n",
        "    # TF-IDF left as-is\n",
        "    return DatasetPack(\"20ng_bin\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "def load_har(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"HAR\", version=1, as_frame=True)\n",
        "    X = ds.data.to_numpy(dtype=float)\n",
        "    y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"har6\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "def load_digits8x8(seed=0) -> DatasetPack:\n",
        "    digits = load_digits()  # 1797 samples, 8x8 grayscale\n",
        "    X = digits.data.astype(float) / 16.0\n",
        "    y = digits.target\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"digits8x8\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"pix{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "# -------------------------\n",
        "# Main run for one dataset\n",
        "# -------------------------\n",
        "def run_on_dataset(pack: DatasetPack, ks=(3,5,8,12), seed=0, do_shap=False, do_hsic=False):\n",
        "    ensure_out()\n",
        "    log(f\"\\n=== Dataset: {pack.name} (task={pack.task}) ===\")\n",
        "    # 1) Train base model on train+val\n",
        "    base_model = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    p_va = get_pred_score_vector(pack.task, base_model, pack.Xva)\n",
        "\n",
        "    # 2) Global scores on validation\n",
        "    scores = {}\n",
        "    t0 = time.time(); scores['ExCIR'] = excir_scores_traditional(pack.Xva, p_va); t1 = time.time()\n",
        "    log(f\"ExCIR(traditional) computed in {t1-t0:.3f}s, d={pack.Xva.shape[1]}\")\n",
        "    # Baselines\n",
        "    t0 = time.time(); scores['PFI'] = pfi_scores(base_model, pack.Xva, pack.yva, task=pack.task, n_repeats=10, seed=seed); log(f\"PFI {time.time()-t0:.3f}s\")\n",
        "    scores['TreeGain'] = tree_gain_scores(base_model, pack.Xva.shape[1])\n",
        "    t0 = time.time(); scores['PDP-var'] = pdp_var_scores(base_model, pack.Xva); log(f\"PDP-var {time.time()-t0:.3f}s\")\n",
        "    scores['MI(pred)'] = mi_pred_scores(pack.Xva, p_va, task=pack.task)\n",
        "    scores['MI(label)'] = mi_label_scores(pack.Xva, pack.yva, task=pack.task)\n",
        "    scores['Surrogate-LR'] = surrogate_lr_scores(pack.Xva, p_va, task=pack.task)\n",
        "\n",
        "    if do_shap and HAVE_SHAP:\n",
        "        try:\n",
        "            expl = shap.TreeExplainer(base_model)\n",
        "            phi = expl.shap_values(pack.Xva)\n",
        "            if isinstance(phi, list):\n",
        "                m = np.mean([np.mean(np.abs(p), axis=0) for p in phi], axis=0)\n",
        "            else:\n",
        "                m = np.mean(np.abs(phi), axis=0)\n",
        "            scores['SHAP'] = m\n",
        "        except Exception as e:\n",
        "            log(f\"SHAP failed: {e}\")\n",
        "\n",
        "    # 3) Top-k sufficiency (retrain) for a subset of methods to keep runtime moderate\n",
        "    methods_for_topk = [m for m in ['ExCIR','PFI','TreeGain','MI(pred)','SHAP','Surrogate-LR','PDP-var','MI(label)'] if m in scores]\n",
        "    rows = []\n",
        "    for m in methods_for_topk:\n",
        "        res = topk_sufficiency_eval(m, scores[m], pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, pack.Xte, pack.yte, ks, seed=seed)\n",
        "        for k, metric in res['metrics']:\n",
        "            rows.append((pack.name, m, k, metric))\n",
        "    # Append to CSV\n",
        "    import csv\n",
        "    with open(\"out/summary_topk.csv\", \"a\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"dataset\",\"method\",\"k\",\"metric\"])\n",
        "        for r in rows: writer.writerow(r)\n",
        "\n",
        "    # 4) AOPC insertion/deletion using ExCIR\n",
        "    fracs, ins, dele = aopc_curves(pack.task, base_model, pack.Xte, pack.yte, scores['ExCIR'], steps=9)\n",
        "    plt.figure(figsize=(6.5,4.3))\n",
        "    plt.plot(100*fracs, ins, marker='o', label='Insertion (keep top-% by ExCIR)')\n",
        "    plt.plot(100*fracs, dele, marker='o', label='Deletion (zero top-% by ExCIR)')\n",
        "    plt.xlabel('Revealed / removed top-% features'); plt.ylabel('Test ' + ('Accuracy' if pack.task=='clf' else 'R$^2$'))\n",
        "    plt.title(f'AOPC (ExCIR) — {pack.name}')\n",
        "    plt.legend(); plt.tight_layout(); plt.savefig(f\"out/aopc_{pack.name}.png\", dpi=160); plt.close()\n",
        "\n",
        "    # 5) Stability under small noise (ExCIR)\n",
        "    sp, j8 = stability_noise_eval(lambda X, p: excir_scores_traditional(X, p), pack.Xva, p_va, repeats=20, sigma=0.01)\n",
        "    plt.figure(figsize=(6.0,4.0))\n",
        "    plt.subplot(1,2,1); plt.hist(sp, bins=10); plt.title('Spearman under noise'); plt.xlabel('ρ'); plt.ylabel('count')\n",
        "    plt.subplot(1,2,2); plt.hist(j8, bins=10); plt.title('Jaccard@8 under noise'); plt.xlabel('overlap'); plt.tight_layout()\n",
        "    plt.savefig(f\"out/stability_{pack.name}.png\", dpi=160); plt.close()\n",
        "\n",
        "\n",
        "\n",
        "    # 6) Lightweight vs original agreement/time (ExCIR)  — PARETO\n",
        "    fracs = (0.2, 0.3, 0.4, 0.5, 0.75, 1.0)\n",
        "    F, T, C, J, _ = lightweight_sweep(pack.task, base_model,\n",
        "                                  pack.Xtr, pack.ytr, pack.Xva, pack.yva,\n",
        "                                  fractions=fracs, seed=seed)\n",
        "    # Pareto: time vs agreement; marker size = Jaccard@8\n",
        "    plt.figure(figsize=(6.0,4.5))\n",
        "    plt.scatter(T, C, s=200*np.maximum(J, 0.05), alpha=0.8)\n",
        "    for i, f in enumerate(F):\n",
        "         plt.text(T[i], C[i], f\"{int(100*f)}%\", ha='center', va='bottom', fontsize=9)\n",
        "    plt.xlabel('Wall time (s)')\n",
        "    plt.ylabel('CIR\\u03c1 (full vs light)')  # <-- y-axis is CIRρ\n",
        "    plt.title(f'Agreement–cost (ExCIR) — {pack.name}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"out/pareto_{pack.name}.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    # Runtime vs fraction stays the same\n",
        "    plt.figure(figsize=(6.0,4.0))\n",
        "    plt.plot([int(100*f) for f in F], T, marker='o')\n",
        "    plt.xlabel('Fraction of rows kept (%)')\n",
        "    plt.ylabel('Wall time (s)')\n",
        "    plt.title(f'Runtime vs fraction — {pack.name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"out/runtime_{pack.name}.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    # 7) (optional) correlation heatmap across methods for context\n",
        "    try:\n",
        "        meths = list(scores.keys())\n",
        "        M = np.zeros((len(meths), len(meths)))\n",
        "        for i, a in enumerate(meths):\n",
        "            for j, b in enumerate(meths):\n",
        "                M[i,j] = spearman_safe(scores[a], scores[b])\n",
        "        plt.figure(figsize=(6.2,5.2))\n",
        "        plt.imshow(M, vmin=0, vmax=1, cmap='viridis'); plt.colorbar()\n",
        "        plt.xticks(range(len(meths)), meths, rotation=45, ha='right'); plt.yticks(range(len(meths)), meths)\n",
        "        plt.title(f\"Spearman of score vectors — {pack.name}\")\n",
        "        plt.tight_layout(); plt.savefig(f\"out/overlap_corr_{pack.name}.png\", dpi=160); plt.close()\n",
        "    except Exception as e:\n",
        "        log(f\"Skipped correlation heatmap: {e}\")\n",
        "\n",
        "    # 8) If digits image: class-conditioned ExCIR maps & AOPC by pixels\n",
        "    if pack.name == \"digits8x8\":\n",
        "        clf = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs', random_state=seed)\n",
        "        clf.fit(np.vstack([pack.Xtr, pack.Xva]), np.concatenate([pack.ytr, pack.yva]))\n",
        "        Pva = clf.predict_proba(pack.Xva)  # (n_va, 10)\n",
        "        c = 3\n",
        "        s_c = excir_scores_traditional(pack.Xva, Pva[:, c])\n",
        "        H = s_c.reshape(8,8)\n",
        "        plt.figure(figsize=(4.5,4.5)); plt.imshow(H, cmap='viridis'); plt.colorbar()\n",
        "        plt.title('Class-conditioned ExCIR (digit 3)'); plt.tight_layout()\n",
        "        plt.savefig(\"out/digits_excir_class3.png\", dpi=160); plt.close()\n",
        "\n",
        "        fracs, ins, dele = aopc_curves(\"clf\", clf, pack.Xte, pack.yte, s_c, steps=9)\n",
        "        plt.figure(figsize=(6.2,4.2))\n",
        "        plt.plot(100*fracs, ins, marker='o', label='Insertion (keep top-% pixels)')\n",
        "        plt.plot(100*fracs, dele, marker='o', label='Deletion (zero top-% pixels)')\n",
        "        plt.xlabel('Revealed / removed top-% pixels'); plt.ylabel('Test Accuracy'); plt.title('AOPC — digits (class 3 map)')\n",
        "        plt.legend(); plt.tight_layout(); plt.savefig(\"out/digits_aopc_class3.png\", dpi=160); plt.close()\n",
        "\n",
        "# -------------------------\n",
        "# Orchestrate all datasets\n",
        "# -------------------------\n",
        "ALL_DATASETS = {\n",
        "    \"adult\": load_adult,\n",
        "    \"california\": load_california,\n",
        "    \"20ng_bin\": load_20ng_binary,\n",
        "    \"har6\": load_har,\n",
        "    \"digits8x8\": load_digits8x8,\n",
        "}\n",
        "\n",
        "def main(argv=None):\n",
        "    parser = argparse.ArgumentParser(description=\"ExCIR cross-domain benchmarks (traditional CIR)\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--datasets\", type=str, default=\"adult,california,20ng_bin,har6,digits8x8\",\n",
        "                        help=\"comma-separated subset of datasets to run\")\n",
        "    parser.add_argument(\"--ks\", type=str, default=\"3,5,8,12\",\n",
        "                        help=\"top-k list for sufficiency, comma-separated\")\n",
        "    parser.add_argument(\"--no-shap\", action=\"store_true\", help=\"disable SHAP\")\n",
        "    parser.add_argument(\"--no-hsic\", action=\"store_true\", help=\"disable HSIC(pred) [not used by default here]\")\n",
        "\n",
        "    # Jupyter/Colab friendly parsing\n",
        "    if argv is None:\n",
        "        args, _ = parser.parse_known_args()\n",
        "    else:\n",
        "        args = parser.parse_args(argv)\n",
        "\n",
        "    ensure_out()\n",
        "    set_seed(args.seed)\n",
        "    with open(\"out/log.txt\", \"w\") as f:\n",
        "        f.write(\"ExCIR (traditional) benchmark log\\n\")\n",
        "\n",
        "    ks = tuple(int(s) for s in args.ks.split(\",\"))\n",
        "    names = [s.strip() for s in args.datasets.split(\",\") if s.strip()]\n",
        "\n",
        "    # fresh CSV\n",
        "    with open(\"out/summary_topk.csv\", \"w\") as f:\n",
        "        f.write(\"dataset,method,k,metric\\n\")\n",
        "\n",
        "    for nm in names:\n",
        "        if nm not in ALL_DATASETS:\n",
        "            log(f\"Unknown dataset key: {nm} (skip)\")\n",
        "            continue\n",
        "        try:\n",
        "            pack = ALL_DATASETS[nm](seed=args.seed)\n",
        "            run_on_dataset(pack, ks=ks, seed=args.seed,\n",
        "                           do_shap=(not args.no_shap and HAVE_SHAP),\n",
        "                           do_hsic=(not args.no_hsic))\n",
        "            # CIR-first agreement bundle:\n",
        "            agreement_graphs_for_dataset(pack, seed=args.seed)\n",
        "        except Exception as e:\n",
        "            log(f\"[{nm}] ERROR: {e}\")\n",
        "\n",
        "    log(\"\\nDone. See the 'out/' folder for CSVs and figures.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "HzdxQfWfxxBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# excir_lib.py\n",
        "# ==========================================================\n",
        "# ExCIR library (original midpoint/ANOVA CIR only — no rho shortcut)\n",
        "# - per-feature ExCIR for any data\n",
        "# - block ExCIR (scalar & vector outputs) via ridge-regularized CCA\n",
        "# - class-conditioned image ExCIR (pixels / patches)\n",
        "# - lightweight environment checks: CIR-agreement, projection residual, MMD, KDE-KL\n",
        "# - AOPC utilities\n",
        "# ==========================================================\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Iterable, List, Tuple, Optional, Dict, Callable\n",
        "\n",
        "# Optional (for KDE-KL)\n",
        "try:\n",
        "    from scipy.stats import gaussian_kde\n",
        "    HAVE_SCIPY = True\n",
        "except Exception:\n",
        "    HAVE_SCIPY = False\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Core: CIR between two scalar signals (original formula)\n",
        "# ----------------------------------------------------------\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Canonical Information Ratio (original midpoint/ANOVA form) between two 1D signals z and s:\n",
        "\n",
        "        m = 0.5*(mean(z) + mean(s))\n",
        "        SS_between = n * [(mean(z)-m)^2 + (mean(s)-m)^2] = (n/2) * (mean(z)-mean(s))^2\n",
        "        SS_total   = sum_i (z_i - m)^2 + sum_i (s_i - m)^2\n",
        "        CIR        = SS_between / SS_total  ∈ [0,1]\n",
        "\n",
        "    This is the *only* formula used throughout (no rho shortcut).\n",
        "    \"\"\"\n",
        "    z = np.asarray(z, dtype=float).ravel()\n",
        "    s = np.asarray(s, dtype=float).ravel()\n",
        "    assert z.shape == s.shape and z.ndim == 1, \"z and s must be same-length 1D arrays\"\n",
        "    n = z.size\n",
        "    mz, ms = z.mean(), s.mean()\n",
        "    m = 0.5 * (mz + ms)\n",
        "    ss_between = (n / 2.0) * (mz - ms) ** 2\n",
        "    ss_total = ((z - m) ** 2).sum() + ((s - m) ** 2).sum()\n",
        "    if ss_total <= 0:\n",
        "        return 0.0\n",
        "    return float(ss_between / ss_total)\n",
        "\n",
        "def cir_agreement(score_full: np.ndarray, score_light: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    CIR-style *agreement* between two score vectors across features using the same original CIR.\n",
        "    Treat the two score vectors as paired 1D signals (features = \"samples\").\n",
        "    \"\"\"\n",
        "    a = np.asarray(score_full, dtype=float).ravel()\n",
        "    b = np.asarray(score_light, dtype=float).ravel()\n",
        "    return cir_pair(a, b)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Per-feature ExCIR (global, 1D output scores)\n",
        "# ----------------------------------------------------------\n",
        "def excir_feature_scores(X: np.ndarray, p: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    ExCIR per feature j: CIR( X[:, j], p ) using the original midpoint formula.\n",
        "    Works for tabular/text/signal data; p = model score/prob/regression output on SAME samples.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(p, dtype=float).ravel()\n",
        "    assert X.shape[0] == p.size\n",
        "    _, d = X.shape\n",
        "    out = np.zeros(d, dtype=float)\n",
        "    for j in range(d):\n",
        "        out[j] = cir_pair(X[:, j], p)\n",
        "    return out\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Block ExCIR via ridge-regularized CCA (scalar & vector)\n",
        "# ----------------------------------------------------------\n",
        "def _safe_cov(X: np.ndarray) -> np.ndarray:\n",
        "    Xc = X - X.mean(axis=0, keepdims=True)\n",
        "    n = max(X.shape[0], 1)\n",
        "    return (Xc.T @ Xc) / n\n",
        "\n",
        "def _cca_scalar_y_block_w(Xb: np.ndarray, y: np.ndarray, ridge: float = 1e-6) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Scalar-output block direction:\n",
        "      w* ∝ (Σ_xx + λI)^(-1) Σ_xy, then scale so Var(w^T X_centered)=1 and flip sign so Cov(z_centered, y_centered) ≥ 0.\n",
        "    IMPORTANT: We *evaluate* z on ORIGINAL Xb (not centered) so its mean is not forced to 0,\n",
        "               which is required by the original CIR formula.\n",
        "    \"\"\"\n",
        "    Xb = np.asarray(Xb, dtype=float); y = np.asarray(y, dtype=float).ravel()\n",
        "    n, pb = Xb.shape\n",
        "    Xc = Xb - Xb.mean(axis=0, keepdims=True)\n",
        "    yc = y - y.mean()\n",
        "    Sxx = _safe_cov(Xb) + ridge * np.eye(pb)\n",
        "    Sxy = (Xc.T @ yc) / max(n, 1)\n",
        "    w = np.linalg.solve(Sxx, Sxy)                 # pb,\n",
        "    zc = Xc @ w                                   # centered z for scaling/sign\n",
        "    vz = zc.var()\n",
        "    if vz <= 1e-12:\n",
        "        return np.zeros(pb)\n",
        "    w = w / np.sqrt(vz + 1e-12)\n",
        "    if (zc * yc).mean() < 0:\n",
        "        w = -w\n",
        "    return w\n",
        "\n",
        "def _cca_vector(Xb: np.ndarray, Y: np.ndarray, ridge_x: float = 1e-6, ridge_y: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Vector-output CCA:\n",
        "      Solve for w (block side) and u (output side) with ridge regularization.\n",
        "      Scale so Var(z_centered)=Var(s_centered)=1 and Cov(z_centered, s_centered) ≥ 0.\n",
        "      Then *evaluate* z = Xb @ w, s = Y @ u on ORIGINAL arrays for the original CIR formula.\n",
        "    \"\"\"\n",
        "    Xb = np.asarray(Xb, dtype=float)\n",
        "    Y = np.asarray(Y, dtype=float)\n",
        "    if Y.ndim == 1:\n",
        "        Y = Y.reshape(-1, 1)\n",
        "    n, pb = Xb.shape\n",
        "    q = Y.shape[1]\n",
        "\n",
        "    Xc = Xb - Xb.mean(axis=0, keepdims=True)\n",
        "    Yc = Y - Y.mean(axis=0, keepdims=True)\n",
        "\n",
        "    Sxx = _safe_cov(Xb) + ridge_x * np.eye(pb)\n",
        "    Syy = _safe_cov(Y)  + ridge_y * np.eye(q)\n",
        "    Gamma = (Xc.T @ Yc) / max(n, 1)               # pb x q\n",
        "\n",
        "    A = np.linalg.solve(Sxx, Gamma) @ np.linalg.solve(Syy, Gamma.T)  # pb x pb\n",
        "    lam, V = np.linalg.eig(A)\n",
        "    k = int(np.argmax(np.real(lam)))\n",
        "    w = np.real(V[:, k])\n",
        "\n",
        "    zc = Xc @ w\n",
        "    vz = zc.var()\n",
        "    if vz <= 1e-12:\n",
        "        return np.zeros_like(w), np.zeros(q)\n",
        "    w = w / np.sqrt(vz + 1e-12)\n",
        "    zc = zc / np.sqrt(vz + 1e-12)\n",
        "\n",
        "    u = np.linalg.solve(Syy, Gamma.T @ w)         # unscaled\n",
        "    sc = Yc @ u\n",
        "    vs = sc.var()\n",
        "    if vs > 1e-12:\n",
        "        u = u / np.sqrt(vs + 1e-12)\n",
        "        sc = sc / np.sqrt(vs + 1e-12)\n",
        "\n",
        "    if (zc * sc).mean() < 0:\n",
        "        w, u = -w, -u\n",
        "    return w, u\n",
        "\n",
        "def block_excir_scores(\n",
        "    X: np.ndarray,\n",
        "    target: np.ndarray,\n",
        "    blocks: List[Iterable[int]],\n",
        "    vector_output: bool = False,\n",
        "    ridge_x: float = 1e-6,\n",
        "    ridge_y: float = 1e-6,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    ExCIR per block using the original CIR between canonical summaries:\n",
        "      scalar target:  z_b = w^T Xb, CIR(z_b, y)\n",
        "      vector target:  (w,u) via CCA, z_b = w^T Xb, s_b = u^T Y, CIR(z_b, s_b)\n",
        "    NOTE: z_b and s_b are evaluated on ORIGINAL X and Y (means not forced to 0),\n",
        "          so the midpoint form is meaningful.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    tgt = np.asarray(target, dtype=float)\n",
        "    out = np.zeros(len(blocks), dtype=float)\n",
        "\n",
        "    for bi, idxs in enumerate(blocks):\n",
        "        cols = np.array(list(idxs), dtype=int)\n",
        "        Xb = X[:, cols]\n",
        "        if not vector_output:\n",
        "            if tgt.ndim != 1:\n",
        "                raise ValueError(\"Scalar target expected for vector_output=False.\")\n",
        "            w = _cca_scalar_y_block_w(Xb, tgt, ridge=ridge_x)\n",
        "            z = Xb @ w                  # ORIGINAL Xb (not centered)\n",
        "            out[bi] = cir_pair(z, tgt)\n",
        "        else:\n",
        "            Y = tgt if tgt.ndim == 2 else tgt.reshape(-1, 1)\n",
        "            w, u = _cca_vector(Xb, Y, ridge_x=ridge_x, ridge_y=ridge_y)\n",
        "            z = Xb @ w                  # ORIGINAL data\n",
        "            s = Y @ u\n",
        "            out[bi] = cir_pair(z, s)\n",
        "    return out\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Class-conditioned ExCIR for images (pixels / patches)\n",
        "# ----------------------------------------------------------\n",
        "def image_pixel_excir_scores(X_images: np.ndarray, p_class: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Per-pixel ExCIR for class c across many images, using original CIR per pixel: CIR(x_pix, p_c).\n",
        "    X_images: (n,H,W) or (n,H,W,C). Returns a heatmap of shape (H,W[,C]).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X_images, dtype=float)\n",
        "    p = np.asarray(p_class, dtype=float).ravel()\n",
        "    assert X.shape[0] == p.size\n",
        "    n = X.shape[0]\n",
        "    flat = X.reshape(n, -1)\n",
        "    scores = excir_feature_scores(flat, p)   # original CIR inside\n",
        "    return scores.reshape(X.shape[1:])\n",
        "\n",
        "def make_patch_blocks(H: int, W: int, patch: Tuple[int,int]) -> List[np.ndarray]:\n",
        "    \"\"\"Non-overlapping patch blocks on HxW grid (row-major flatten).\"\"\"\n",
        "    ph, pw = patch\n",
        "    blocks = []\n",
        "    for r in range(0, H, ph):\n",
        "        for c in range(0, W, pw):\n",
        "            rr = np.arange(r, min(r+ph, H))\n",
        "            cc = np.arange(c, min(c+pw, W))\n",
        "            R, C = np.meshgrid(rr, cc, indexing='ij')\n",
        "            idxs = (R * W + C).ravel()\n",
        "            blocks.append(idxs)\n",
        "    return blocks\n",
        "\n",
        "def image_patch_excir_scores(\n",
        "    X_images: np.ndarray,\n",
        "    p_class: np.ndarray,\n",
        "    patch: Tuple[int,int]=(4,4),\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Patch-level ExCIR: block ExCIR with scalar target p_class on (n,H,W) or (n,H,W,C).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X_images, dtype=float)\n",
        "    p = np.asarray(p_class, dtype=float).ravel()\n",
        "    assert X.shape[0] == p.size\n",
        "    n = X.shape[0]\n",
        "    if X.ndim == 3:\n",
        "        H, W = X.shape[1:3]\n",
        "        Xflat = X.reshape(n, H*W)\n",
        "        blocks = make_patch_blocks(H, W, patch)\n",
        "    elif X.ndim == 4:\n",
        "        H, W, C = X.shape[1], X.shape[2], X.shape[3]\n",
        "        Xflat = X.reshape(n, H*W*C)\n",
        "        blocks = []\n",
        "        ph, pw = patch\n",
        "        for r in range(0, H, ph):\n",
        "            for c in range(0, W, pw):\n",
        "                rr = np.arange(r, min(r+ph, H))\n",
        "                cc = np.arange(c, min(c+pw, W))\n",
        "                R, Cc = np.meshgrid(rr, cc, indexing='ij')\n",
        "                base = (R * W + Cc).ravel()\n",
        "                idxs = []\n",
        "                for ch in range(C):\n",
        "                    idxs.append(base + ch * (H*W))\n",
        "                blocks.append(np.concatenate(idxs))\n",
        "    else:\n",
        "        raise ValueError(\"X_images must be (n,H,W) or (n,H,W,C).\")\n",
        "    return block_excir_scores(Xflat, p, blocks, vector_output=False)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Lightweight environment: 3 checks (agreement, residual, MMD, KDE-KL)\n",
        "# ----------------------------------------------------------\n",
        "def projection_alignment_residual(y_full: np.ndarray, y_light: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Best affine fit Y ≈ [Y_light, 1] Θ; returns normalized Frobenius residual ||Y - Yhat|| / ||Y||.\n",
        "    Works for 1D or qD outputs (treat outputs per sample as vectors).\n",
        "    \"\"\"\n",
        "    Y  = np.asarray(y_full, dtype=float)\n",
        "    Yp = np.asarray(y_light, dtype=float)\n",
        "    if Y.ndim == 1:  Y  = Y.reshape(-1, 1)\n",
        "    if Yp.ndim == 1: Yp = Yp.reshape(-1, 1)\n",
        "    n = Y.shape[0]\n",
        "    Phi = np.hstack([Yp, np.ones((n, 1))])\n",
        "    Theta, *_ = np.linalg.lstsq(Phi, Y, rcond=None)\n",
        "    Yhat = Phi @ Theta\n",
        "    num = np.linalg.norm(Y - Yhat, ord='fro')\n",
        "    den = np.linalg.norm(Y, ord='fro') + 1e-12\n",
        "    return float(num / den)\n",
        "\n",
        "def mmd_gaussian(y_full: np.ndarray, y_light: np.ndarray, sigma2: Optional[float] = None) -> float:\n",
        "    \"\"\"\n",
        "    Unbiased MMD^2 with Gaussian kernel for 1D or qD outputs (median heuristic if sigma2 None).\n",
        "    \"\"\"\n",
        "    Y = np.asarray(y_full, dtype=float); Z = np.asarray(y_light, dtype=float)\n",
        "    if Y.ndim == 1: Y = Y.reshape(-1, 1)\n",
        "    if Z.ndim == 1: Z = Z.reshape(-1, 1)\n",
        "    n, q = Y.shape; m, q2 = Z.shape; assert q == q2\n",
        "\n",
        "    def rbf(X, Y, s2):\n",
        "        XX = np.sum(X**2, axis=1, keepdims=True)\n",
        "        YY = np.sum(Y**2, axis=1, keepdims=True)\n",
        "        D = XX + YY.T - 2 * (X @ Y.T)\n",
        "        return np.exp(-D / (2.0 * s2))\n",
        "\n",
        "    if sigma2 is None:\n",
        "        W = np.vstack([Y, Z])\n",
        "        D = np.sum((W[:, None, :] - W[None, :, :])**2, axis=2)\n",
        "        sigma2 = float(np.median(D[D > 0]) + 1e-12)\n",
        "\n",
        "    Kyy = rbf(Y, Y, sigma2); Kzz = rbf(Z, Z, sigma2); Kyz = rbf(Y, Z, sigma2)\n",
        "    np.fill_diagonal(Kyy, 0.0); np.fill_diagonal(Kzz, 0.0)\n",
        "    term = Kyy.sum()/(n*(n-1) + 1e-12) + Kzz.sum()/(m*(m-1) + 1e-12) - 2.0 * Kyz.mean()\n",
        "    return float(max(term, 0.0))\n",
        "\n",
        "def kde_kl_symmetric(y_full: np.ndarray, y_light: np.ndarray, grid_points: int = 400) -> float:\n",
        "    \"\"\"\n",
        "    Symmetric KL between KDEs (standardized 1D outputs). Requires SciPy.\n",
        "    \"\"\"\n",
        "    if not HAVE_SCIPY:\n",
        "        raise RuntimeError(\"SciPy required for KDE-KL. Please install scipy.\")\n",
        "    y  = np.asarray(y_full, dtype=float).ravel()\n",
        "    yp = np.asarray(y_light, dtype=float).ravel()\n",
        "    ys  = (y  - y.mean())  / (y.std()  + 1e-12)\n",
        "    yps = (yp - yp.mean()) / (yp.std() + 1e-12)\n",
        "    kde_p = gaussian_kde(ys); kde_q = gaussian_kde(yps)\n",
        "    both = np.concatenate([ys, yps]); lo = np.percentile(both, 0.5); hi = np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None); q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo) / max(grid_points - 1, 1)\n",
        "    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))) * dx)\n",
        "    return 0.5 * (kl_pq + kl_qp)\n",
        "\n",
        "@dataclass\n",
        "class LightweightReport:\n",
        "    fraction: float\n",
        "    cir_agree: float\n",
        "    proj_resid: float\n",
        "    mmd2: float\n",
        "    kl_sym: Optional[float]\n",
        "\n",
        "def lightweight_check(\n",
        "    y_full: np.ndarray,\n",
        "    y_light: np.ndarray,\n",
        "    excir_full: np.ndarray,\n",
        "    excir_light: np.ndarray,\n",
        "    compute_kl: bool = True\n",
        ") -> LightweightReport:\n",
        "    \"\"\"\n",
        "    3-checks between full and lightweight runs:\n",
        "      - cir_agree : CIR between ExCIR vectors (original CIR)\n",
        "      - proj_resid: affine alignment residual between outputs\n",
        "      - mmd2      : Gaussian-kernel MMD^2 between outputs\n",
        "      - kl_sym    : symmetric KL via KDE (1D only, optional)\n",
        "    \"\"\"\n",
        "    cir = cir_agreement(excir_full, excir_light)\n",
        "    resid = projection_alignment_residual(y_full, y_light)\n",
        "    mmd2 = mmd_gaussian(y_full, y_light)\n",
        "    klv = None\n",
        "    if compute_kl and HAVE_SCIPY and y_full.ndim == 1 and y_light.ndim == 1:\n",
        "        klv = kde_kl_symmetric(y_full, y_light)\n",
        "    return LightweightReport(fraction=np.nan, cir_agree=cir, proj_resid=resid, mmd2=mmd2, kl_sym=klv)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# AOPC utilities (optional)\n",
        "# ----------------------------------------------------------\n",
        "def aopc_curves(\n",
        "    model_predict: Callable[[np.ndarray], np.ndarray],\n",
        "    X: np.ndarray,\n",
        "    y_true: np.ndarray,\n",
        "    scores: np.ndarray,\n",
        "    task: str = \"clf\",\n",
        "    steps: int = 11\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Insertion/Deletion AOPC using a predictor and a feature ranking (scores).\n",
        "    task: 'clf' -> accuracy, 'reg' -> R^2\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import accuracy_score, r2_score\n",
        "    idx = np.argsort(-scores)\n",
        "    n, d = X.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f * d))\n",
        "        keep = idx[:k]\n",
        "        mask = np.zeros(d, dtype=bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, X, 0.0)\n",
        "        X_del = np.where(mask, 0.0, X)\n",
        "        yhat_ins = model_predict(X_ins)\n",
        "        yhat_del = model_predict(X_del)\n",
        "        if task == 'clf':\n",
        "            ins.append(accuracy_score(y_true, yhat_ins))\n",
        "            dele.append(accuracy_score(y_true, yhat_del))\n",
        "        else:\n",
        "            ins.append(r2_score(y_true, yhat_ins))\n",
        "            dele.append(r2_score(y_true, yhat_del))\n",
        "    return fracs, np.array(ins), np.array(dele)\n",
        "\n",
        "# ==========================================================\n",
        "# Usage examples (commented)\n",
        "# ==========================================================\n",
        "\"\"\"\n",
        "# --- Per-feature ExCIR (tabular/text/signal) ---\n",
        "scores = excir_feature_scores(X_va, p_va)          # original CIR per feature\n",
        "scores_light = excir_feature_scores(X_va, p_va_lt) # lightweight run\n",
        "agree = cir_agreement(scores, scores_light)        # ∈ [0,1], original CIR\n",
        "\n",
        "# --- Block ExCIR (scalar target) ---\n",
        "blocks = [[0,1,2],[3,4,5,6],[7]]\n",
        "b_scores = block_excir_scores(X_va, p_va, blocks, vector_output=False)\n",
        "\n",
        "# --- Block ExCIR (vector target, e.g. class-prob matrix P_va ∈ R^{n×q}) ---\n",
        "b_scores_vec = block_excir_scores(X_va, P_va, blocks, vector_output=True)\n",
        "\n",
        "# --- Image (class-conditioned) ---\n",
        "pix_map = image_pixel_excir_scores(X_imgs_va, P_va[:, c])     # (H,W[,C])\n",
        "patch_scores = image_patch_excir_scores(X_imgs_va, P_va[:, c], patch=(4,4))\n",
        "\n",
        "# --- Lightweight 3-checks ---\n",
        "rep = lightweight_check(y_full=p_va, y_light=p_va_lt, excir_full=scores, excir_light=scores_light, compute_kl=True)\n",
        "print(rep.cir_agree, rep.proj_resid, rep.mmd2, rep.kl_sym)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "XblLnonOxzjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# excir_lib.py\n",
        "# ==========================================================\n",
        "# ExCIR library (original midpoint/ANOVA CIR only for feature/block/class)\n",
        "# + CIR_ρ agreement for comparing two score vectors across runs\n",
        "# ==========================================================\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Iterable, List, Tuple, Optional, Dict, Callable\n",
        "\n",
        "# Optional (for KDE-KL)\n",
        "try:\n",
        "    from scipy.stats import gaussian_kde\n",
        "    HAVE_SCIPY = True\n",
        "except Exception:\n",
        "    HAVE_SCIPY = False\n",
        "\n",
        "# ---------------- Core: original CIR (midpoint/ANOVA) ----------------\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray) -> float:\n",
        "    z = np.asarray(z, dtype=float).ravel()\n",
        "    s = np.asarray(s, dtype=float).ravel()\n",
        "    assert z.shape == s.shape and z.ndim == 1\n",
        "    n = z.size\n",
        "    mz, ms = z.mean(), s.mean()\n",
        "    m = 0.5 * (mz + ms)\n",
        "    ss_between = (n / 2.0) * (mz - ms) ** 2\n",
        "    ss_total   = ((z - m) ** 2).sum() + ((s - m) ** 2).sum()\n",
        "    if ss_total <= 0:\n",
        "        return 0.0\n",
        "    return float(ss_between / ss_total)\n",
        "\n",
        "# ---------------- Agreement metric for *two score vectors* ----------------\n",
        "def cir_agree_rho(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    CIR-style *agreement* between two score vectors via correlation:\n",
        "        rho = Corr(a, b)\n",
        "        CIR_ρ = rho^2 / (1 + rho^2) ∈ [0,1)\n",
        "    This measures alignment (rank/shape), unlike mean-separation CIR which\n",
        "    collapses to ~0 if the two vectors have the same mean.\n",
        "    \"\"\"\n",
        "    a = np.asarray(a, dtype=float).ravel()\n",
        "    b = np.asarray(b, dtype=float).ravel()\n",
        "    if a.std() <= 1e-12 or b.std() <= 1e-12:\n",
        "        return 0.0\n",
        "    rho = np.corrcoef(a, b)[0, 1]\n",
        "    if not np.isfinite(rho):\n",
        "        return 0.0\n",
        "    r2 = rho * rho\n",
        "    return float(r2 / (1.0 + r2))\n",
        "\n",
        "# ---------------- Per-feature ExCIR ----------------\n",
        "def excir_feature_scores(X: np.ndarray, p: np.ndarray) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(p, dtype=float).ravel()\n",
        "    assert X.shape[0] == p.size\n",
        "    d = X.shape[1]\n",
        "    out = np.zeros(d, dtype=float)\n",
        "    for j in range(d):\n",
        "        out[j] = cir_pair(X[:, j], p)\n",
        "    return out\n",
        "\n",
        "# ---------------- Block ExCIR via ridge CCA (scalar & vector) ----------------\n",
        "def _safe_cov(X: np.ndarray) -> np.ndarray:\n",
        "    Xc = X - X.mean(axis=0, keepdims=True)\n",
        "    n = max(X.shape[0], 1)\n",
        "    return (Xc.T @ Xc) / n\n",
        "\n",
        "def _cca_scalar_y_block_w(Xb: np.ndarray, y: np.ndarray, ridge: float = 1e-6) -> np.ndarray:\n",
        "    Xb = np.asarray(Xb, dtype=float); y = np.asarray(y, dtype=float).ravel()\n",
        "    n, pb = Xb.shape\n",
        "    Xc = Xb - Xb.mean(axis=0, keepdims=True)\n",
        "    yc = y - y.mean()\n",
        "    Sxx = _safe_cov(Xb) + ridge * np.eye(pb)\n",
        "    Sxy = (Xc.T @ yc) / max(n, 1)\n",
        "    w = np.linalg.solve(Sxx, Sxy)                 # pb,\n",
        "    zc = Xc @ w\n",
        "    vz = zc.var()\n",
        "    if vz <= 1e-12: return np.zeros(pb)\n",
        "    w /= np.sqrt(vz + 1e-12)\n",
        "    if (zc * yc).mean() < 0: w = -w\n",
        "    return w\n",
        "\n",
        "def _cca_vector(Xb: np.ndarray, Y: np.ndarray, ridge_x: float = 1e-6, ridge_y: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    Xb = np.asarray(Xb, dtype=float)\n",
        "    Y  = np.asarray(Y,  dtype=float)\n",
        "    if Y.ndim == 1: Y = Y.reshape(-1, 1)\n",
        "    n, pb = Xb.shape; q = Y.shape[1]\n",
        "    Xc = Xb - Xb.mean(axis=0, keepdims=True)\n",
        "    Yc = Y  - Y.mean(axis=0, keepdims=True)\n",
        "    Sxx = _safe_cov(Xb) + ridge_x * np.eye(pb)\n",
        "    Syy = _safe_cov(Y)  + ridge_y * np.eye(q)\n",
        "    Gamma = (Xc.T @ Yc) / max(n, 1)               # pb x q\n",
        "    A = np.linalg.solve(Sxx, Gamma) @ np.linalg.solve(Syy, Gamma.T)  # pb x pb\n",
        "    lam, V = np.linalg.eig(A)\n",
        "    k = int(np.argmax(np.real(lam)))\n",
        "    w = np.real(V[:, k])\n",
        "    zc = Xc @ w; vz = zc.var()\n",
        "    if vz <= 1e-12: return np.zeros_like(w), np.zeros(q)\n",
        "    w /= np.sqrt(vz + 1e-12); zc = zc / np.sqrt(vz + 1e-12)\n",
        "    u = np.linalg.solve(Syy, Gamma.T @ w)\n",
        "    sc = Yc @ u; vs = sc.var()\n",
        "    if vs > 1e-12:\n",
        "        u /= np.sqrt(vs + 1e-12); sc = sc / np.sqrt(vs + 1e-12)\n",
        "    if (zc * sc).mean() < 0: w, u = -w, -u\n",
        "    return w, u\n",
        "\n",
        "def block_excir_scores(\n",
        "    X: np.ndarray,\n",
        "    target: np.ndarray,\n",
        "    blocks: List[Iterable[int]],\n",
        "    vector_output: bool = False,\n",
        "    ridge_x: float = 1e-6,\n",
        "    ridge_y: float = 1e-6,\n",
        ") -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    tgt = np.asarray(target, dtype=float)\n",
        "    out = np.zeros(len(blocks), dtype=float)\n",
        "    for bi, idxs in enumerate(blocks):\n",
        "        cols = np.array(list(idxs), dtype=int)\n",
        "        Xb = X[:, cols]\n",
        "        if not vector_output:\n",
        "            if tgt.ndim != 1: raise ValueError(\"Scalar target expected.\")\n",
        "            w = _cca_scalar_y_block_w(Xb, tgt, ridge=ridge_x)\n",
        "            z = Xb @ w\n",
        "            out[bi] = cir_pair(z, tgt)\n",
        "        else:\n",
        "            Y = tgt if tgt.ndim == 2 else tgt.reshape(-1, 1)\n",
        "            w, u = _cca_vector(Xb, Y, ridge_x=ridge_x, ridge_y=ridge_y)\n",
        "            z = Xb @ w; s = Y @ u\n",
        "            out[bi] = cir_pair(z, s)\n",
        "    return out\n",
        "\n",
        "# ---------------- Image: class-conditioned ExCIR ----------------\n",
        "def image_pixel_excir_scores(X_images: np.ndarray, p_class: np.ndarray) -> np.ndarray:\n",
        "    X = np.asarray(X_images, dtype=float)\n",
        "    p = np.asarray(p_class, dtype=float).ravel()\n",
        "    assert X.shape[0] == p.size\n",
        "    n = X.shape[0]\n",
        "    flat = X.reshape(n, -1)\n",
        "    scores = excir_feature_scores(flat, p)        # original CIR\n",
        "    return scores.reshape(X.shape[1:])\n",
        "\n",
        "def make_patch_blocks(H: int, W: int, patch: Tuple[int,int]) -> List[np.ndarray]:\n",
        "    ph, pw = patch\n",
        "    blocks = []\n",
        "    for r in range(0, H, ph):\n",
        "        for c in range(0, W, pw):\n",
        "            rr = np.arange(r, min(r+ph, H))\n",
        "            cc = np.arange(c, min(c+pw, W))\n",
        "            R, C = np.meshgrid(rr, cc, indexing='ij')\n",
        "            idxs = (R * W + C).ravel()\n",
        "            blocks.append(idxs)\n",
        "    return blocks\n",
        "\n",
        "def image_patch_excir_scores(X_images: np.ndarray, p_class: np.ndarray, patch: Tuple[int,int]=(4,4)) -> np.ndarray:\n",
        "    X = np.asarray(X_images, dtype=float)\n",
        "    p = np.asarray(p_class, dtype=float).ravel()\n",
        "    assert X.shape[0] == p.size\n",
        "    n = X.shape[0]\n",
        "    if X.ndim == 3:\n",
        "        H, W = X.shape[1:3]\n",
        "        Xflat = X.reshape(n, H*W)\n",
        "        blocks = make_patch_blocks(H, W, patch)\n",
        "    elif X.ndim == 4:\n",
        "        H, W, C = X.shape[1], X.shape[2], X.shape[3]\n",
        "        Xflat = X.reshape(n, H*W*C)\n",
        "        ph, pw = patch\n",
        "        blocks = []\n",
        "        for r in range(0, H, ph):\n",
        "            for c in range(0, W, pw):\n",
        "                rr = np.arange(r, min(r+ph, H))\n",
        "                cc = np.arange(c, min(c+pw, W))\n",
        "                R, Cc = np.meshgrid(rr, cc, indexing='ij')\n",
        "                base = (R * W + Cc).ravel()\n",
        "                idxs = []\n",
        "                for ch in range(C):\n",
        "                    idxs.append(base + ch * (H*W))\n",
        "                blocks.append(np.concatenate(idxs))\n",
        "    else:\n",
        "        raise ValueError(\"X_images must be (n,H,W) or (n,H,W,C).\")\n",
        "    return block_excir_scores(Xflat, p, blocks, vector_output=False)\n",
        "\n",
        "# ---------------- Lightweight checks ----------------\n",
        "def projection_alignment_residual(y_full: np.ndarray, y_light: np.ndarray) -> float:\n",
        "    Y  = np.asarray(y_full, dtype=float)\n",
        "    Yp = np.asarray(y_light, dtype=float)\n",
        "    if Y.ndim == 1:  Y  = Y.reshape(-1, 1)\n",
        "    if Yp.ndim == 1: Yp = Yp.reshape(-1, 1)\n",
        "    n = Y.shape[0]\n",
        "    Phi = np.hstack([Yp, np.ones((n, 1))])\n",
        "    Theta, *_ = np.linalg.lstsq(Phi, Y, rcond=None)\n",
        "    Yhat = Phi @ Theta\n",
        "    num = np.linalg.norm(Y - Yhat, ord='fro')\n",
        "    den = np.linalg.norm(Y, ord='fro') + 1e-12\n",
        "    return float(num / den)\n",
        "\n",
        "def mmd_gaussian(y_full: np.ndarray, y_light: np.ndarray, sigma2: Optional[float] = None) -> float:\n",
        "    Y = np.asarray(y_full, dtype=float); Z = np.asarray(y_light, dtype=float)\n",
        "    if Y.ndim == 1: Y = Y.reshape(-1, 1)\n",
        "    if Z.ndim == 1: Z = Z.reshape(-1, 1)\n",
        "    n, q = Y.shape; m, q2 = Z.shape; assert q == q2\n",
        "\n",
        "    def rbf(X, Y, s2):\n",
        "        XX = np.sum(X**2, axis=1, keepdims=True)\n",
        "        YY = np.sum(Y**2, axis=1, keepdims=True)\n",
        "        D = XX + YY.T - 2 * (X @ Y.T)\n",
        "        return np.exp(-D / (2.0 * s2))\n",
        "\n",
        "    if sigma2 is None:\n",
        "        W = np.vstack([Y, Z])\n",
        "        D = np.sum((W[:, None, :] - W[None, :, :])**2, axis=2)\n",
        "        sigma2 = float(np.median(D[D > 0]) + 1e-12)\n",
        "\n",
        "    Kyy = rbf(Y, Y, sigma2); Kzz = rbf(Z, Z, sigma2); Kyz = rbf(Y, Z, sigma2)\n",
        "    np.fill_diagonal(Kyy, 0.0); np.fill_diagonal(Kzz, 0.0)\n",
        "    term = Kyy.sum()/(n*(n-1) + 1e-12) + Kzz.sum()/(m*(m-1) + 1e-12) - 2.0 * Kyz.mean()\n",
        "    return float(max(term, 0.0))\n",
        "\n",
        "def kde_kl_symmetric(y_full: np.ndarray, y_light: np.ndarray, grid_points: int = 400) -> float:\n",
        "    if not HAVE_SCIPY:\n",
        "        raise RuntimeError(\"SciPy required for KDE-KL.\")\n",
        "    y  = np.asarray(y_full, dtype=float).ravel()\n",
        "    yp = np.asarray(y_light, dtype=float).ravel()\n",
        "    ys  = (y  - y.mean())  / (y.std()  + 1e-12)\n",
        "    yps = (yp - yp.mean()) / (yp.std() + 1e-12)\n",
        "    kde_p = gaussian_kde(ys); kde_q = gaussian_kde(yps)\n",
        "    both = np.concatenate([ys, yps]); lo = np.percentile(both, 0.5); hi = np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None); q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo) / max(grid_points - 1, 1)\n",
        "    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))) * dx)\n",
        "    return 0.5 * (kl_pq + kl_qp)\n"
      ],
      "metadata": {
        "id": "sK9LS9xFx2h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "ExCIR Benchmarks (single-file version)\n",
        "--------------------------------------\n",
        "• Implements original ExCIR (midpoint/ANOVA) for features, blocks, and class-conditioned pixels.\n",
        "• Adds CIR_ρ = ρ^2/(1+ρ^2) as an agreement index between two score vectors (full vs. lightweight).\n",
        "• Produces:\n",
        "    out/aopc_<dataset>.png\n",
        "    out/agreement_bundle_<dataset>.png\n",
        "    out/agreement_map_<dataset>.png\n",
        "    out/log.txt\n",
        "Usage:\n",
        "    python excirc_bench_single.py --datasets adult,20ng_bin,digits8x8,har6 --seed 0\n",
        "\"\"\"\n",
        "\n",
        "import os, time, argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Iterable, Optional\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from sklearn.datasets import fetch_openml, load_digits, fetch_california_housing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# --- SciPy (optional) for KDE–KL ---\n",
        "try:\n",
        "    from scipy.stats import gaussian_kde\n",
        "    HAVE_SCIPY = True\n",
        "except Exception:\n",
        "    HAVE_SCIPY = False\n",
        "\n",
        "# ===========================================================\n",
        "# Utilities\n",
        "# ===========================================================\n",
        "def ensure_out(): os.makedirs(\"out\", exist_ok=True)\n",
        "\n",
        "def log(msg: str):\n",
        "    print(msg, flush=True)\n",
        "    with open(\"out/log.txt\", \"a\") as f: f.write(msg + \"\\n\")\n",
        "\n",
        "def set_seed(seed:int=0):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def standardize_fit(Xtr, Xva, Xte):\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    return ss.fit_transform(Xtr), ss.transform(Xva), ss.transform(Xte)\n",
        "\n",
        "def train_gboost(task, Xtr, ytr, Xva, yva, seed=0):\n",
        "    Xall = np.vstack([Xtr, Xva]); yall = np.concatenate([ytr, yva])\n",
        "    model = GradientBoostingClassifier(random_state=seed) if task=='clf' else GradientBoostingRegressor(random_state=seed)\n",
        "    model.fit(Xall, yall)\n",
        "    return model\n",
        "\n",
        "def get_pred_score_vector(task, model, X):\n",
        "    if task == 'clf':\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            return model.predict_proba(X)[:, -1]\n",
        "        s = model.decision_function(X)\n",
        "        return 1/(1+np.exp(-s))\n",
        "    return model.predict(X)\n",
        "\n",
        "def rank_from_scores(scores): return np.argsort(-scores)\n",
        "\n",
        "# ===========================================================\n",
        "# Original CIR (midpoint/ANOVA) and agreement CIR_ρ\n",
        "# ===========================================================\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Original ExCIR on two 1D signals (midpoint/ANOVA ratio in [0,1]).\n",
        "    \"\"\"\n",
        "    z = np.asarray(z, dtype=float).ravel()\n",
        "    s = np.asarray(s, dtype=float).ravel()\n",
        "    assert z.shape == s.shape and z.ndim == 1\n",
        "    n = z.size\n",
        "    mz, ms = z.mean(), s.mean()\n",
        "    m = 0.5 * (mz + ms)\n",
        "    ss_between = (n / 2.0) * (mz - ms) ** 2\n",
        "    ss_total   = ((z - m) ** 2).sum() + ((s - m) ** 2).sum()\n",
        "    if ss_total <= 0:\n",
        "        return 0.0\n",
        "    return float(ss_between / ss_total)\n",
        "\n",
        "def cir_agree_rho(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Agreement index between two score vectors via correlation:\n",
        "        rho = Corr(a, b)\n",
        "        CIR_ρ = rho^2 / (1 + rho^2) in [0,1)\n",
        "    Unlike mean-sep CIR on (a,b), this measures *alignment*.\n",
        "    \"\"\"\n",
        "    a = np.asarray(a, dtype=float).ravel()\n",
        "    b = np.asarray(b, dtype=float).ravel()\n",
        "    if a.std() <= 1e-12 or b.std() <= 1e-12:\n",
        "        return 0.0\n",
        "    rho = np.corrcoef(a, b)[0, 1]\n",
        "    if not np.isfinite(rho):\n",
        "        return 0.0\n",
        "    r2 = rho * rho\n",
        "    return float(r2 / (1.0 + r2))\n",
        "\n",
        "def excir_feature_scores(X: np.ndarray, p: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Per-feature ExCIR (original). For each column j: CIR( X[:,j], p ).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(p, dtype=float).ravel()\n",
        "    assert X.shape[0] == p.size\n",
        "    d = X.shape[1]\n",
        "    out = np.zeros(d, dtype=float)\n",
        "    for j in range(d):\n",
        "        out[j] = cir_pair(X[:, j], p)\n",
        "    return out\n",
        "\n",
        "# ===========================================================\n",
        "# Agreement beyond ranks: projection residual & KDE–KL\n",
        "# ===========================================================\n",
        "def projection_alignment_residual(y_full: np.ndarray, y_light: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Best affine fit Y_full ≈ Y_light @ A + 1 b^T; return normalized residual.\n",
        "    Works for 1D or q-D outputs.\n",
        "    \"\"\"\n",
        "    Y  = np.asarray(y_full, dtype=float)\n",
        "    Yp = np.asarray(y_light, dtype=float)\n",
        "    if Y.ndim == 1:  Y  = Y.reshape(-1, 1)\n",
        "    if Yp.ndim == 1: Yp = Yp.reshape(-1, 1)\n",
        "    n = Y.shape[0]\n",
        "    Phi = np.hstack([Yp, np.ones((n, 1))])\n",
        "    Theta, *_ = np.linalg.lstsq(Phi, Y, rcond=None)\n",
        "    Yhat = Phi @ Theta\n",
        "    num = np.linalg.norm(Y - Yhat, ord='fro')\n",
        "    den = np.linalg.norm(Y, ord='fro') + 1e-12\n",
        "    return float(num / den)\n",
        "\n",
        "def kde_kl_symmetric(y_full: np.ndarray, y_light: np.ndarray, grid_points: int = 400) -> float:\n",
        "    \"\"\"\n",
        "    Symmetric KL between 1D KDEs of standardized outputs.\n",
        "    Requires SciPy; returns np.nan if SciPy missing.\n",
        "    \"\"\"\n",
        "    if not HAVE_SCIPY:\n",
        "        return np.nan\n",
        "    y  = np.asarray(y_full, dtype=float).ravel()\n",
        "    yp = np.asarray(y_light, dtype=float).ravel()\n",
        "    ys  = (y  - y.mean())  / (y.std()  + 1e-12)\n",
        "    yps = (yp - yp.mean()) / (yp.std() + 1e-12)\n",
        "    kde_p = gaussian_kde(ys); kde_q = gaussian_kde(yps)\n",
        "    both = np.concatenate([ys, yps])\n",
        "    lo = np.percentile(both, 0.5); hi = np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None)\n",
        "    q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo) / max(grid_points - 1, 1)\n",
        "    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))) * dx)\n",
        "    return 0.5 * (kl_pq + kl_qp)\n",
        "\n",
        "# ===========================================================\n",
        "# Evaluation helpers\n",
        "# ===========================================================\n",
        "def topk_sufficiency_eval(method_name, scores, task, Xtr, ytr, Xva, yva, Xte, yte, ks, seed=0):\n",
        "    ranks = rank_from_scores(scores); rows=[]\n",
        "    for k in ks:\n",
        "        keep = ranks[:k]\n",
        "        model_k = GradientBoostingClassifier(random_state=seed) if task=='clf' else GradientBoostingRegressor(random_state=seed)\n",
        "        Xtr_k = np.vstack([Xtr, Xva])[:, keep]; ytr_k = np.concatenate([ytr, yva])\n",
        "        model_k.fit(Xtr_k, ytr_k)\n",
        "        yhat = model_k.predict(Xte[:, keep])\n",
        "        metric = accuracy_score(yte, yhat) if task=='clf' else r2_score(yte, yhat)\n",
        "        rows.append((k, metric))\n",
        "    return {\"method\": method_name, \"metrics\": rows}\n",
        "\n",
        "def aopc_curves(task, model, Xte, yte, scores, steps=11):\n",
        "    idx = rank_from_scores(scores)\n",
        "    n, d = Xte.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f * d))\n",
        "        keep = idx[:k]\n",
        "        mask = np.zeros(d, dtype=bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0)\n",
        "        X_del = np.where(mask, 0.0, Xte)\n",
        "        if task == 'clf':\n",
        "            ins.append(accuracy_score(yte, model.predict(X_ins)))\n",
        "            dele.append(accuracy_score(yte, model.predict(X_del)))\n",
        "        else:\n",
        "            ins.append(r2_score(yte, model.predict(X_ins)))\n",
        "            dele.append(r2_score(yte, model.predict(X_del)))\n",
        "    return fracs, np.array(ins), np.array(dele)\n",
        "\n",
        "# ===========================================================\n",
        "# Datasets\n",
        "# ===========================================================\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str; task: str\n",
        "    Xtr: np.ndarray; ytr: np.ndarray\n",
        "    Xva: np.ndarray; yva: np.ndarray\n",
        "    Xte: np.ndarray; yte: np.ndarray\n",
        "\n",
        "def load_adult(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "    dfX = ds.data.select_dtypes(include=[np.number]).copy()  # numeric only\n",
        "    y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "    X = dfX.to_numpy(dtype=float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"adult\",\"clf\", Xtr,ytr,Xva,yva,Xte,yte)\n",
        "\n",
        "def load_california(seed=0) -> DatasetPack:\n",
        "    ds = fetch_california_housing()  # numeric\n",
        "    X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"california\",\"reg\", Xtr,ytr,Xva,yva,Xte,yte)\n",
        "\n",
        "def load_20ng_binary(seed=0) -> DatasetPack:\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    cats = ['comp.graphics','sci.space']\n",
        "    tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "    te = fetch_20newsgroups(subset='test',  categories=cats, remove=('headers','footers','quotes'))\n",
        "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
        "    Xtr_full = tfidf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "    Xte = tfidf.transform(te.data).astype(np.float32).toarray()\n",
        "    ytr_full = tr.target; yte = te.target\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2, stratify=ytr_full, random_state=seed)\n",
        "    return DatasetPack(\"20ng_bin\",\"clf\", Xtr,ytr,Xva,yva,Xte,yte)\n",
        "\n",
        "def load_har(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"HAR\", version=1, as_frame=True)\n",
        "    X = ds.data.to_numpy(dtype=float)\n",
        "    y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"har6\",\"clf\", Xtr,ytr,Xva,yva,Xte,yte)\n",
        "\n",
        "def load_digits8x8(seed=0) -> DatasetPack:\n",
        "    digits = load_digits()\n",
        "    X = (digits.data.astype(float) / 16.0)\n",
        "    y = digits.target\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"digits8x8\",\"clf\", Xtr,ytr,Xva,yva,Xte,yte)\n",
        "\n",
        "ALL = {\n",
        "    \"adult\": load_adult,\n",
        "    \"california\": load_california,\n",
        "    \"20ng_bin\": load_20ng_binary,\n",
        "    \"har6\": load_har,\n",
        "    \"digits8x8\": load_digits8x8,\n",
        "}\n",
        "\n",
        "# ===========================================================\n",
        "# Agreement plots (full vs. lightweight)\n",
        "# ===========================================================\n",
        "def agreement_graphs_for_dataset(pack, seed=0, fractions=(0.2,0.3,0.4,0.5,0.75,1.0)):\n",
        "    ensure_out()\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    # Full model & ExCIR on validation\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    p_full = get_pred_score_vector(pack.task, base, pack.Xva)\n",
        "    s_full = excir_feature_scores(pack.Xva, p_full)\n",
        "\n",
        "    Xbig = np.vstack([pack.Xtr, pack.Xva]); ybig = np.concatenate([pack.ytr, pack.yva])\n",
        "\n",
        "    F, T, CIR_MEANSEP, CIR_RHO, J8, RESID, KLS = [], [], [], [], [], [], []\n",
        "    for f in fractions:\n",
        "        t0 = time.time()\n",
        "        idx = rng.choice(np.arange(Xbig.shape[0]), size=max(2, int(f*Xbig.shape[0])), replace=False)\n",
        "        Xsub, ysub = Xbig[idx], ybig[idx]\n",
        "        model = GradientBoostingClassifier(random_state=seed) if pack.task=='clf' else GradientBoostingRegressor(random_state=seed)\n",
        "        model.fit(Xsub, ysub)\n",
        "        p_sub = get_pred_score_vector(pack.task, model, pack.Xva)\n",
        "        s_sub = excir_feature_scores(pack.Xva, p_sub)\n",
        "        T.append(time.time() - t0)\n",
        "\n",
        "        # Agreement measures\n",
        "        CIR_MEANSEP.append(cir_pair(s_full, s_sub))          # often ~0 when means match\n",
        "        CIR_RHO.append(cir_agree_rho(s_full, s_sub))         # alignment in [0,1)\n",
        "\n",
        "        # top-8 overlap\n",
        "        def jaccard_topk(a, b, k=8):\n",
        "            A = set(rank_from_scores(a)[:k]); B = set(rank_from_scores(b)[:k])\n",
        "            return len(A & B) / (len(A | B) + 1e-12)\n",
        "        J8.append(jaccard_topk(s_full, s_sub, k=min(8, s_full.size)))\n",
        "\n",
        "        RESID.append(projection_alignment_residual(p_full, p_sub))\n",
        "        KLS.append(kde_kl_symmetric(p_full, p_sub) if p_full.ndim==1 else np.nan)\n",
        "\n",
        "        F.append(f)\n",
        "\n",
        "    F = np.array(F); T = np.array(T)\n",
        "    CIR_MEANSEP = np.array(CIR_MEANSEP); CIR_RHO = np.array(CIR_RHO)\n",
        "    J8 = np.array(J8); RESID = np.array(RESID); KLS = np.array(KLS)\n",
        "\n",
        "    # 3-panel bundle: CIR_ρ, residual, KL\n",
        "    plt.figure(figsize=(9.4,3.6))\n",
        "    ax1 = plt.subplot(1,3,1)\n",
        "    ax1.plot([int(100*f) for f in F], CIR_RHO, marker='o')\n",
        "    ax1.set_ylim(0, 1.0); ax1.set_xlabel('% rows kept'); ax1.set_ylabel('CIR_ρ (alignment)')\n",
        "    ax1.set_title('Agreement (corr-based)')\n",
        "\n",
        "    ax2 = plt.subplot(1,3,2)\n",
        "    ax2.plot([int(100*f) for f in F], RESID, marker='o')\n",
        "    ax2.set_xlabel('% rows kept'); ax2.set_ylabel('Projection residual')\n",
        "    ax2.set_title('Shape agreement (affine residual)')\n",
        "\n",
        "    ax3 = plt.subplot(1,3,3)\n",
        "    ax3.plot([int(100*f) for f in F], KLS, marker='o')\n",
        "    ax3.set_xlabel('% rows kept'); ax3.set_ylabel('Sym. KL (KDE)')\n",
        "    ax3.set_title('Distribution match')\n",
        "    plt.suptitle(f'Agreement beyond ranks — {pack.name}', y=1.02, fontsize=11)\n",
        "    plt.tight_layout(); plt.savefig(f\"out/agreement_bundle_{pack.name}.png\", dpi=160); plt.close()\n",
        "\n",
        "    # Pareto-like map: x=CIR_ρ, y=residual, color=KL, size=J@8\n",
        "    plt.figure(figsize=(5.8,4.8))\n",
        "    sc = plt.scatter(CIR_RHO, RESID, c=KLS, s=200*np.maximum(J8,0.05), alpha=0.85)\n",
        "    for i,f in enumerate(F):\n",
        "        plt.text(CIR_RHO[i], RESID[i], f\"{int(100*f)}%\", ha='center', va='bottom', fontsize=9)\n",
        "    plt.xlabel('CIR_ρ (alignment)'); plt.ylabel('Projection residual')\n",
        "    cbar = plt.colorbar(sc); cbar.set_label('Symmetric KL (KDE)')\n",
        "    plt.title(f'Agreement map — {pack.name}')\n",
        "    plt.grid(alpha=0.3); plt.tight_layout(); plt.savefig(f\"out/agreement_map_{pack.name}.png\", dpi=160); plt.close()\n",
        "\n",
        "    log(f\"[{pack.name}] CIR_mean-sep @ {F}: {np.round(CIR_MEANSEP,4)}\")\n",
        "    log(f\"[{pack.name}] CIR_ρ         @ {F}: {np.round(CIR_RHO,4)}\")\n",
        "    log(f\"[{pack.name}] resid         @ {F}: {np.round(RESID,4)}\")\n",
        "    log(f\"[{pack.name}] KL(KDE)       @ {F}: {np.round(KLS,4)}\")\n",
        "    log(f\"[{pack.name}] Jaccard@8     @ {F}: {np.round(J8,3)}\")\n",
        "\n",
        "# ===========================================================\n",
        "# Main per-dataset run\n",
        "# ===========================================================\n",
        "def run_on_dataset(pack: DatasetPack, ks=(3,5,8,12), seed=0):\n",
        "    ensure_out()\n",
        "    log(f\"\\n=== Dataset: {pack.name} (task={pack.task}) ===\")\n",
        "    base_model = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    p_va = get_pred_score_vector(pack.task, base_model, pack.Xva)\n",
        "\n",
        "    # ExCIR (original)\n",
        "    t0 = time.time(); excir = excir_feature_scores(pack.Xva, p_va); t1 = time.time()\n",
        "    log(f\"ExCIR(traditional) computed in {t1-t0:.3f}s, d={pack.Xva.shape[1]}\")\n",
        "\n",
        "    # Baselines (fast set)\n",
        "    t0 = time.time(); pfi = permutation_importance(base_model, pack.Xva, pack.yva, n_repeats=10, random_state=seed).importances_mean; log(f\"PFI {time.time()-t0:.3f}s\")\n",
        "    # PDP-var (optional – can be slower on large d)\n",
        "    def pdp_var_scores(model, X, task='clf', grid_points=20, random_state=0):\n",
        "        from sklearn.inspection import partial_dependence\n",
        "        d = X.shape[1]; scores = np.zeros(d)\n",
        "        idx = np.random.RandomState(random_state).choice(np.arange(X.shape[0]), size=min(2000, X.shape[0]), replace=False)\n",
        "        Xs = X[idx]\n",
        "        for j in range(d):\n",
        "            try:\n",
        "                pd = partial_dependence(model, Xs, features=[j], kind='average', grid_resolution=grid_points)\n",
        "                curve = pd.average[0]; scores[j] = np.var(curve)\n",
        "            except Exception:\n",
        "                scores[j] = 0.0\n",
        "        return scores\n",
        "    t0 = time.time(); pdp = pdp_var_scores(base_model, pack.Xva, task=pack.task); log(f\"PDP-var {time.time()-t0:.3f}s\")\n",
        "\n",
        "    # AOPC (ExCIR)\n",
        "    fracs, ins, dele = aopc_curves(pack.task, base_model, pack.Xte, pack.yte, excir, steps=9)\n",
        "    plt.figure(figsize=(6.5,4.3))\n",
        "    plt.plot(100*fracs, ins, marker='o', label='Insertion (keep top-%)')\n",
        "    plt.plot(100*fracs, dele, marker='o', label='Deletion (zero top-%)')\n",
        "    plt.xlabel('Revealed / removed top-% features'); plt.ylabel('Test ' + ('Accuracy' if pack.task=='clf' else 'R$^2$'))\n",
        "    plt.title(f'AOPC (ExCIR) — {pack.name}')\n",
        "    plt.legend(); plt.tight_layout(); plt.savefig(f\"out/aopc_{pack.name}.png\", dpi=160); plt.close()\n",
        "\n",
        "    # Agreement bundle (full vs lightweight)\n",
        "    agreement_graphs_for_dataset(pack, seed=seed)\n",
        "\n",
        "# ===========================================================\n",
        "# Entry point\n",
        "# ===========================================================\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"ExCIR single-file benchmarks (original ExCIR + CIR_ρ agreement)\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--datasets\", type=str, default=\"adult,20ng_bin,digits8x8,har6,california\")\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    ensure_out()\n",
        "    with open(\"out/log.txt\",\"w\") as f: f.write(\"ExCIR benchmark log\\n\")\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    names = [s.strip() for s in args.datasets.split(\",\") if s.strip()]\n",
        "    for nm in names:\n",
        "        loader = ALL.get(nm)\n",
        "        if loader is None:\n",
        "            log(f\"Unknown dataset key: {nm} (skip)\"); continue\n",
        "        try:\n",
        "            pack = loader(seed=args.seed)\n",
        "            run_on_dataset(pack, seed=args.seed)\n",
        "        except Exception as e:\n",
        "            log(f\"[{nm}] ERROR: {e}\")\n",
        "\n",
        "    log(\"\\nDone. See the 'out/' folder for figures and logs.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "2819Mctkx7aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# excir_lib.py\n",
        "# Core ExCIR utilities (original pooled-mean formula), baselines, datasets, and plots.\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, time, warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Iterable, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance, partial_dependence\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from sklearn.datasets import fetch_openml, load_digits\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# -----------------------\n",
        "# Core ExCIR (original)\n",
        "# -----------------------\n",
        "\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    \"\"\"\n",
        "    Original ExCIR between two 1-D signals z, s of equal length n':\n",
        "      CIR = n' * [ (mean(z)-m)^2 + (mean(s)-m)^2 ] /\n",
        "            [ sum_i (z_i - m)^2 + sum_i (s_i - m)^2 ],\n",
        "    where m = 0.5 * (mean(z) + mean(s)).\n",
        "    \"\"\"\n",
        "    z = np.asarray(z, dtype=float).ravel()\n",
        "    s = np.asarray(s, dtype=float).ravel()\n",
        "    assert z.shape[0] == s.shape[0], \"z and s must have the same length\"\n",
        "    mu_z, mu_s = z.mean(), s.mean()\n",
        "    m = 0.5 * (mu_z + mu_s)\n",
        "    num = z.shape[0] * ((mu_z - m) ** 2 + (mu_s - m) ** 2)\n",
        "    den = np.sum((z - m) ** 2) + np.sum((s - m) ** 2) + eps\n",
        "    return float(num / den)\n",
        "\n",
        "def excir_feature_scores(X: np.ndarray, p: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Original ExCIR per feature j: treat Z := X[:, j] and S := p as two groups of length n'.\n",
        "    Vectorized form of cir_pair for all columns.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(p, dtype=float).ravel()\n",
        "    n, d = X.shape\n",
        "    assert p.shape[0] == n, \"X and p must align on rows\"\n",
        "\n",
        "    mu_p = p.mean()\n",
        "    m_j = 0.5 * (X.mean(axis=0) + mu_p)                 # grand mean per feature\n",
        "    num = n * ((X.mean(axis=0) - m_j) ** 2 + (mu_p - m_j) ** 2)\n",
        "    den = np.sum((X - m_j) ** 2, axis=0) + np.sum((p - m_j) ** 2) + eps\n",
        "    return num / den\n",
        "\n",
        "def excir_block_scores(X: np.ndarray, p: np.ndarray, blocks: List[List[int]]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Block ExCIR: for each block (list of feature indices), create Z_block as\n",
        "    the mean of z-scored features in the block, then compute CIR(Z_block, p).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float); p = np.asarray(p, dtype=float).ravel()\n",
        "    Z = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-12)\n",
        "    out = []\n",
        "    for idxs in blocks:\n",
        "        zb = Z[:, idxs].mean(axis=1)\n",
        "        out.append(cir_pair(zb, p))\n",
        "    return np.asarray(out)\n",
        "\n",
        "# -----------------------\n",
        "# Agreement beyond ranks\n",
        "# -----------------------\n",
        "\n",
        "def projection_alignment_residual(y_full: np.ndarray, y_light: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Best affine map y_full ≈ a*y_light + b; return normalized residual ||res||/||y_full||.\n",
        "    \"\"\"\n",
        "    y = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "    n = y.shape[0]\n",
        "    Phi = np.c_[yp, np.ones(n)]\n",
        "    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n",
        "    yhat = Phi @ theta\n",
        "    return float(np.linalg.norm(y - yhat) / (np.linalg.norm(y) + 1e-12))\n",
        "\n",
        "def kde_kl_symmetric(y_full: np.ndarray, y_light: np.ndarray, grid_points: int = 400) -> float:\n",
        "    \"\"\"\n",
        "    Symmetric KL between 1-D KDEs of the two outputs (no standardization).\n",
        "    \"\"\"\n",
        "    y = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "    kde_p = gaussian_kde(y)\n",
        "    kde_q = gaussian_kde(yp)\n",
        "    both = np.r_[y, yp]\n",
        "    lo, hi = np.percentile(both, 0.5), np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None)\n",
        "    q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo) / max(grid_points - 1, 1)\n",
        "    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))) * dx)\n",
        "    return 0.5 * (kl_pq + kl_qp)\n",
        "\n",
        "def jaccard_topk(a_scores: np.ndarray, b_scores: np.ndarray, k: int = 8) -> float:\n",
        "    A = set(np.argsort(-a_scores)[:k]); B = set(np.argsort(-b_scores)[:k])\n",
        "    return len(A & B) / (len(A | B) + 1e-12)\n",
        "\n",
        "# -----------------------\n",
        "# Baselines\n",
        "# -----------------------\n",
        "\n",
        "def pfi_scores(model, X, y, n_repeats: int = 10, seed: int = 0) -> np.ndarray:\n",
        "    return permutation_importance(model, X, y, n_repeats=n_repeats,\n",
        "                                  random_state=seed, scoring=None).importances_mean\n",
        "\n",
        "def tree_gain_scores(model, d: int) -> np.ndarray:\n",
        "    try:\n",
        "        w = model.feature_importances_\n",
        "        if w is None or len(w) != d:\n",
        "            return np.zeros(d)\n",
        "        return w\n",
        "    except Exception:\n",
        "        return np.zeros(d)\n",
        "\n",
        "def pdp_var_scores(model, X, grid_points: int = 20, random_state: int = 0) -> np.ndarray:\n",
        "    d = X.shape[1]; scores = np.zeros(d); rs = check_random_state(random_state)\n",
        "    idx = rs.choice(np.arange(X.shape[0]), size=min(2000, X.shape[0]), replace=False)\n",
        "    Xs = X[idx]\n",
        "    for j in range(d):\n",
        "        try:\n",
        "            pd = partial_dependence(model, Xs, features=[j], kind='average', grid_resolution=grid_points)\n",
        "            scores[j] = np.var(pd.average[0])\n",
        "        except Exception:\n",
        "            scores[j] = 0.0\n",
        "    return scores\n",
        "\n",
        "def surrogate_lr_scores(X, teacher_scores, alpha: float = 1.0) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float); y = np.asarray(teacher_scores, dtype=float).ravel()\n",
        "    reg = Ridge(alpha=alpha, random_state=0)\n",
        "    reg.fit(X, y)\n",
        "    w = np.abs(reg.coef_)\n",
        "    if w.ndim > 1:\n",
        "        w = np.linalg.norm(w, axis=0)\n",
        "    s = w / (w.sum() + 1e-12)\n",
        "    return s\n",
        "\n",
        "def mi_pred_scores(X, preds, task: str) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(preds, dtype=float).ravel()\n",
        "    if task == \"clf\":\n",
        "        z = (p > np.median(p)).astype(int)\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "    else:\n",
        "        z = np.digitize(p, np.quantile(p, [1/3, 2/3]))\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "\n",
        "def mi_label_scores(X, y, task: str) -> np.ndarray:\n",
        "    if task == \"clf\":\n",
        "        return mutual_info_classif(X, y, random_state=0)\n",
        "    else:\n",
        "        return mutual_info_regression(X, y, random_state=0)\n",
        "\n",
        "# -----------------------\n",
        "# AOPC & stability\n",
        "# -----------------------\n",
        "\n",
        "def rank_from_scores(scores: np.ndarray) -> np.ndarray:\n",
        "    return np.argsort(-scores)\n",
        "\n",
        "def aopc_curves(task: str, model, Xte, yte, scores: np.ndarray, steps: int = 11):\n",
        "    idx = rank_from_scores(scores)\n",
        "    n, d = Xte.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f * d))\n",
        "        keep = idx[:k]\n",
        "        mask = np.zeros(d, dtype=bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0)\n",
        "        X_del = np.where(mask, 0.0, Xte)\n",
        "        y_ins = model.predict(X_ins); y_del = model.predict(X_del)\n",
        "        if task == \"clf\":\n",
        "            ins.append(accuracy_score(yte, y_ins))\n",
        "            dele.append(accuracy_score(yte, y_del))\n",
        "        else:\n",
        "            ins.append(r2_score(yte, y_ins))\n",
        "            dele.append(r2_score(yte, y_del))\n",
        "    return fracs, np.asarray(ins), np.asarray(dele)\n",
        "\n",
        "def stability_noise_eval(scores_fn, Xva, p_va, repeats: int = 20, sigma: float = 0.01):\n",
        "    base = scores_fn(Xva, p_va)\n",
        "    cirs, j8 = [], []\n",
        "    for _ in range(repeats):\n",
        "        noise = np.random.normal(0, sigma, size=Xva.shape)\n",
        "        s_noisy = scores_fn(Xva + noise, p_va)\n",
        "        cirs.append(cir_pair(base, s_noisy))\n",
        "        j8.append(jaccard_topk(base, s_noisy, k=min(8, Xva.shape[1])))\n",
        "    return np.asarray(cirs), np.asarray(j8)\n",
        "\n",
        "# -----------------------\n",
        "# Lightweight sweep (three checks)\n",
        "# -----------------------\n",
        "\n",
        "def train_gboost(task: str, Xtr, ytr, Xva, yva, seed: int = 0):\n",
        "    Xall = np.vstack([Xtr, Xva]); yall = np.r_[ytr, yva]\n",
        "    if task == \"clf\":\n",
        "        m = GradientBoostingClassifier(random_state=seed)\n",
        "    else:\n",
        "        m = GradientBoostingRegressor(random_state=seed)\n",
        "    m.fit(Xall, yall)\n",
        "    return m\n",
        "\n",
        "def predict_scores(task: str, model, X) -> np.ndarray:\n",
        "    if task == \"clf\":\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            return model.predict_proba(X)[:, -1]\n",
        "        s = model.decision_function(X)\n",
        "        return 1.0/(1.0+np.exp(-s))\n",
        "    return model.predict(X)\n",
        "\n",
        "def lightweight_three_checks(pack, seed: int = 0, fractions=(0.2,0.3,0.4,0.5,0.75,1.0)):\n",
        "    \"\"\"\n",
        "    Train full model; then train on fractions of rows and compare to full:\n",
        "      1) CIR agreement between ExCIR vectors (original CIR on feature-score vectors)\n",
        "      2) Projection alignment residual on outputs\n",
        "      3) Symmetric KL via KDE on outputs\n",
        "    Also return Jaccard@8 on feature rankings and wall time.\n",
        "    \"\"\"\n",
        "    rng = check_random_state(seed)\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    p_full = predict_scores(pack.task, base, pack.Xva)\n",
        "    s_full = excir_feature_scores(pack.Xva, p_full)\n",
        "\n",
        "    Xbig = np.vstack([pack.Xtr, pack.Xva])\n",
        "    ybig = np.r_[pack.ytr, pack.yva]\n",
        "\n",
        "    F, Times, CIRs, Resid, KLs, J8 = [], [], [], [], [], []\n",
        "    for f in fractions:\n",
        "        t0 = time.time()\n",
        "        n_rows = int(f * Xbig.shape[0])\n",
        "        idx = rng.choice(np.arange(Xbig.shape[0]), size=n_rows, replace=False)\n",
        "        m = GradientBoostingClassifier(random_state=seed) if pack.task == \"clf\" else GradientBoostingRegressor(random_state=seed)\n",
        "        m.fit(Xbig[idx], ybig[idx])\n",
        "        p_sub = predict_scores(pack.task, m, pack.Xva)\n",
        "        s_sub = excir_feature_scores(pack.Xva, p_sub)\n",
        "\n",
        "        CIRs.append(cir_pair(s_full, s_sub))\n",
        "        Resid.append(projection_alignment_residual(p_full, p_sub))\n",
        "        KLs.append(kde_kl_symmetric(p_full, p_sub))\n",
        "        J8.append(jaccard_topk(s_full, s_sub, k=min(8, s_full.shape[0])))\n",
        "        Times.append(time.time() - t0)\n",
        "        F.append(f)\n",
        "\n",
        "    return (np.asarray(F), np.asarray(Times), np.asarray(CIRs),\n",
        "            np.asarray(Resid), np.asarray(KLs), np.asarray(J8))\n",
        "\n",
        "def plot_agreement_bundle(pack, F, CIRs, Resid, KLs, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(9.4, 3.6))\n",
        "\n",
        "    ax1 = plt.subplot(1,3,1)\n",
        "    ax1.plot((100*F).astype(int), CIRs, marker='o')\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.set_xlabel('% rows kept'); ax1.set_ylabel('CIR(full, light)')\n",
        "    ax1.set_title('Agreement (CIR)')\n",
        "\n",
        "    ax2 = plt.subplot(1,3,2)\n",
        "    ax2.plot((100*F).astype(int), Resid, marker='o')\n",
        "    ax2.set_xlabel('% rows kept'); ax2.set_ylabel('Projection residual')\n",
        "    ax2.set_title('Shape agreement')\n",
        "\n",
        "    ax3 = plt.subplot(1,3,3)\n",
        "    ax3.plot((100*F).astype(int), KLs, marker='o')\n",
        "    ax3.set_xlabel('% rows kept'); ax3.set_ylabel('Sym. KL (KDE)')\n",
        "    ax3.set_title('Distribution match')\n",
        "\n",
        "    plt.suptitle(f'Agreement beyond ranks — {pack.name}', y=1.02, fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"agreement_bundle_{pack.name}.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def plot_pareto(pack, Times, CIRs, F, J8, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(6.2, 4.8))\n",
        "    plt.scatter(Times, CIRs, s=200*np.maximum(J8, 0.05), alpha=0.85)\n",
        "    for i, f in enumerate(F):\n",
        "        plt.text(Times[i], CIRs[i], f\"{int(100*f)}%\", ha='center', va='bottom', fontsize=9)\n",
        "    plt.xlabel('Wall time (s)'); plt.ylabel('CIR(full, light)')\n",
        "    plt.title(f'Agreement–cost (ExCIR) — {pack.name}')\n",
        "    plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"pareto_{pack.name}.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def plot_runtime_vs_fraction(pack, F, Times, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(6.0,4.0))\n",
        "    plt.plot((100*F).astype(int), Times, marker='o')\n",
        "    plt.xlabel('Fraction of rows kept (%)'); plt.ylabel('Wall time (s)')\n",
        "    plt.title(f'Runtime vs fraction — {pack.name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"runtime_{pack.name}.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# -----------------------\n",
        "# Datasets\n",
        "# -----------------------\n",
        "\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str\n",
        "    task: str\n",
        "    Xtr: np.ndarray\n",
        "    ytr: np.ndarray\n",
        "    Xva: np.ndarray\n",
        "    yva: np.ndarray\n",
        "    Xte: np.ndarray\n",
        "    yte: np.ndarray\n",
        "    feature_names: List[str]\n",
        "\n",
        "def _standardize_fit(Xtr, Xva, Xte):\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    return ss.fit_transform(Xtr), ss.transform(Xva), ss.transform(Xte)\n",
        "\n",
        "def load_adult(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "    dfX = ds.data.select_dtypes(include=[np.number]).copy()  # keep numeric only\n",
        "    y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "    X = dfX.to_numpy(dtype=float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"adult\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, list(dfX.columns))\n",
        "\n",
        "def load_20ng_binary(seed=0) -> DatasetPack:\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    cats = ['comp.graphics', 'sci.space']\n",
        "    tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "    te = fetch_20newsgroups(subset='test', categories=cats, remove=('headers','footers','quotes'))\n",
        "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
        "    Xtr_full = tfidf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "    Xte = tfidf.transform(te.data).astype(np.float32).toarray()\n",
        "    ytr_full, yte = tr.target, te.target\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2, stratify=ytr_full, random_state=seed)\n",
        "    return DatasetPack(\"20ng_bin\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "def load_har(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"HAR\", version=1, as_frame=True)  # 6-class smartphone HAR\n",
        "    X = ds.data.to_numpy(dtype=float)\n",
        "    y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"har6\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "def load_digits8x8(seed=0) -> DatasetPack:\n",
        "    digits = load_digits()\n",
        "    X = digits.data.astype(float) / 16.0\n",
        "    y = digits.target\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"digits8x8\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"pix{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "# -----------------------\n",
        "# End-to-end run for one dataset\n",
        "# -----------------------\n",
        "\n",
        "def ensure_out(outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "def run_on_dataset(pack: DatasetPack, ks=(3,5,8,12), seed: int = 0,\n",
        "                   do_shap: bool = False, do_pfi: bool = True, outdir=\"out\") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Train a GBM, compute ExCIR + baselines on validation, evaluate:\n",
        "      - top-k sufficiency via retraining,\n",
        "      - AOPC curves (ExCIR ranking),\n",
        "      - stability under noise,\n",
        "      - agreement/time Pareto and runtime curves.\n",
        "    Saves figures/CSV into outdir. Returns a dict of score vectors.\n",
        "    \"\"\"\n",
        "    ensure_out(outdir)\n",
        "    # 1) train base model\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    p_va = predict_scores(pack.task, base, pack.Xva)\n",
        "\n",
        "    # 2) score vectors on validation\n",
        "    scores: Dict[str, np.ndarray] = {}\n",
        "    scores[\"ExCIR\"] = excir_feature_scores(pack.Xva, p_va)\n",
        "\n",
        "    if do_pfi:\n",
        "        scores[\"PFI\"] = pfi_scores(base, pack.Xva, pack.yva, n_repeats=10, seed=seed)\n",
        "    scores[\"TreeGain\"]  = tree_gain_scores(base, pack.Xva.shape[1])\n",
        "    scores[\"PDP-var\"]   = pdp_var_scores(base, pack.Xva)\n",
        "    scores[\"MI(pred)\"]  = mi_pred_scores(pack.Xva, p_va, task=pack.task)\n",
        "    scores[\"MI(label)\"] = mi_label_scores(pack.Xva, pack.yva, task=pack.task)\n",
        "    scores[\"Surrogate-LR\"] = surrogate_lr_scores(pack.Xva, p_va)\n",
        "\n",
        "    # 3) top-k sufficiency (write/append CSV)\n",
        "    import csv\n",
        "    csv_path = os.path.join(outdir, \"summary_topk.csv\")\n",
        "    header = not os.path.exists(csv_path)\n",
        "    with open(csv_path, \"a\", newline=\"\") as f:\n",
        "        wr = csv.writer(f)\n",
        "        if header:\n",
        "            wr.writerow([\"dataset\",\"method\",\"k\",\"metric\"])\n",
        "        for m, v in scores.items():\n",
        "            ranks = rank_from_scores(v)\n",
        "            for k in ks:\n",
        "                keep = ranks[:k]\n",
        "                model_k = GradientBoostingClassifier(random_state=seed) if pack.task==\"clf\" \\\n",
        "                          else GradientBoostingRegressor(random_state=seed)\n",
        "                Xtr_k = np.vstack([pack.Xtr, pack.Xva])[:, keep]\n",
        "                ytr_k = np.r_[pack.ytr, pack.yva]\n",
        "                model_k.fit(Xtr_k, ytr_k)\n",
        "                yhat = model_k.predict(pack.Xte[:, keep])\n",
        "                metric = accuracy_score(pack.yte, yhat) if pack.task==\"clf\" else r2_score(pack.yte, yhat)\n",
        "                wr.writerow([pack.name, m, k, metric])\n",
        "\n",
        "    # 4) AOPC (ExCIR)\n",
        "    fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scores[\"ExCIR\"], steps=9)\n",
        "    plt.figure(figsize=(6.5,4.3))\n",
        "    plt.plot(100*fr, ins, marker='o', label='Insertion (keep top-% by ExCIR)')\n",
        "    plt.plot(100*fr, dele, marker='o', label='Deletion (zero top-% by ExCIR)')\n",
        "    plt.xlabel('Revealed / removed top-% features'); plt.ylabel('Test ' + ('Accuracy' if pack.task=='clf' else 'R$^2$'))\n",
        "    plt.title(f'AOPC (ExCIR) — {pack.name}')\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"aopc_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # 5) stability under noise (CIR + Jaccard@8)\n",
        "    cirs, j8 = stability_noise_eval(lambda X, p: excir_feature_scores(X, p), pack.Xva, p_va, repeats=20, sigma=0.01)\n",
        "    plt.figure(figsize=(6.0,4.0))\n",
        "    plt.subplot(1,2,1); plt.hist(cirs, bins=10); plt.title('CIR under noise'); plt.xlabel('CIR'); plt.ylabel('count')\n",
        "    plt.subplot(1,2,2); plt.hist(j8, bins=10); plt.title('Jaccard@8 under noise'); plt.xlabel('overlap')\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(outdir, f\"stability_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # 6) lightweight checks + plots\n",
        "    F, T, CIRs, Resid, KLs, J8 = lightweight_three_checks(pack, seed=seed)\n",
        "    plot_agreement_bundle(pack, F, CIRs, Resid, KLs, outdir=outdir)\n",
        "    plot_pareto(pack, T, CIRs, F, J8, outdir=outdir)\n",
        "    plot_runtime_vs_fraction(pack, F, T, outdir=outdir)\n",
        "\n",
        "    # 7) if digits: class-conditioned ExCIR for class 3 (heatmap + AOPC)\n",
        "    if pack.name == \"digits8x8\":\n",
        "        clf = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs', random_state=seed)\n",
        "        clf.fit(np.vstack([pack.Xtr, pack.Xva]), np.r_[pack.ytr, pack.yva])\n",
        "        Pva = clf.predict_proba(pack.Xva)\n",
        "        c = 3\n",
        "        s_c = excir_feature_scores(pack.Xva, Pva[:, c])\n",
        "        H = s_c.reshape(8,8)\n",
        "        plt.figure(figsize=(4.5,4.5)); plt.imshow(H, cmap='viridis'); plt.colorbar()\n",
        "        plt.title('Class-conditioned ExCIR (digit 3)'); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_excir_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "        fr, ins, dele = aopc_curves(\"clf\", clf, pack.Xte, pack.yte, s_c, steps=9)\n",
        "        plt.figure(figsize=(6.2,4.2))\n",
        "        plt.plot(100*fr, ins, marker='o', label='Insertion (keep top-% pixels)')\n",
        "        plt.plot(100*fr, dele, marker='o', label='Deletion (zero top-% pixels)')\n",
        "        plt.xlabel('Revealed / removed top-% pixels'); plt.ylabel('Test Accuracy')\n",
        "        plt.title('AOPC — digits (class 3 map)'); plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_aopc_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Registry of loaders\n",
        "ALL_DATASETS = {\n",
        "    \"adult\": load_adult,\n",
        "    \"20ng_bin\": load_20ng_binary,\n",
        "    \"har6\": load_har,\n",
        "    \"digits8x8\": load_digits8x8,\n",
        "}\n"
      ],
      "metadata": {
        "id": "EXzbfAXqx8J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# excirc_bench.py\n",
        "# Runner script that uses excir_lib.py to reproduce all experiments.\n",
        "\n",
        "import argparse, numpy as np, os\n",
        "from excir_lib import (\n",
        "    DatasetPack, ALL_DATASETS, run_on_dataset, ensure_out\n",
        ")\n",
        "\n",
        "def main(argv=None):\n",
        "    p = argparse.ArgumentParser(description=\"ExCIR (original) — cross-domain benchmarks\")\n",
        "    p.add_argument(\"--datasets\", type=str,\n",
        "                   default=\"adult,20ng_bin,digits8x8,har6\",\n",
        "                   help=\"comma-separated list from {adult,20ng_bin,digits8x8,har6}\")\n",
        "    p.add_argument(\"--seed\", type=int, default=0)\n",
        "    p.add_argument(\"--ks\", type=str, default=\"3,5,8,12\")\n",
        "    p.add_argument(\"--outdir\", type=str, default=\"out\")\n",
        "    p.add_argument(\"--no-pfi\", action=\"store_true\", help=\"disable PFI (to go faster)\")\n",
        "    args = p.parse_args(argv)\n",
        "\n",
        "    ensure_out(args.outdir)\n",
        "    ks = tuple(int(x) for x in args.ks.split(\",\"))\n",
        "\n",
        "    names = [s.strip() for s in args.datasets.split(\",\") if s.strip()]\n",
        "    for nm in names:\n",
        "        if nm not in ALL_DATASETS:\n",
        "            print(f\"[skip] Unknown dataset key: {nm}\")\n",
        "            continue\n",
        "        try:\n",
        "            loader = ALL_DATASETS[nm]\n",
        "            pack = loader(seed=args.seed)\n",
        "            print(f\"=== Dataset: {pack.name} (task={pack.task}) ===\")\n",
        "            run_on_dataset(pack, ks=ks, seed=args.seed, do_pfi=(not args.no_pfi), outdir=args.outdir)\n",
        "        except Exception as e:\n",
        "            print(f\"[{nm}] ERROR: {e}\")\n",
        "\n",
        "    print(f\"\\nDone. See '{args.outdir}/' for CSVs and figures.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "I5kYtQb0x_Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "ExCIR (original Z,S formula) — all-in-one runner + library\n",
        "Produces in ./out:\n",
        "  summary_topk.csv\n",
        "  aopc_<dataset>.png\n",
        "  agreement_bundle_<dataset>.png  (CIR agreement, projection residual, KL via KDE)\n",
        "  pareto_<dataset>.png            (CIR vs time; size = Jaccard@8)\n",
        "  runtime_<dataset>.png\n",
        "  stability_<dataset>.png         (CIR under noise, Jaccard@8)\n",
        "  digits_excir_class3.png, digits_aopc_class3.png (digits only)\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, time, argparse\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance, partial_dependence\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from sklearn.datasets import fetch_openml, load_digits\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# -----------------------\n",
        "# Core ExCIR (original formula)\n",
        "# -----------------------\n",
        "\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    \"\"\"\n",
        "    Original ExCIR between two 1-D signals z, s (length n'):\n",
        "      CIR = n' * [ (mean(z)-m)^2 + (mean(s)-m)^2 ] /\n",
        "            [ sum_i (z_i - m)^2 + sum_i (s_i - m)^2 ],\n",
        "    where m = 0.5 * (mean(z) + mean(s)).\n",
        "    \"\"\"\n",
        "    z = np.asarray(z, dtype=float).ravel()\n",
        "    s = np.asarray(s, dtype=float).ravel()\n",
        "    assert z.shape[0] == s.shape[0], \"z and s must have the same length\"\n",
        "    mu_z, mu_s = z.mean(), s.mean()\n",
        "    m = 0.5 * (mu_z + mu_s)\n",
        "    num = z.shape[0] * ((mu_z - m) ** 2 + (mu_s - m) ** 2)\n",
        "    den = np.sum((z - m) ** 2) + np.sum((s - m) ** 2) + eps\n",
        "    return float(num / den)\n",
        "\n",
        "def excir_feature_scores(X: np.ndarray, p: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Per-feature ExCIR using the original Z,S formula (vectorized across columns).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(p, dtype=float).ravel()\n",
        "    n, d = X.shape\n",
        "    assert p.shape[0] == n, \"X and p must align on rows\"\n",
        "\n",
        "    mu_p = p.mean()\n",
        "    m_j = 0.5 * (X.mean(axis=0) + mu_p)                 # grand mean per feature\n",
        "    num = n * ((X.mean(axis=0) - m_j) ** 2 + (mu_p - m_j) ** 2)\n",
        "\n",
        "    # FIX: Correctly broadcast p against the per-feature mean m_j\n",
        "    term1 = np.sum((X - m_j) ** 2, axis=0)\n",
        "    term2 = np.sum((p[:, np.newaxis] - m_j) ** 2, axis=0)\n",
        "    den = term1 + term2 + eps\n",
        "\n",
        "    return num / den\n",
        "\n",
        "def excir_block_scores(X: np.ndarray, p: np.ndarray, blocks: List[List[int]]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Block ExCIR: average z-scored features inside each block, then apply cir_pair.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float); p = np.asarray(p, dtype=float).ravel()\n",
        "    Z = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-12)\n",
        "    out = []\n",
        "    for idxs in blocks:\n",
        "        zb = Z[:, idxs].mean(axis=1)\n",
        "        out.append(cir_pair(zb, p))\n",
        "    return np.asarray(out)\n",
        "\n",
        "# -----------------------\n",
        "# Agreement beyond ranks\n",
        "# -----------------------\n",
        "\n",
        "def projection_alignment_residual(y_full: np.ndarray, y_light: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Best affine map y_full ≈ a*y_light + b; normalized residual ||res||/||y_full||.\n",
        "    \"\"\"\n",
        "    y = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "    n = y.shape[0]\n",
        "    Phi = np.c_[yp, np.ones(n)]\n",
        "    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n",
        "    yhat = Phi @ theta\n",
        "    return float(np.linalg.norm(y - yhat) / (np.linalg.norm(y) + 1e-12))\n",
        "\n",
        "def kde_kl_symmetric(y_full: np.ndarray, y_light: np.ndarray, grid_points: int = 400) -> float:\n",
        "    \"\"\"\n",
        "    Symmetric KL between 1-D KDEs of the two outputs (no standardization).\n",
        "    \"\"\"\n",
        "    y = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "    kde_p = gaussian_kde(y)\n",
        "    kde_q = gaussian_kde(yp)\n",
        "    both = np.r_[y, yp]\n",
        "    lo, hi = np.percentile(both, 0.5), np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None)\n",
        "    q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo) / max(grid_points - 1, 1)\n",
        "    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))) * dx)\n",
        "    return 0.5 * (kl_pq + kl_qp)\n",
        "\n",
        "def jaccard_topk(a_scores: np.ndarray, b_scores: np.ndarray, k: int = 8) -> float:\n",
        "    A = set(np.argsort(-a_scores)[:k]); B = set(np.argsort(-b_scores)[:k])\n",
        "    return len(A & B) / (len(A | B) + 1e-12)\n",
        "\n",
        "# -----------------------\n",
        "# Baselines\n",
        "# -----------------------\n",
        "\n",
        "def pfi_scores(model, X, y, n_repeats: int = 10, seed: int = 0) -> np.ndarray:\n",
        "    return permutation_importance(model, X, y, n_repeats=n_repeats,\n",
        "                                  random_state=seed, scoring=None).importances_mean\n",
        "\n",
        "def tree_gain_scores(model, d: int) -> np.ndarray:\n",
        "    try:\n",
        "        w = model.feature_importances_\n",
        "        if w is None or len(w) != d:\n",
        "            return np.zeros(d)\n",
        "        return w\n",
        "    except Exception:\n",
        "        return np.zeros(d)\n",
        "\n",
        "def pdp_var_scores(model, X, grid_points: int = 20, random_state: int = 0) -> np.ndarray:\n",
        "    d = X.shape[1]; scores = np.zeros(d); rs = check_random_state(random_state)\n",
        "    idx = rs.choice(np.arange(X.shape[0]), size=min(2000, X.shape[0]), replace=False)\n",
        "    Xs = X[idx]\n",
        "    for j in range(d):\n",
        "        try:\n",
        "            pd = partial_dependence(model, Xs, features=[j], kind='average', grid_resolution=grid_points)\n",
        "            scores[j] = np.var(pd.average[0])\n",
        "        except Exception:\n",
        "            scores[j] = 0.0\n",
        "    return scores\n",
        "\n",
        "def surrogate_lr_scores(X, teacher_scores, alpha: float = 1.0) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float); y = np.asarray(teacher_scores, dtype=float).ravel()\n",
        "    reg = Ridge(alpha=alpha, random_state=0)\n",
        "    reg.fit(X, y)\n",
        "    w = np.abs(reg.coef_)\n",
        "    if w.ndim > 1:\n",
        "        w = np.linalg.norm(w, axis=0)\n",
        "    s = w / (w.sum() + 1e-12)\n",
        "    return s\n",
        "\n",
        "def mi_pred_scores(X, preds, task: str) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(preds, dtype=float).ravel()\n",
        "    if task == \"clf\":\n",
        "        z = (p > np.median(p)).astype(int)\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "    else:\n",
        "        z = np.digitize(p, np.quantile(p, [1/3, 2/3]))\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "\n",
        "def mi_label_scores(X, y, task: str) -> np.ndarray:\n",
        "    if task == \"clf\":\n",
        "        return mutual_info_classif(X, y, random_state=0)\n",
        "    else:\n",
        "        return mutual_info_regression(X, y, random_state=0)\n",
        "\n",
        "# -----------------------\n",
        "# AOPC & stability\n",
        "# -----------------------\n",
        "\n",
        "def rank_from_scores(scores: np.ndarray) -> np.ndarray:\n",
        "    return np.argsort(-scores)\n",
        "\n",
        "def aopc_curves(task: str, model, Xte, yte, scores: np.ndarray, steps: int = 11):\n",
        "    idx = rank_from_scores(scores)\n",
        "    n, d = Xte.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f * d))\n",
        "        keep = idx[:k]\n",
        "        mask = np.zeros(d, dtype=bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0)\n",
        "        X_del = np.where(mask, 0.0, Xte)\n",
        "        y_ins = model.predict(X_ins); y_del = model.predict(X_del)\n",
        "        if task == \"clf\":\n",
        "            ins.append(accuracy_score(yte, y_ins))\n",
        "            dele.append(accuracy_score(yte, y_del))\n",
        "        else:\n",
        "            ins.append(r2_score(yte, y_ins))\n",
        "            dele.append(r2_score(yte, y_del))\n",
        "    return fracs, np.asarray(ins), np.asarray(dele)\n",
        "\n",
        "def stability_noise_eval(scores_fn, Xva, p_va, repeats: int = 20, sigma: float = 0.01):\n",
        "    base = scores_fn(Xva, p_va)\n",
        "    cirs, j8 = [], []\n",
        "    for _ in range(repeats):\n",
        "        noise = np.random.normal(0, sigma, size=Xva.shape)\n",
        "        s_noisy = scores_fn(Xva + noise, p_va)\n",
        "        cirs.append(cir_pair(base, s_noisy))\n",
        "        j8.append(jaccard_topk(base, s_noisy, k=min(8, Xva.shape[1])))\n",
        "    return np.asarray(cirs), np.asarray(j8)\n",
        "\n",
        "# -----------------------\n",
        "# Lightweight three checks + plots\n",
        "# -----------------------\n",
        "\n",
        "def train_gboost(task: str, Xtr, ytr, Xva, yva, seed: int = 0):\n",
        "    Xall = np.vstack([Xtr, Xva]); yall = np.r_[ytr, yva]\n",
        "    if task == \"clf\":\n",
        "        m = GradientBoostingClassifier(random_state=seed)\n",
        "    else:\n",
        "        m = GradientBoostingRegressor(random_state=seed)\n",
        "    m.fit(Xall, yall)\n",
        "    return m\n",
        "\n",
        "def predict_scores(task: str, model, X) -> np.ndarray:\n",
        "    if task == \"clf\":\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            return model.predict_proba(X)[:, -1]\n",
        "        s = model.decision_function(X)\n",
        "        return 1.0/(1.0+np.exp(-s))\n",
        "    return model.predict(X)\n",
        "\n",
        "def lightweight_three_checks(pack, seed: int = 0, fractions=(0.2,0.3,0.4,0.5,0.75,1.0)):\n",
        "    rng = check_random_state(seed)\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    p_full = predict_scores(pack.task, base, pack.Xva)\n",
        "    s_full = excir_feature_scores(pack.Xva, p_full)\n",
        "\n",
        "    Xbig = np.vstack([pack.Xtr, pack.Xva])\n",
        "    ybig = np.r_[pack.ytr, pack.yva]\n",
        "\n",
        "    F, Times, CIRs, Resid, KLs, J8 = [], [], [], [], [], []\n",
        "    for f in fractions:\n",
        "        t0 = time.time()\n",
        "        n_rows = int(f * Xbig.shape[0])\n",
        "        idx = rng.choice(np.arange(Xbig.shape[0]), size=n_rows, replace=False)\n",
        "        m = GradientBoostingClassifier(random_state=seed) if pack.task == \"clf\" else GradientBoostingRegressor(random_state=seed)\n",
        "        m.fit(Xbig[idx], ybig[idx])\n",
        "        p_sub = predict_scores(pack.task, m, pack.Xva)\n",
        "        s_sub = excir_feature_scores(pack.Xva, p_sub)\n",
        "\n",
        "        CIRs.append(cir_pair(s_full, s_sub))\n",
        "        Resid.append(projection_alignment_residual(p_full, p_sub))\n",
        "        KLs.append(kde_kl_symmetric(p_full, p_sub))\n",
        "        J8.append(jaccard_topk(s_full, s_sub, k=min(8, s_full.shape[0])))\n",
        "        Times.append(time.time() - t0)\n",
        "        F.append(f)\n",
        "\n",
        "    return (np.asarray(F), np.asarray(Times), np.asarray(CIRs),\n",
        "            np.asarray(Resid), np.asarray(KLs), np.asarray(J8))\n",
        "\n",
        "def plot_agreement_bundle(pack, F, CIRs, Resid, KLs, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(9.4, 3.6))\n",
        "\n",
        "    ax1 = plt.subplot(1,3,1)\n",
        "    ax1.plot((100*F).astype(int), CIRs, marker='o')\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.set_xlabel('% rows kept'); ax1.set_ylabel('CIR(full, light)')\n",
        "    ax1.set_title('Agreement (CIR)')\n",
        "\n",
        "    ax2 = plt.subplot(1,3,2)\n",
        "    ax2.plot((100*F).astype(int), Resid, marker='o')\n",
        "    ax2.set_xlabel('% rows kept'); ax2.set_ylabel('Projection residual')\n",
        "    ax2.set_title('Shape agreement')\n",
        "\n",
        "    ax3 = plt.subplot(1,3,3)\n",
        "    ax3.plot((100*F).astype(int), KLs, marker='o')\n",
        "    ax3.set_xlabel('% rows kept'); ax3.set_ylabel('Sym. KL (KDE)')\n",
        "    ax3.set_title('Distribution match')\n",
        "\n",
        "    plt.suptitle(f'Agreement beyond ranks — {pack.name}', y=1.02, fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"agreement_bundle_{pack.name}.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def plot_pareto(pack, Times, CIRs, F, J8, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(6.2, 4.8))\n",
        "    plt.scatter(Times, CIRs, s=200*np.maximum(J8, 0.05), alpha=0.85)\n",
        "    for i, f in enumerate(F):\n",
        "        plt.text(Times[i], CIRs[i], f\"{int(100*f)}%\", ha='center', va='bottom', fontsize=9)\n",
        "    plt.xlabel('Wall time (s)'); plt.ylabel('CIR(full, light)')\n",
        "    plt.title(f'Agreement–cost (ExCIR) — {pack.name}')\n",
        "    plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"pareto_{pack.name}.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def plot_runtime_vs_fraction(pack, F, Times, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(6.0,4.0))\n",
        "    plt.plot((100*F).astype(int), Times, marker='o')\n",
        "    plt.xlabel('Fraction of rows kept (%)'); plt.ylabel('Wall time (s)')\n",
        "    plt.title(f'Runtime vs fraction — {pack.name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"runtime_{pack.name}.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# -----------------------\n",
        "# Datasets\n",
        "# -----------------------\n",
        "\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str\n",
        "    task: str\n",
        "    Xtr: np.ndarray\n",
        "    ytr: np.ndarray\n",
        "    Xva: np.ndarray\n",
        "    yva: np.ndarray\n",
        "    Xte: np.ndarray\n",
        "    yte: np.ndarray\n",
        "    feature_names: List[str]\n",
        "\n",
        "def _standardize_fit(Xtr, Xva, Xte):\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    return ss.fit_transform(Xtr), ss.transform(Xva), ss.transform(Xte)\n",
        "\n",
        "def load_adult(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "    dfX = ds.data.select_dtypes(include=[np.number]).copy()  # keep numeric only\n",
        "    y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "    X = dfX.to_numpy(dtype=float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"adult\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, list(dfX.columns))\n",
        "\n",
        "def load_20ng_binary(seed=0) -> DatasetPack:\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    cats = ['comp.graphics', 'sci.space']\n",
        "    tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "    te = fetch_20newsgroups(subset='test', categories=cats, remove=('headers','footers','quotes'))\n",
        "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
        "    Xtr_full = tfidf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "    Xte = tfidf.transform(te.data).astype(np.float32).toarray()\n",
        "    ytr_full, yte = tr.target, te.target\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2, stratify=ytr_full, random_state=seed)\n",
        "    return DatasetPack(\"20ng_bin\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "def load_har(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"HAR\", version=1, as_frame=True)  # 6-class smartphone HAR\n",
        "    X = ds.data.to_numpy(dtype=float)\n",
        "    y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"har6\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "def load_digits8x8(seed=0) -> DatasetPack:\n",
        "    digits = load_digits()\n",
        "    X = digits.data.astype(float) / 16.0\n",
        "    y = digits.target\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"digits8x8\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"pix{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "ALL_DATASETS = {\n",
        "    \"adult\": load_adult,\n",
        "    \"20ng_bin\": load_20ng_binary,\n",
        "    \"har6\": load_har,\n",
        "    \"digits8x8\": load_digits8x8,\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# End-to-end per dataset\n",
        "# -----------------------\n",
        "\n",
        "def ensure_out(outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "def run_on_dataset(pack: DatasetPack, ks=(3,5,8,12), seed: int = 0,\n",
        "                   do_pfi: bool = True, outdir=\"out\") -> Dict[str, np.ndarray]:\n",
        "    ensure_out(outdir)\n",
        "\n",
        "    # 1) train base model\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    p_va = predict_scores(pack.task, base, pack.Xva)\n",
        "\n",
        "    # 2) score vectors on validation\n",
        "    scores: Dict[str, np.ndarray] = {}\n",
        "    scores[\"ExCIR\"] = excir_feature_scores(pack.Xva, p_va)\n",
        "\n",
        "    if do_pfi:\n",
        "        scores[\"PFI\"] = pfi_scores(base, pack.Xva, pack.yva, n_repeats=10, seed=seed)\n",
        "    scores[\"TreeGain\"]  = tree_gain_scores(base, pack.Xva.shape[1])\n",
        "    scores[\"PDP-var\"]   = pdp_var_scores(base, pack.Xva)\n",
        "    scores[\"MI(pred)\"]  = mi_pred_scores(pack.Xva, p_va, task=pack.task)\n",
        "    scores[\"MI(label)\"] = mi_label_scores(pack.Xva, pack.yva, task=pack.task)\n",
        "    scores[\"Surrogate-LR\"] = surrogate_lr_scores(pack.Xva, p_va)\n",
        "\n",
        "    # 3) top-k sufficiency -> CSV\n",
        "    import csv\n",
        "    csv_path = os.path.join(outdir, \"summary_topk.csv\")\n",
        "    header = not os.path.exists(csv_path)\n",
        "    with open(csv_path, \"a\", newline=\"\") as f:\n",
        "        wr = csv.writer(f)\n",
        "        if header:\n",
        "            wr.writerow([\"dataset\",\"method\",\"k\",\"metric\"])\n",
        "        for m, v in scores.items():\n",
        "            ranks = rank_from_scores(v)\n",
        "            for k in ks:\n",
        "                keep = ranks[:k]\n",
        "                model_k = GradientBoostingClassifier(random_state=seed) if pack.task==\"clf\" \\\n",
        "                          else GradientBoostingRegressor(random_state=seed)\n",
        "                Xtr_k = np.vstack([pack.Xtr, pack.Xva])[:, keep]\n",
        "                ytr_k = np.r_[pack.ytr, pack.yva]\n",
        "                model_k.fit(Xtr_k, ytr_k)\n",
        "                yhat = model_k.predict(pack.Xte[:, keep])\n",
        "                metric = accuracy_score(pack.yte, yhat) if pack.task==\"clf\" else r2_score(pack.yte, yhat)\n",
        "                wr.writerow([pack.name, m, k, metric])\n",
        "\n",
        "    # 4) AOPC (ExCIR)\n",
        "    fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scores[\"ExCIR\"], steps=9)\n",
        "    plt.figure(figsize=(6.5,4.3))\n",
        "    plt.plot(100*fr, ins, marker='o', label='Insertion (keep top-% by ExCIR)')\n",
        "    plt.plot(100*fr, dele, marker='o', label='Deletion (zero top-% by ExCIR)')\n",
        "    plt.xlabel('Revealed / removed top-% features'); plt.ylabel('Test ' + ('Accuracy' if pack.task=='clf' else 'R$^2$'))\n",
        "    plt.title(f'AOPC (ExCIR) — {pack.name}')\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"aopc_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # 5) stability (CIR + Jaccard@8)\n",
        "    cirs, j8 = stability_noise_eval(lambda X, p: excir_feature_scores(X, p), pack.Xva, p_va, repeats=20, sigma=0.01)\n",
        "    plt.figure(figsize=(6.0,4.0))\n",
        "    plt.subplot(1,2,1); plt.hist(cirs, bins=10); plt.title('CIR under noise'); plt.xlabel('CIR'); plt.ylabel('count')\n",
        "    plt.subplot(1,2,2); plt.hist(j8, bins=10); plt.title('Jaccard@8 under noise'); plt.xlabel('overlap')\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(outdir, f\"stability_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # 6) lightweight three checks + plots\n",
        "    F, T, CIRs, Resid, KLs, J8 = lightweight_three_checks(pack, seed=seed)\n",
        "    plot_agreement_bundle(pack, F, CIRs, Resid, KLs, outdir=outdir)\n",
        "    plot_pareto(pack, T, CIRs, F, J8, outdir=outdir)\n",
        "    plot_runtime_vs_fraction(pack, F, T, outdir=outdir)\n",
        "\n",
        "    # 7) digits: class-conditioned ExCIR (class 3)\n",
        "    if pack.name == \"digits8x8\":\n",
        "        clf = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs', random_state=seed)\n",
        "        clf.fit(np.vstack([pack.Xtr, pack.Xva]), np.r_[pack.ytr, pack.yva])\n",
        "        Pva = clf.predict_proba(pack.Xva)\n",
        "        c = 3\n",
        "        s_c = excir_feature_scores(pack.Xva, Pva[:, c])\n",
        "        H = s_c.reshape(8,8)\n",
        "        plt.figure(figsize=(4.5,4.5)); plt.imshow(H, cmap='viridis'); plt.colorbar()\n",
        "        plt.title('Class-conditioned ExCIR (digit 3)'); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_excir_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "        fr, ins, dele = aopc_curves(\"clf\", clf, pack.Xte, pack.yte, s_c, steps=9)\n",
        "        plt.figure(figsize=(6.2,4.2))\n",
        "        plt.plot(100*fr, ins, marker='o', label='Insertion (keep top-% pixels)')\n",
        "        plt.plot(100*fr, dele, marker='o', label='Deletion (zero top-% pixels)')\n",
        "        plt.xlabel('Revealed / removed top-% pixels'); plt.ylabel('Test Accuracy')\n",
        "        plt.title('AOPC — digits (class 3 map)'); plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_aopc_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "    return scores\n",
        "\n",
        "# -----------------------\n",
        "# CLI\n",
        "# -----------------------\n",
        "\n",
        "# -----------------------\n",
        "# CLI (Colab-safe)\n",
        "# -----------------------\n",
        "\n",
        "def _run_everything(datasets, seed, ks_tuple, outdir, do_pfi):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    registry = {\n",
        "        \"adult\": load_adult,\n",
        "        \"20ng_bin\": load_20ng_binary,\n",
        "        \"har6\": load_har,\n",
        "        \"digits8x8\": load_digits8x8,\n",
        "    }\n",
        "    names = [s.strip() for s in datasets.split(\",\") if s.strip()]\n",
        "    for nm in names:\n",
        "        if nm not in registry:\n",
        "            print(f\"[skip] Unknown dataset: {nm}\")\n",
        "            continue\n",
        "        try:\n",
        "            pack = registry[nm](seed=seed)\n",
        "            print(f\"=== Dataset: {pack.name} (task={pack.task}) ===\")\n",
        "            run_on_dataset(pack, ks=ks_tuple, seed=seed, do_pfi=do_pfi, outdir=outdir)\n",
        "        except Exception as e:\n",
        "            print(f\"[{nm}] ERROR: {e}\")\n",
        "    print(f\"\\nDone. See '{outdir}/' for CSVs and figures.\")\n",
        "\n",
        "def main(argv=None):\n",
        "    import argparse\n",
        "    p = argparse.ArgumentParser(description=\"ExCIR (original) — cross-domain benchmarks (single file)\")\n",
        "    p.add_argument(\"--datasets\", type=str, default=\"adult,20ng_bin,digits8x8,har6\",\n",
        "                   help=\"subset from {adult,20ng_bin,digits8x8,har6}\")\n",
        "    p.add_argument(\"--seed\", type=int, default=0)\n",
        "    p.add_argument(\"--ks\", type=str, default=\"3,5,8,12\")\n",
        "    p.add_argument(\"--outdir\", type=str, default=\"out\")\n",
        "    p.add_argument(\"--no-pfi\", action=\"store_true\", help=\"disable PFI (speed)\")\n",
        "\n",
        "    # COLAB/JUPYTER FRIENDLY: swallow unknown args like \"-f <kernel.json>\"\n",
        "    if argv is None:\n",
        "        args, _ = p.parse_known_args()\n",
        "    else:\n",
        "        args = p.parse_args(argv)\n",
        "\n",
        "    ks_tuple = tuple(int(x) for x in args.ks.split(\",\"))\n",
        "    _run_everything(datasets=args.datasets,\n",
        "                    seed=args.seed,\n",
        "                    ks_tuple=ks_tuple,\n",
        "                    outdir=args.outdir,\n",
        "                    do_pfi=(not args.no_pfi))\n",
        "\n",
        "# Programmatic entry point (use this in notebooks)\n",
        "def run_bench(datasets=\"adult,20ng_bin,digits8x8,har6\",\n",
        "              seed=0, ks=(3,5,8,12), outdir=\"out\", no_pfi=False):\n",
        "    _run_everything(datasets=datasets, seed=seed, ks_tuple=ks, outdir=outdir, do_pfi=(not no_pfi))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()   # safe in Colab because we used parse_known_args() above"
      ],
      "metadata": {
        "id": "FANDmz8cyB8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "ExCIR (original Z,S formula) — all-in-one runner + library\n",
        "Datasets (10 modalities):\n",
        "  - adult (tabular clf, OpenML numeric subset)\n",
        "  - 20ng_bin (text clf, TF-IDF)\n",
        "  - digits8x8 (image clf)\n",
        "  - har6 (wearable signals clf, OpenML)\n",
        "  - california (tabular reg)\n",
        "  - diabetes (tabular reg)\n",
        "  - iris (tabular clf, simple multi-class)\n",
        "  - wine (tabular clf, multi-class)\n",
        "  - synthetic_clf (synthetic, controlled features)\n",
        "  - synthetic_reg (synthetic, controlled features)\n",
        "\n",
        "Produces in ./out:\n",
        "  summary_topk.csv\n",
        "  aopc_<dataset>.png\n",
        "  agreement_bundle_<dataset>.png  (CIR agreement, projection residual, KL via KDE)\n",
        "  pareto_<dataset>.png            (CIR vs time; size = Jaccard@8)\n",
        "  runtime_<dataset>.png\n",
        "  stability_<dataset>.png         (CIR under noise, Jaccard@8)\n",
        "  digits_excir_class3.png, digits_aopc_class3.png (digits only)\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, time, sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance, partial_dependence\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from sklearn.datasets import (\n",
        "    fetch_openml, load_digits, load_iris, load_wine,\n",
        "    fetch_20newsgroups, fetch_california_housing, load_diabetes,\n",
        "    make_classification, make_regression\n",
        ")\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Set a higher recursion limit for safety on some datasets, though the\n",
        "# try-except block is the main fix.\n",
        "sys.setrecursionlimit(2000)\n",
        "\n",
        "# -----------------------\n",
        "# Core ExCIR (original formula; vectorized and fixed)\n",
        "# -----------------------\n",
        "\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    \"\"\"\n",
        "    Original ExCIR between two 1-D signals z, s (length n'):\n",
        "      CIR = n' * [ (mean(z)-m)^2 + (mean(s)-m)^2 ] /\n",
        "            [ sum_i (z_i - m)^2 + sum_i (s_i - m)^2 ],\n",
        "    where m = 0.5 * (mean(z) + mean(s)).\n",
        "    \"\"\"\n",
        "    z = np.asarray(z, dtype=float).ravel()\n",
        "    s = np.asarray(s, dtype=float).ravel()\n",
        "    assert z.shape[0] == s.shape[0], \"z and s must have the same length\"\n",
        "    mu_z, mu_s = z.mean(), s.mean()\n",
        "    m = 0.5 * (mu_z + mu_s)\n",
        "    num = z.shape[0] * ((mu_z - m) ** 2 + (mu_s - m) ** 2)\n",
        "    den = np.sum((z - m) ** 2) + np.sum((s - m) ** 2) + eps\n",
        "    return float(num / den)\n",
        "\n",
        "def excir_feature_scores(X: np.ndarray, p: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Per-feature ExCIR using the original Z,S formula (vectorized across columns).\n",
        "    For each feature j:\n",
        "      CIR_j = n * [ (mean(X_j)-m_j)^2 + (mean(p)-m_j)^2 ] /\n",
        "              [ sum_i (X_ij - m_j)^2 + sum_i (p_i - m_j)^2 ],\n",
        "    where m_j = 0.5 * (mean(X_j) + mean(p)).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(p, dtype=float).ravel()\n",
        "    n, d = X.shape\n",
        "    assert p.shape[0] == n, \"X and p must align on rows\"\n",
        "\n",
        "    mu_p = p.mean()\n",
        "    mu_x = X.mean(axis=0)        # (d,)\n",
        "    m_j  = 0.5 * (mu_x + mu_p)   # (d,)\n",
        "\n",
        "    num = n * ((mu_x - m_j)**2 + (mu_p - m_j)**2)  # (d,)\n",
        "\n",
        "    # Denominator with safe broadcasting\n",
        "    den = np.sum((X - m_j)**2, axis=0) + np.sum((p[:, None] - m_j[None, :])**2, axis=0) + eps\n",
        "    return num / den\n",
        "\n",
        "def excir_block_scores(X: np.ndarray, p: np.ndarray, blocks: List[List[int]]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Block ExCIR: average z-scored features inside each block, then apply cir_pair.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float); p = np.asarray(p, dtype=float).ravel()\n",
        "    Z = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-12)\n",
        "    out = []\n",
        "    for idxs in blocks:\n",
        "        zb = Z[:, idxs].mean(axis=1)\n",
        "        out.append(cir_pair(zb, p))\n",
        "    return np.asarray(out)\n",
        "\n",
        "# -----------------------\n",
        "# Agreement beyond ranks\n",
        "# -----------------------\n",
        "\n",
        "def projection_alignment_residual(y_full: np.ndarray, y_light: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Best affine map y_full ≈ a*y_light + b; normalized residual ||res||/||y_full||.\n",
        "    \"\"\"\n",
        "    y = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "    n = y.shape[0]\n",
        "    Phi = np.c_[yp, np.ones(n)]\n",
        "    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n",
        "    yhat = Phi @ theta\n",
        "    return float(np.linalg.norm(y - yhat) / (np.linalg.norm(y) + 1e-12))\n",
        "\n",
        "def kde_kl_symmetric(y_full: np.ndarray, y_light: np.ndarray, grid_points: int = 400) -> float:\n",
        "    \"\"\"\n",
        "    Symmetric KL between 1-D KDEs of the two outputs (no standardization).\n",
        "    \"\"\"\n",
        "    y = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "    kde_p = gaussian_kde(y)\n",
        "    kde_q = gaussian_kde(yp)\n",
        "    both = np.r_[y, yp]\n",
        "    lo, hi = np.percentile(both, 0.5), np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None)\n",
        "    q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo) / max(grid_points - 1, 1)\n",
        "    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))) * dx)\n",
        "    return 0.5 * (kl_pq + kl_qp)\n",
        "\n",
        "def jaccard_topk(a_scores: np.ndarray, b_scores: np.ndarray, k: int = 8) -> float:\n",
        "    A = set(np.argsort(-a_scores)[:k]); B = set(np.argsort(-b_scores)[:k])\n",
        "    return len(A & B) / (len(A | B) + 1e-12)\n",
        "\n",
        "# -----------------------\n",
        "# Baselines\n",
        "# -----------------------\n",
        "\n",
        "def pfi_scores(model, X, y, n_repeats: int = 10, seed: int = 0) -> np.ndarray:\n",
        "    return permutation_importance(model, X, y, n_repeats=n_repeats,\n",
        "                                  random_state=seed, scoring=None).importances_mean\n",
        "\n",
        "def tree_gain_scores(model, d: int) -> np.ndarray:\n",
        "    try:\n",
        "        w = model.feature_importances_\n",
        "        if w is None or len(w) != d:\n",
        "            return np.zeros(d)\n",
        "        return w\n",
        "    except Exception:\n",
        "        return np.zeros(d)\n",
        "\n",
        "def pdp_var_scores(model, X, grid_points: int = 20, random_state: int = 0) -> np.ndarray:\n",
        "    d = X.shape[1]; scores = np.zeros(d); rs = check_random_state(random_state)\n",
        "    idx = rs.choice(np.arange(X.shape[0]), size=min(2000, X.shape[0]), replace=False)\n",
        "    Xs = X[idx]\n",
        "    for j in range(d):\n",
        "        try:\n",
        "            pd = partial_dependence(model, Xs, features=[j], kind='average', grid_resolution=grid_points)\n",
        "            scores[j] = np.var(pd.average[0])\n",
        "        except Exception:\n",
        "            scores[j] = 0.0\n",
        "    return scores\n",
        "\n",
        "def surrogate_lr_scores(X, teacher_scores, alpha: float = 1.0) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float); y = np.asarray(teacher_scores, dtype=float).ravel()\n",
        "    reg = Ridge(alpha=alpha, random_state=0)\n",
        "    reg.fit(X, y)\n",
        "    w = np.abs(reg.coef_)\n",
        "    if w.ndim > 1:\n",
        "        w = np.linalg.norm(w, axis=0)\n",
        "    s = w / (w.sum() + 1e-12)\n",
        "    return s\n",
        "\n",
        "def mi_pred_scores(X, preds, task: str) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(preds, dtype=float).ravel()\n",
        "    if task == \"clf\":\n",
        "        z = (p > np.median(p)).astype(int)\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "    else:\n",
        "        z = np.digitize(p, np.quantile(p, [1/3, 2/3]))\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "\n",
        "def mi_label_scores(X, y, task: str) -> np.ndarray:\n",
        "    if task == \"clf\":\n",
        "        return mutual_info_classif(X, y, random_state=0)\n",
        "    else:\n",
        "        return mutual_info_regression(X, y, random_state=0)\n",
        "\n",
        "# -----------------------\n",
        "# AOPC & stability\n",
        "# -----------------------\n",
        "\n",
        "def rank_from_scores(scores: np.ndarray) -> np.ndarray:\n",
        "    return np.argsort(-scores)\n",
        "\n",
        "def aopc_curves(task: str, model, Xte, yte, scores: np.ndarray, steps: int = 11):\n",
        "    idx = rank_from_scores(scores)\n",
        "    n, d = Xte.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f * d))\n",
        "        keep = idx[:k]\n",
        "        mask = np.zeros(d, dtype=bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0)\n",
        "        X_del = np.where(mask, 0.0, Xte)\n",
        "        y_ins = model.predict(X_ins); y_del = model.predict(X_del)\n",
        "        if task == \"clf\":\n",
        "            ins.append(accuracy_score(yte, y_ins))\n",
        "            dele.append(accuracy_score(yte, y_del))\n",
        "        else:\n",
        "            ins.append(r2_score(yte, y_ins))\n",
        "            dele.append(r2_score(yte, y_del))\n",
        "    return fracs, np.asarray(ins), np.asarray(dele)\n",
        "\n",
        "def stability_noise_eval(scores_fn, Xva, p_va, repeats: int = 20, sigma: float = 0.01):\n",
        "    base = scores_fn(Xva, p_va)\n",
        "    cirs, j8 = [], []\n",
        "    for _ in range(repeats):\n",
        "        noise = np.random.normal(0, sigma, size=Xva.shape)\n",
        "        s_noisy = scores_fn(Xva + noise, p_va)\n",
        "        cirs.append(cir_pair(base, s_noisy))\n",
        "        j8.append(jaccard_topk(base, s_noisy, k=min(8, Xva.shape[1])))\n",
        "    return np.asarray(cirs), np.asarray(j8)\n",
        "\n",
        "# -----------------------\n",
        "# Lightweight three checks + plots\n",
        "# -----------------------\n",
        "\n",
        "def train_gboost(task: str, Xtr, ytr, Xva, yva, seed: int = 0):\n",
        "    Xall = np.vstack([Xtr, Xva]); yall = np.r_[ytr, yva]\n",
        "    if task == \"clf\":\n",
        "        m = GradientBoostingClassifier(random_state=seed)\n",
        "    else:\n",
        "        m = GradientBoostingRegressor(random_state=seed)\n",
        "    # The recursion error often occurs here on small datasets.\n",
        "    try:\n",
        "        m.fit(Xall, yall)\n",
        "    except RecursionError:\n",
        "        print(\"  - skipping GBM training due to RecursionError on small dataset.\")\n",
        "        return None # Return None if training fails\n",
        "    return m\n",
        "\n",
        "def predict_scores(task: str, model, X) -> np.ndarray:\n",
        "    if model is None: return np.zeros(X.shape[0])\n",
        "    if task == \"clf\":\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            return model.predict_proba(X)[:, -1]\n",
        "        s = model.decision_function(X)\n",
        "        return 1.0/(1.0+np.exp(-s))\n",
        "    return model.predict(X)\n",
        "\n",
        "def lightweight_three_checks(pack, seed: int = 0, fractions=(0.2,0.3,0.4,0.5,0.75,1.0)):\n",
        "    rng = check_random_state(seed)\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    if base is None:\n",
        "        return (np.asarray(fractions), np.full(len(fractions), np.nan),\n",
        "                np.full(len(fractions), np.nan), np.full(len(fractions), np.nan),\n",
        "                np.full(len(fractions), np.nan), np.full(len(fractions), np.nan))\n",
        "\n",
        "    p_full = predict_scores(pack.task, base, pack.Xva)\n",
        "    s_full = excir_feature_scores(pack.Xva, p_full)\n",
        "\n",
        "    Xbig = np.vstack([pack.Xtr, pack.Xva])\n",
        "    ybig = np.r_[pack.ytr, pack.yva]\n",
        "\n",
        "    F, Times, CIRs, Resid, KLs, J8 = [], [], [], [], [], []\n",
        "    for f in fractions:\n",
        "        t0 = time.time()\n",
        "        n_rows = int(f * Xbig.shape[0])\n",
        "        idx = rng.choice(np.arange(Xbig.shape[0]), size=n_rows, replace=False)\n",
        "        m = GradientBoostingClassifier(random_state=seed) if pack.task == \"clf\" else GradientBoostingRegressor(random_state=seed)\n",
        "        try:\n",
        "            m.fit(Xbig[idx], ybig[idx])\n",
        "            p_sub = predict_scores(pack.task, m, pack.Xva)\n",
        "            s_sub = excir_feature_scores(pack.Xva, p_sub)\n",
        "            CIRs.append(cir_pair(s_full, s_sub))\n",
        "            Resid.append(projection_alignment_residual(p_full, p_sub))\n",
        "            KLs.append(kde_kl_symmetric(p_full, p_sub))\n",
        "            J8.append(jaccard_topk(s_full, s_sub, k=min(8, s_full.shape[0])))\n",
        "        except RecursionError:\n",
        "            print(f\"  - skipping check for f={f} due to RecursionError.\")\n",
        "            CIRs.append(np.nan); Resid.append(np.nan); KLs.append(np.nan); J8.append(np.nan)\n",
        "\n",
        "        Times.append(time.time() - t0)\n",
        "        F.append(f)\n",
        "\n",
        "    return (np.asarray(F), np.asarray(Times), np.asarray(CIRs),\n",
        "            np.asarray(Resid), np.asarray(KLs), np.asarray(J8))\n",
        "\n",
        "def plot_agreement_bundle(pack, F, CIRs, Resid, KLs, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(9.4, 3.6))\n",
        "\n",
        "    ax1 = plt.subplot(1,3,1)\n",
        "    ax1.plot((100*F).astype(int), CIRs, marker='o')\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.set_xlabel('% rows kept'); ax1.set_ylabel('CIR(full, light)')\n",
        "    ax1.set_title('Agreement (CIR)')\n",
        "\n",
        "    ax2 = plt.subplot(1,3,2)\n",
        "    ax2.plot((100*F).astype(int), Resid, marker='o')\n",
        "    ax2.set_xlabel('% rows kept'); ax2.set_ylabel('Projection residual')\n",
        "    ax2.set_title('Shape agreement')\n",
        "\n",
        "    ax3 = plt.subplot(1,3,3)\n",
        "    ax3.plot((100*F).astype(int), KLs, marker='o')\n",
        "    ax3.set_xlabel('% rows kept'); ax3.set_ylabel('Sym. KL (KDE)')\n",
        "    ax3.set_title('Distribution match')\n",
        "\n",
        "    plt.suptitle(f'Agreement beyond ranks — {pack.name}', y=1.02, fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"agreement_bundle_{pack.name}.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def plot_pareto(pack, Times, CIRs, F, J8, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(6.2, 4.8))\n",
        "    plt.scatter(Times, CIRs, s=200*np.maximum(J8, 0.05), alpha=0.85)\n",
        "    for i, f in enumerate(F):\n",
        "        plt.text(Times[i], CIRs[i], f\"{int(100*f)}%\", ha='center', va='bottom', fontsize=9)\n",
        "    plt.xlabel('Wall time (s)'); plt.ylabel('CIR(full, light)')\n",
        "    plt.title(f'Agreement–cost (ExCIR) — {pack.name}')\n",
        "    plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"pareto_{pack.name}.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def plot_runtime_vs_fraction(pack, F, Times, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(6.0,4.0))\n",
        "    plt.plot((100*F).astype(int), Times, marker='o')\n",
        "    plt.xlabel('Fraction of rows kept (%)'); plt.ylabel('Wall time (s)')\n",
        "    plt.title(f'Runtime vs fraction — {pack.name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"runtime_{pack.name}.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# -----------------------\n",
        "# Datasets\n",
        "# -----------------------\n",
        "\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str\n",
        "    task: str  # \"clf\" | \"reg\"\n",
        "    Xtr: np.ndarray\n",
        "    ytr: np.ndarray\n",
        "    Xva: np.ndarray\n",
        "    yva: np.ndarray\n",
        "    Xte: np.ndarray\n",
        "    yte: np.ndarray\n",
        "    feature_names: List[str]\n",
        "\n",
        "def _standardize_fit(Xtr, Xva, Xte):\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    return ss.fit_transform(Xtr), ss.transform(Xva), ss.transform(Xte)\n",
        "\n",
        "def load_adult(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "    dfX = ds.data.select_dtypes(include=[np.number]).copy()\n",
        "    y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "    X = dfX.to_numpy(dtype=float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"adult\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, list(dfX.columns))\n",
        "\n",
        "def load_20ng_binary(seed=0) -> DatasetPack:\n",
        "    cats = ['comp.graphics', 'sci.space']\n",
        "    tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "    te = fetch_20newsgroups(subset='test', categories=cats, remove=('headers','footers','quotes'))\n",
        "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
        "    Xtr_full = tfidf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "    Xte = tfidf.transform(te.data).astype(np.float32).toarray()\n",
        "    ytr_full, yte = tr.target, te.target\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2, stratify=ytr_full, random_state=seed)\n",
        "    return DatasetPack(\"20ng_bin\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(Xtr.shape[1])])\n",
        "\n",
        "def load_har(seed=0) -> DatasetPack:\n",
        "    ds = fetch_openml(\"HAR\", version=1, as_frame=True)\n",
        "    X = ds.data.to_numpy(dtype=float)\n",
        "    y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"har6\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"f{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_digits8x8(seed=0) -> DatasetPack:\n",
        "    digits = load_digits()\n",
        "    X = digits.data.astype(float) / 16.0\n",
        "    y = digits.target\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"digits8x8\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"pix{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_california(seed=0) -> DatasetPack:\n",
        "    ds = fetch_california_housing()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"california\", \"reg\", Xtr, ytr, Xva, yva, Xte, yte, list(ds.feature_names))\n",
        "\n",
        "def load_diabetes(seed=0) -> DatasetPack:\n",
        "    ds = load_diabetes()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    feat_names = list(ds.feature_names) if hasattr(ds, \"feature_names\") else [f\"f{i}\" for i in range(X.shape[1])]\n",
        "    return DatasetPack(\"diabetes\", \"reg\", Xtr, ytr, Xva, yva, Xte, yte, feat_names)\n",
        "\n",
        "def load_iris(seed=0) -> DatasetPack:\n",
        "    ds = load_iris()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"iris\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, list(ds.feature_names))\n",
        "\n",
        "def load_wine(seed=0) -> DatasetPack:\n",
        "    ds = load_wine()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"wine\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, list(ds.feature_names))\n",
        "\n",
        "def load_synthetic_clf(seed=0) -> DatasetPack:\n",
        "    X, y = make_classification(\n",
        "        n_samples=5000, n_features=50, n_informative=10, n_redundant=15, n_repeated=5,\n",
        "        n_classes=2, random_state=seed\n",
        "    )\n",
        "    feature_names = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"synthetic_clf\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, feature_names)\n",
        "\n",
        "def load_synthetic_reg(seed=0) -> DatasetPack:\n",
        "    X, y = make_regression(\n",
        "        n_samples=5000, n_features=50, n_informative=10,\n",
        "        n_targets=1, random_state=seed\n",
        "    )\n",
        "    feature_names = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"synthetic_reg\", \"reg\", Xtr, ytr, Xva, yva, Xte, yte, feature_names)\n",
        "\n",
        "ALL_DATASETS = {\n",
        "    \"adult\": load_adult,\n",
        "    \"20ng_bin\": load_20ng_binary,\n",
        "    \"har6\": load_har,\n",
        "    \"digits8x8\": load_digits8x8,\n",
        "    \"california\": load_california,\n",
        "    \"diabetes\": load_diabetes,\n",
        "    \"iris\": load_iris,\n",
        "    \"wine\": load_wine,\n",
        "    \"synthetic_clf\": load_synthetic_clf,\n",
        "    \"synthetic_reg\": load_synthetic_reg,\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# End-to-end per dataset\n",
        "# -----------------------\n",
        "\n",
        "def ensure_out(outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "def run_on_dataset(pack: DatasetPack, ks=(3,5,8,12), seed: int = 0,\n",
        "                   do_pfi: bool = True, outdir=\"out\") -> Dict[str, np.ndarray]:\n",
        "    ensure_out(outdir)\n",
        "\n",
        "    print(\"  - Training base model...\")\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    if base is None:\n",
        "        print(\"  - Skipping feature importance benchmarks due to model training failure.\")\n",
        "        return {}\n",
        "\n",
        "    p_va = predict_scores(pack.task, base, pack.Xva)\n",
        "\n",
        "    print(\"  - Calculating feature importance scores...\")\n",
        "    scores: Dict[str, np.ndarray] = {}\n",
        "    scores[\"ExCIR\"] = excir_feature_scores(pack.Xva, p_va)\n",
        "\n",
        "    if do_pfi:\n",
        "        try:\n",
        "            scores[\"PFI\"] = pfi_scores(base, pack.Xva, pack.yva, n_repeats=10, seed=seed)\n",
        "        except Exception:\n",
        "            scores[\"PFI\"] = np.zeros(pack.Xva.shape[1])\n",
        "    scores[\"TreeGain\"]  = tree_gain_scores(base, pack.Xva.shape[1])\n",
        "    scores[\"PDP-var\"]   = pdp_var_scores(base, pack.Xva)\n",
        "    scores[\"MI(pred)\"]  = mi_pred_scores(pack.Xva, p_va, task=pack.task)\n",
        "    scores[\"MI(label)\"] = mi_label_scores(pack.Xva, pack.yva, task=pack.task)\n",
        "    scores[\"Surrogate-LR\"] = surrogate_lr_scores(pack.Xva, p_va)\n",
        "\n",
        "    # 3) top-k sufficiency -> CSV (train smaller GBM on top-k features; eval on test)\n",
        "    import csv\n",
        "    csv_path = os.path.join(outdir, \"summary_topk.csv\")\n",
        "    header = not os.path.exists(csv_path)\n",
        "    with open(csv_path, \"a\", newline=\"\") as f:\n",
        "        wr = csv.writer(f)\n",
        "        if header:\n",
        "            wr.writerow([\"dataset\",\"method\",\"k\",\"metric_name\",\"metric_value\"])\n",
        "        for m, v in scores.items():\n",
        "            ranks = rank_from_scores(v)\n",
        "            for k in ks:\n",
        "                keep = ranks[:k]\n",
        "                model_k = GradientBoostingClassifier(random_state=seed) if pack.task==\"clf\" \\\n",
        "                          else GradientBoostingRegressor(random_state=seed)\n",
        "                Xtr_k = np.vstack([pack.Xtr, pack.Xva])[:, keep]\n",
        "                ytr_k = np.r_[pack.ytr, pack.yva]\n",
        "\n",
        "                try:\n",
        "                    model_k.fit(Xtr_k, ytr_k)\n",
        "                    yhat = model_k.predict(pack.Xte[:, keep])\n",
        "                    metric_val = accuracy_score(pack.yte, yhat) if pack.task==\"clf\" else r2_score(pack.yte, yhat)\n",
        "                    wr.writerow([pack.name, m, k, \"accuracy\" if pack.task==\"clf\" else \"r2\", metric_val])\n",
        "                except RecursionError:\n",
        "                    print(f\"  - Skipping top-k sufficiency for {pack.name} k={k} due to RecursionError.\")\n",
        "                    wr.writerow([pack.name, m, k, \"accuracy\" if pack.task==\"clf\" else \"r2\", np.nan])\n",
        "\n",
        "\n",
        "    # 4) AOPC (ExCIR)\n",
        "    print(\"  - Plotting AOPC curves...\")\n",
        "    fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scores[\"ExCIR\"], steps=9)\n",
        "    plt.figure(figsize=(6.5,4.3))\n",
        "    plt.plot(100*fr, ins, marker='o', label='Insertion (keep top-% by ExCIR)')\n",
        "    plt.plot(100*fr, dele, marker='o', label='Deletion (zero top-% by ExCIR)')\n",
        "    plt.xlabel('Revealed / removed top-% features'); plt.ylabel('Test ' + ('Accuracy' if pack.task=='clf' else 'R$^2$'))\n",
        "    plt.title(f'AOPC (ExCIR) — {pack.name}')\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"aopc_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # 5) stability (CIR + Jaccard@8)\n",
        "    print(\"  - Evaluating stability under noise...\")\n",
        "    cirs, j8 = stability_noise_eval(lambda X, p: excir_feature_scores(X, p), pack.Xva, p_va, repeats=20, sigma=0.01)\n",
        "    plt.figure(figsize=(6.6,4.0))\n",
        "    plt.subplot(1,2,1); plt.hist(cirs, bins=10); plt.title('CIR under noise'); plt.xlabel('CIR'); plt.ylabel('count')\n",
        "    plt.subplot(1,2,2); plt.hist(j8, bins=10); plt.title('Jaccard@8 under noise'); plt.xlabel('overlap')\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(outdir, f\"stability_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # 6) lightweight three checks + plots\n",
        "    print(\"  - Running lightweight checks...\")\n",
        "    F, T, CIRs, Resid, KLs, J8 = lightweight_three_checks(pack, seed=seed)\n",
        "    plot_agreement_bundle(pack, F, CIRs, Resid, KLs, outdir=outdir)\n",
        "    plot_pareto(pack, T, CIRs, F, J8, outdir=outdir)\n",
        "    plot_runtime_vs_fraction(pack, F, T, outdir=outdir)\n",
        "\n",
        "    # 7) digits: class-conditioned ExCIR (class 3)\n",
        "    if pack.name == \"digits8x8\":\n",
        "        print(\"  - Running special checks for digits dataset...\")\n",
        "        clf = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs', random_state=seed)\n",
        "        clf.fit(np.vstack([pack.Xtr, pack.Xva]), np.r_[pack.ytr, pack.yva])\n",
        "        Pva = clf.predict_proba(pack.Xva)\n",
        "        c = 3\n",
        "        s_c = excir_feature_scores(pack.Xva, Pva[:, c])\n",
        "        H = s_c.reshape(8,8)\n",
        "        plt.figure(figsize=(4.5,4.5)); plt.imshow(H, cmap='viridis'); plt.colorbar()\n",
        "        plt.title('Class-conditioned ExCIR (digit 3)'); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_excir_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "        fr, ins, dele = aopc_curves(\"clf\", clf, pack.Xte, pack.yte, s_c, steps=9)\n",
        "        plt.figure(figsize=(6.2,4.2))\n",
        "        plt.plot(100*fr, ins, marker='o', label='Insertion (keep top-% pixels)')\n",
        "        plt.plot(100*fr, dele, marker='o', label='Deletion (zero top-% pixels)')\n",
        "        plt.xlabel('Revealed / removed top-% pixels'); plt.ylabel('Test Accuracy')\n",
        "        plt.title('AOPC — digits (class 3 map)'); plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_aopc_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "    return scores\n",
        "\n",
        "# -----------------------\n",
        "# CLI\n",
        "# -----------------------\n",
        "\n",
        "def _run_everything(datasets, seed, ks_tuple, outdir, do_pfi):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    registry = ALL_DATASETS\n",
        "    names = [s.strip() for s in datasets.split(\",\") if s.strip()]\n",
        "    for nm in names:\n",
        "        if nm not in registry:\n",
        "            print(f\"[skip] Unknown dataset: {nm}\")\n",
        "            continue\n",
        "        try:\n",
        "            pack = registry[nm](seed=seed)\n",
        "            print(f\"=== Dataset: {pack.name} (task={pack.task}) ===\")\n",
        "            run_on_dataset(pack, ks=ks_tuple, seed=seed, do_pfi=do_pfi, outdir=outdir)\n",
        "        except Exception as e:\n",
        "            print(f\"[{nm}] ERROR: {e}\")\n",
        "    print(f\"\\nDone. See '{outdir}/' for CSVs and figures.\")\n",
        "\n",
        "def main(argv=None):\n",
        "    import argparse\n",
        "    p = argparse.ArgumentParser(description=\"ExCIR (original) — cross-domain benchmarks\")\n",
        "    p.add_argument(\"--datasets\", type=str, default=\"adult,20ng_bin,digits8x8,har6,california,diabetes,iris,wine,synthetic_clf,synthetic_reg\",\n",
        "                   help=\"comma-separated list of datasets to run\")\n",
        "    p.add_argument(\"--seed\", type=int, default=0)\n",
        "    p.add_argument(\"--ks\", type=str, default=\"3,5,8,12\")\n",
        "    p.add_argument(\"--outdir\", type=str, default=\"out\")\n",
        "    p.add_argument(\"--no-pfi\", action=\"store_true\", help=\"disable PFI (speed)\")\n",
        "\n",
        "    if argv is None:\n",
        "        args, _ = p.parse_known_args()\n",
        "    else:\n",
        "        args = p.parse_args(argv)\n",
        "\n",
        "    ks_tuple = tuple(int(x) for x in args.ks.split(\",\"))\n",
        "    _run_everything(datasets=args.datasets,\n",
        "                    seed=args.seed,\n",
        "                    ks_tuple=ks_tuple,\n",
        "                    outdir=args.outdir,\n",
        "                    do_pfi=(not args.no_pfi))\n",
        "\n",
        "def run_bench(datasets=\"adult,20ng_bin,digits8x8,har6,california,diabetes,iris,wine,synthetic_clf,synthetic_reg\",\n",
        "              seed=0, ks=(3,5,8,12), outdir=\"out\", no_pfi=False):\n",
        "    _run_everything(datasets=datasets, seed=seed, ks_tuple=ks, outdir=outdir, do_pfi=(not no_pfi))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "IiMe8O1lyIjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Enhanced ExCIR with Additional Data Types and Datasets\n",
        "Original datasets + NEW: time series, audio features, molecular descriptors,\n",
        "image features (CNN), network/graph data, genomic data, financial time series,\n",
        "sensor fusion data, and more complex multimodal scenarios.\n",
        "\n",
        "NEW DATASETS (15 additional):\n",
        "  - ecg_heartbeat (time series, 1D signals)\n",
        "  - fashion_mnist_features (image features via CNN)\n",
        "  - protein_structure (molecular descriptors)\n",
        "  - stock_technical (financial time series features)\n",
        "  - sensor_fusion (multi-sensor IoT data)\n",
        "  - gene_expression (genomic data)\n",
        "  - network_topology (graph features)\n",
        "  - audio_mfcc (audio features)\n",
        "  - weather_station (meteorological time series)\n",
        "  - eeg_signals (brain signals)\n",
        "  - satellite_ndvi (remote sensing)\n",
        "  - industrial_process (process control)\n",
        "  - social_network (social media features)\n",
        "  - cyber_security (network intrusion)\n",
        "  - mixed_modal (combining multiple data types)\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, time, sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde, entropy\n",
        "from scipy.signal import periodogram\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance, partial_dependence\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from sklearn.datasets import (\n",
        "    fetch_openml, load_digits, load_iris, load_wine,\n",
        "    fetch_20newsgroups, fetch_california_housing, load_diabetes,\n",
        "    make_classification, make_regression\n",
        ")\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Set higher recursion limit\n",
        "sys.setrecursionlimit(3000)\n",
        "\n",
        "# -----------------------\n",
        "# Core ExCIR (same as original)\n",
        "# -----------------------\n",
        "\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    \"\"\"Original ExCIR between two 1-D signals z, s\"\"\"\n",
        "    z = np.asarray(z, dtype=float).ravel()\n",
        "    s = np.asarray(s, dtype=float).ravel()\n",
        "    assert z.shape[0] == s.shape[0], \"z and s must have the same length\"\n",
        "    mu_z, mu_s = z.mean(), s.mean()\n",
        "    m = 0.5 * (mu_z + mu_s)\n",
        "    num = z.shape[0] * ((mu_z - m) ** 2 + (mu_s - m) ** 2)\n",
        "    den = np.sum((z - m) ** 2) + np.sum((s - m) ** 2) + eps\n",
        "    return float(num / den)\n",
        "\n",
        "def excir_feature_scores(X: np.ndarray, p: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    \"\"\"Per-feature ExCIR using the original Z,S formula (vectorized)\"\"\"\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(p, dtype=float).ravel()\n",
        "    n, d = X.shape\n",
        "    assert p.shape[0] == n, \"X and p must align on rows\"\n",
        "\n",
        "    mu_p = p.mean()\n",
        "    mu_x = X.mean(axis=0)\n",
        "    m_j  = 0.5 * (mu_x + mu_p)\n",
        "\n",
        "    num = n * ((mu_x - m_j)**2 + (mu_p - m_j)**2)\n",
        "    den = np.sum((X - m_j)**2, axis=0) + np.sum((p[:, None] - m_j[None, :])**2, axis=0) + eps\n",
        "    return num / den\n",
        "\n",
        "def excir_block_scores(X: np.ndarray, p: np.ndarray, blocks: List[List[int]]) -> np.ndarray:\n",
        "    \"\"\"Block ExCIR: average z-scored features inside each block\"\"\"\n",
        "    X = np.asarray(X, dtype=float); p = np.asarray(p, dtype=float).ravel()\n",
        "    Z = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-12)\n",
        "    out = []\n",
        "    for idxs in blocks:\n",
        "        zb = Z[:, idxs].mean(axis=1)\n",
        "        out.append(cir_pair(zb, p))\n",
        "    return np.asarray(out)\n",
        "\n",
        "# -----------------------\n",
        "# Agreement metrics (same as original)\n",
        "# -----------------------\n",
        "\n",
        "def projection_alignment_residual(y_full: np.ndarray, y_light: np.ndarray) -> float:\n",
        "    \"\"\"Best affine map y_full ≈ a*y_light + b; normalized residual\"\"\"\n",
        "    y = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "    n = y.shape[0]\n",
        "    Phi = np.c_[yp, np.ones(n)]\n",
        "    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n",
        "    yhat = Phi @ theta\n",
        "    return float(np.linalg.norm(y - yhat) / (np.linalg.norm(y) + 1e-12))\n",
        "\n",
        "def kde_kl_symmetric(y_full: np.ndarray, y_light: np.ndarray, grid_points: int = 400) -> float:\n",
        "    \"\"\"Symmetric KL between 1-D KDEs\"\"\"\n",
        "    y = np.asarray(y_full).ravel()\n",
        "    yp = np.asarray(y_light).ravel()\n",
        "    kde_p = gaussian_kde(y)\n",
        "    kde_q = gaussian_kde(yp)\n",
        "    both = np.r_[y, yp]\n",
        "    lo, hi = np.percentile(both, 0.5), np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None)\n",
        "    q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo) / max(grid_points - 1, 1)\n",
        "    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))) * dx)\n",
        "    return 0.5 * (kl_pq + kl_qp)\n",
        "\n",
        "def jaccard_topk(a_scores: np.ndarray, b_scores: np.ndarray, k: int = 8) -> float:\n",
        "    A = set(np.argsort(-a_scores)[:k]); B = set(np.argsort(-b_scores)[:k])\n",
        "    return len(A & B) / (len(A | B) + 1e-12)\n",
        "\n",
        "# -----------------------\n",
        "# Baselines (same as original)\n",
        "# -----------------------\n",
        "\n",
        "def pfi_scores(model, X, y, n_repeats: int = 5, seed: int = 0) -> np.ndarray:\n",
        "    \"\"\"Permutation feature importance (reduced repeats for speed)\"\"\"\n",
        "    try:\n",
        "        return permutation_importance(model, X, y, n_repeats=n_repeats,\n",
        "                                      random_state=seed, scoring=None).importances_mean\n",
        "    except Exception:\n",
        "        return np.zeros(X.shape[1])\n",
        "\n",
        "def tree_gain_scores(model, d: int) -> np.ndarray:\n",
        "    try:\n",
        "        w = model.feature_importances_\n",
        "        if w is None or len(w) != d:\n",
        "            return np.zeros(d)\n",
        "        return w\n",
        "    except Exception:\n",
        "        return np.zeros(d)\n",
        "\n",
        "def pdp_var_scores(model, X, grid_points: int = 15, random_state: int = 0) -> np.ndarray:\n",
        "    d = X.shape[1]; scores = np.zeros(d); rs = check_random_state(random_state)\n",
        "    idx = rs.choice(np.arange(X.shape[0]), size=min(1000, X.shape[0]), replace=False)\n",
        "    Xs = X[idx]\n",
        "    for j in range(d):\n",
        "        try:\n",
        "            pd = partial_dependence(model, Xs, features=[j], kind='average', grid_resolution=grid_points)\n",
        "            scores[j] = np.var(pd.average[0])\n",
        "        except Exception:\n",
        "            scores[j] = 0.0\n",
        "    return scores\n",
        "\n",
        "def surrogate_lr_scores(X, teacher_scores, alpha: float = 1.0) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float); y = np.asarray(teacher_scores, dtype=float).ravel()\n",
        "    reg = Ridge(alpha=alpha, random_state=0)\n",
        "    reg.fit(X, y)\n",
        "    w = np.abs(reg.coef_)\n",
        "    if w.ndim > 1:\n",
        "        w = np.linalg.norm(w, axis=0)\n",
        "    s = w / (w.sum() + 1e-12)\n",
        "    return s\n",
        "\n",
        "def mi_pred_scores(X, preds, task: str) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = np.asarray(preds, dtype=float).ravel()\n",
        "    if task == \"clf\":\n",
        "        z = (p > np.median(p)).astype(int)\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "    else:\n",
        "        z = np.digitize(p, np.quantile(p, [1/3, 2/3]))\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "\n",
        "def mi_label_scores(X, y, task: str) -> np.ndarray:\n",
        "    if task == \"clf\":\n",
        "        return mutual_info_classif(X, y, random_state=0)\n",
        "    else:\n",
        "        return mutual_info_regression(X, y, random_state=0)\n",
        "\n",
        "# -----------------------\n",
        "# AOPC & stability (same as original)\n",
        "# -----------------------\n",
        "\n",
        "def rank_from_scores(scores: np.ndarray) -> np.ndarray:\n",
        "    return np.argsort(-scores)\n",
        "\n",
        "def aopc_curves(task: str, model, Xte, yte, scores: np.ndarray, steps: int = 9):\n",
        "    idx = rank_from_scores(scores)\n",
        "    n, d = Xte.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f * d))\n",
        "        keep = idx[:k]\n",
        "        mask = np.zeros(d, dtype=bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0)\n",
        "        X_del = np.where(mask, 0.0, Xte)\n",
        "        try:\n",
        "            y_ins = model.predict(X_ins); y_del = model.predict(X_del)\n",
        "            if task == \"clf\":\n",
        "                ins.append(accuracy_score(yte, y_ins))\n",
        "                dele.append(accuracy_score(yte, y_del))\n",
        "            else:\n",
        "                ins.append(r2_score(yte, y_ins))\n",
        "                dele.append(r2_score(yte, y_del))\n",
        "        except Exception:\n",
        "            ins.append(np.nan); dele.append(np.nan)\n",
        "    return fracs, np.asarray(ins), np.asarray(dele)\n",
        "\n",
        "def stability_noise_eval(scores_fn, Xva, p_va, repeats: int = 15, sigma: float = 0.01):\n",
        "    base = scores_fn(Xva, p_va)\n",
        "    cirs, j8 = [], []\n",
        "    for _ in range(repeats):\n",
        "        noise = np.random.normal(0, sigma, size=Xva.shape)\n",
        "        s_noisy = scores_fn(Xva + noise, p_va)\n",
        "        cirs.append(cir_pair(base, s_noisy))\n",
        "        j8.append(jaccard_topk(base, s_noisy, k=min(8, Xva.shape[1])))\n",
        "    return np.asarray(cirs), np.asarray(j8)\n",
        "\n",
        "# -----------------------\n",
        "# Lightweight checks + plots (same logic, optimized)\n",
        "# -----------------------\n",
        "\n",
        "def train_gboost(task: str, Xtr, ytr, Xva, yva, seed: int = 0):\n",
        "    Xall = np.vstack([Xtr, Xva]); yall = np.r_[ytr, yva]\n",
        "    if task == \"clf\":\n",
        "        m = GradientBoostingClassifier(random_state=seed, n_estimators=50)  # Reduced for speed\n",
        "    else:\n",
        "        m = GradientBoostingRegressor(random_state=seed, n_estimators=50)\n",
        "    try:\n",
        "        m.fit(Xall, yall)\n",
        "    except (RecursionError, Exception) as e:\n",
        "        print(f\"  - skipping GBM training due to {type(e).__name__}\")\n",
        "        return None\n",
        "    return m\n",
        "\n",
        "def predict_scores(task: str, model, X) -> np.ndarray:\n",
        "    if model is None: return np.zeros(X.shape[0])\n",
        "    try:\n",
        "        if task == \"clf\":\n",
        "            if hasattr(model, \"predict_proba\"):\n",
        "                proba = model.predict_proba(X)\n",
        "                return proba[:, -1] if proba.shape[1] > 1 else proba[:, 0]\n",
        "            s = model.decision_function(X)\n",
        "            return 1.0/(1.0+np.exp(-s))\n",
        "        return model.predict(X)\n",
        "    except Exception:\n",
        "        return np.zeros(X.shape[0])\n",
        "\n",
        "def lightweight_three_checks(pack, seed: int = 0, fractions=(0.2,0.4,0.6,0.8,1.0)):\n",
        "    rng = check_random_state(seed)\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    if base is None:\n",
        "        return (np.asarray(fractions), np.full(len(fractions), np.nan),\n",
        "                np.full(len(fractions), np.nan), np.full(len(fractions), np.nan),\n",
        "                np.full(len(fractions), np.nan), np.full(len(fractions), np.nan))\n",
        "\n",
        "    p_full = predict_scores(pack.task, base, pack.Xva)\n",
        "    s_full = excir_feature_scores(pack.Xva, p_full)\n",
        "\n",
        "    Xbig = np.vstack([pack.Xtr, pack.Xva])\n",
        "    ybig = np.r_[pack.ytr, pack.yva]\n",
        "\n",
        "    F, Times, CIRs, Resid, KLs, J8 = [], [], [], [], [], []\n",
        "    for f in fractions:\n",
        "        t0 = time.time()\n",
        "        n_rows = int(f * Xbig.shape[0])\n",
        "        idx = rng.choice(np.arange(Xbig.shape[0]), size=n_rows, replace=False)\n",
        "        m = GradientBoostingClassifier(random_state=seed, n_estimators=30) if pack.task == \"clf\" else GradientBoostingRegressor(random_state=seed, n_estimators=30)\n",
        "        try:\n",
        "            m.fit(Xbig[idx], ybig[idx])\n",
        "            p_sub = predict_scores(pack.task, m, pack.Xva)\n",
        "            s_sub = excir_feature_scores(pack.Xva, p_sub)\n",
        "            CIRs.append(cir_pair(s_full, s_sub))\n",
        "            Resid.append(projection_alignment_residual(p_full, p_sub))\n",
        "            KLs.append(kde_kl_symmetric(p_full, p_sub))\n",
        "            J8.append(jaccard_topk(s_full, s_sub, k=min(8, s_full.shape[0])))\n",
        "        except Exception as e:\n",
        "            print(f\"  - skipping check for f={f} due to {type(e).__name__}\")\n",
        "            CIRs.append(np.nan); Resid.append(np.nan); KLs.append(np.nan); J8.append(np.nan)\n",
        "\n",
        "        Times.append(time.time() - t0)\n",
        "        F.append(f)\n",
        "\n",
        "    return (np.asarray(F), np.asarray(Times), np.asarray(CIRs),\n",
        "            np.asarray(Resid), np.asarray(KLs), np.asarray(J8))\n",
        "\n",
        "def plot_agreement_bundle(pack, F, CIRs, Resid, KLs, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(10, 3.8))\n",
        "\n",
        "    ax1 = plt.subplot(1,3,1)\n",
        "    ax1.plot((100*F).astype(int), CIRs, marker='o', linewidth=2, markersize=6)\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.set_xlabel('% rows kept'); ax1.set_ylabel('CIR(full, light)')\n",
        "    ax1.set_title('Agreement (CIR)')\n",
        "    ax1.grid(alpha=0.3)\n",
        "\n",
        "    ax2 = plt.subplot(1,3,2)\n",
        "    ax2.plot((100*F).astype(int), Resid, marker='o', linewidth=2, markersize=6)\n",
        "    ax2.set_xlabel('% rows kept'); ax2.set_ylabel('Projection residual')\n",
        "    ax2.set_title('Shape agreement')\n",
        "    ax2.grid(alpha=0.3)\n",
        "\n",
        "    ax3 = plt.subplot(1,3,3)\n",
        "    ax3.plot((100*F).astype(int), KLs, marker='o', linewidth=2, markersize=6)\n",
        "    ax3.set_xlabel('% rows kept'); ax3.set_ylabel('Sym. KL (KDE)')\n",
        "    ax3.set_title('Distribution match')\n",
        "    ax3.grid(alpha=0.3)\n",
        "\n",
        "    plt.suptitle(f'Agreement beyond ranks — {pack.name}', y=1.02, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"agreement_bundle_{pack.name}.png\"), dpi=160, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_pareto(pack, Times, CIRs, F, J8, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(6.5, 5.0))\n",
        "    scatter = plt.scatter(Times, CIRs, s=300*np.maximum(J8, 0.05), alpha=0.7, c=J8, cmap='viridis')\n",
        "    for i, f in enumerate(F):\n",
        "        plt.annotate(f\"{int(100*f)}%\", (Times[i], CIRs[i]), xytext=(5, 5),\n",
        "                    textcoords='offset points', fontsize=9)\n",
        "    plt.xlabel('Wall time (s)'); plt.ylabel('CIR(full, light)')\n",
        "    plt.title(f'Agreement–cost (ExCIR) — {pack.name}')\n",
        "    plt.colorbar(scatter, label='Jaccard@8')\n",
        "    plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"pareto_{pack.name}.png\"), dpi=160, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_runtime_vs_fraction(pack, F, Times, outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(6.2,4.2))\n",
        "    plt.plot((100*F).astype(int), Times, marker='o', linewidth=2, markersize=6)\n",
        "    plt.xlabel('Fraction of rows kept (%)'); plt.ylabel('Wall time (s)')\n",
        "    plt.title(f'Runtime vs fraction — {pack.name}')\n",
        "    plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"runtime_{pack.name}.png\"), dpi=160, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# -----------------------\n",
        "# Dataset structure (same as original)\n",
        "# -----------------------\n",
        "\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str\n",
        "    task: str  # \"clf\" | \"reg\"\n",
        "    Xtr: np.ndarray\n",
        "    ytr: np.ndarray\n",
        "    Xva: np.ndarray\n",
        "    yva: np.ndarray\n",
        "    Xte: np.ndarray\n",
        "    yte: np.ndarray\n",
        "    feature_names: List[str]\n",
        "\n",
        "def _standardize_fit(Xtr, Xva, Xte):\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    return ss.fit_transform(Xtr), ss.transform(Xva), ss.transform(Xte)\n",
        "\n",
        "def _split_xy(X, y, task: str, seed: int = 0) -> DatasetPack:\n",
        "    \"\"\"Helper: do 60/20/20 split and standardize; infer feature names if absent.\"\"\"\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2,\n",
        "                                          stratify=y if task==\"clf\" else None,\n",
        "                                          random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.25,\n",
        "                                          stratify=ytr if task==\"clf\" else None,\n",
        "                                          random_state=seed)\n",
        "    Xtr_s, Xva_s, Xte_s = _standardize_fit(Xtr, Xva, Xte)\n",
        "    feat_names = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "    return Xtr_s, ytr, Xva_s, yva, Xte_s, yte, feat_names\n",
        "\n",
        "# -----------------------\n",
        "# ORIGINAL DATASETS (with fallbacks)\n",
        "# -----------------------\n",
        "\n",
        "def load_adult(seed=0) -> DatasetPack:\n",
        "    try:\n",
        "        ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "        dfX = ds.data.select_dtypes(include=[np.number]).copy()\n",
        "        y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "        X = dfX.to_numpy(dtype=float)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"adult\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, list(dfX.columns))\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_20ng_binary(seed=0) -> DatasetPack:\n",
        "    try:\n",
        "        cats = ['comp.graphics', 'sci.space']\n",
        "        tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "        te = fetch_20newsgroups(subset='test', categories=cats, remove=('headers','footers','quotes'))\n",
        "        tfidf = TfidfVectorizer(max_features=3000, ngram_range=(1,2), stop_words='english')\n",
        "        Xtr_full = tfidf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "        Xte = tfidf.transform(te.data).astype(np.float32).toarray()\n",
        "        ytr_full, yte = tr.target, te.target\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2, stratify=ytr_full, random_state=seed)\n",
        "        return DatasetPack(\"20ng_bin\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"tfidf_{i}\" for i in range(Xtr.shape[1])])\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_har(seed=0) -> DatasetPack:\n",
        "    try:\n",
        "        ds = fetch_openml(\"HAR\", version=1, as_frame=True)\n",
        "        X = ds.data.to_numpy(dtype=float)\n",
        "        y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"har6\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"sensor_{i}\" for i in range(X.shape[1])])\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_digits8x8(seed=0) -> DatasetPack:\n",
        "    digits = load_digits()\n",
        "    X = digits.data.astype(float) / 16.0\n",
        "    y = digits.target\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"digits8x8\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, [f\"pix{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_california(seed=0) -> DatasetPack:\n",
        "    try:\n",
        "        ds = fetch_california_housing()\n",
        "        X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"california\", \"reg\", Xtr, ytr, Xva, yva, Xte, yte, list(ds.feature_names))\n",
        "    except Exception:\n",
        "        return load_synthetic_reg(seed)\n",
        "\n",
        "def load_diabetes(seed=0) -> DatasetPack:\n",
        "    ds = load_diabetes()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    feat_names = list(ds.feature_names) if hasattr(ds, \"feature_names\") else [f\"f{i}\" for i in range(X.shape[1])]\n",
        "    return DatasetPack(\"diabetes\", \"reg\", Xtr, ytr, Xva, yva, Xte, yte, feat_names)\n",
        "\n",
        "def load_iris(seed=0) -> DatasetPack:\n",
        "    ds = load_iris()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"iris\", \"clf\", Xtr, ytr, Xva, yva, Xte, yte, list(ds.feature_names))\n",
        "\n",
        "def load_wine(seed=0) -> DatasetPack:\n",
        "    ds = load_wine()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, *train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)[::2])\n",
        "    # Re-split properly\n",
        "    Xtr2, Xva2, ytr2, yva2 = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr_s, Xva_s, Xte_s = _standardize_fit(Xtr2, Xva2, Xte)\n",
        "    return DatasetPack(\"wine\", \"clf\", Xtr_s, ytr2, Xva_s, yva2, Xte_s, yte, list(ds.feature_names))\n",
        "\n",
        "def load_synthetic_clf(seed=0) -> DatasetPack:\n",
        "    X, y = make_classification(\n",
        "        n_samples=3000, n_features=40, n_informative=10, n_redundant=10, n_repeated=5,\n",
        "        n_classes=2, random_state=seed\n",
        "    )\n",
        "    Xtr, Xva, Xte, ytr, yva, yte = None, None, None, None, None, None\n",
        "    Xtr_s, ytr, Xva_s, yva, Xte_s, yte, feat_names = _split_xy(X, y, \"clf\", seed)\n",
        "    return DatasetPack(\"synthetic_clf\", \"clf\", Xtr_s, ytr, Xva_s, yva, Xte_s, yte, [f\"synth_clf_{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_synthetic_reg(seed=0) -> DatasetPack:\n",
        "    X, y = make_regression(\n",
        "        n_samples=3000, n_features=40, n_informative=10,\n",
        "        n_targets=1, random_state=seed\n",
        "    )\n",
        "    Xtr_s, ytr, Xva_s, yva, Xte_s, yte, feat_names = _split_xy(X, y, \"reg\", seed)\n",
        "    return DatasetPack(\"synthetic_reg\", \"reg\", Xtr_s, ytr, Xva_s, yva, Xte_s, yte, [f\"synth_reg_{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "# -----------------------\n",
        "# Extra synthetic helpers to mimic modalities\n",
        "# -----------------------\n",
        "\n",
        "def _dominant_freq(x, fs=1.0):\n",
        "    f, Pxx = periodogram(x, fs=fs)\n",
        "    if len(Pxx) == 0: return 0.0\n",
        "    return f[np.argmax(Pxx)] if np.isfinite(Pxx).any() else 0.0\n",
        "\n",
        "def _spec_entropy(x, fs=1.0):\n",
        "    f, Pxx = periodogram(x, fs=fs)\n",
        "    p = Pxx + 1e-12\n",
        "    p /= p.sum()\n",
        "    return float(entropy(p))\n",
        "\n",
        "def _ts_to_feats(T, fs=1.0):\n",
        "    \"\"\"Compute simple 1D time-series features for a single channel array T.\"\"\"\n",
        "    T = np.asarray(T, float)\n",
        "    return np.array([\n",
        "        T.mean(), T.std(), T.min(), T.max(),\n",
        "        _dominant_freq(T, fs=fs),\n",
        "        _spec_entropy(T, fs=fs),\n",
        "        np.mean(np.diff(T)), np.std(np.diff(T))\n",
        "    ], dtype=float)\n",
        "\n",
        "def _gen_ts_dataset(n_samples=2000, length=256, n_channels=1, seed=0, task=\"clf\"):\n",
        "    rs = check_random_state(seed)\n",
        "    fs = 1.0\n",
        "    X_feats = []\n",
        "    y = []\n",
        "    for i in range(n_samples):\n",
        "        label = rs.randint(0,2) if task==\"clf\" else None\n",
        "        chans = []\n",
        "        for c in range(n_channels):\n",
        "            freq = 0.05 + 0.2*rs.rand() + (0.1 if (task==\"clf\" and label==1) else 0.0)\n",
        "            t = np.arange(length)/fs\n",
        "            sig = np.sin(2*np.pi*freq*t) + 0.5*np.sin(2*np.pi*(freq*2)*t + rs.rand()*2*np.pi)\n",
        "            sig += 0.3*rs.randn(length)\n",
        "            chans.append(sig)\n",
        "        chans = np.stack(chans, axis=0)\n",
        "        feats = np.concatenate([_ts_to_feats(ch, fs) for ch in chans], axis=0)\n",
        "        X_feats.append(feats)\n",
        "        if task==\"clf\": y.append(label)\n",
        "        else: y.append(feats.sum() + rs.randn()*0.5)\n",
        "    X = np.vstack(X_feats)\n",
        "    y = np.array(y, dtype=int if task==\"clf\" else float)\n",
        "    return X, y\n",
        "\n",
        "def _split_pack_from_xy(X, y, name: str, task: str, seed: int) -> DatasetPack:\n",
        "    Xtr_s, ytr, Xva_s, yva, Xte_s, yte, feat_names = _split_xy(X, y, task, seed)\n",
        "    return DatasetPack(name, task, Xtr_s, ytr, Xva_s, yva, Xte_s, yte,\n",
        "                       [f\"{name}_f{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "# -----------------------\n",
        "# NEW DATASETS (15 loaders)\n",
        "# -----------------------\n",
        "\n",
        "def load_ecg_heartbeat(seed=0) -> DatasetPack:\n",
        "    X, y = _gen_ts_dataset(n_samples=2500, length=256, n_channels=1, seed=seed, task=\"clf\")\n",
        "    return _split_pack_from_xy(X, y, \"ecg_heartbeat\", \"clf\", seed)\n",
        "\n",
        "def load_fashion_mnist_features(seed=0) -> DatasetPack:\n",
        "    # Mock CNN features: informative low-dim embedding + noise\n",
        "    rs = check_random_state(seed)\n",
        "    n = 5000; d = 128\n",
        "    y = rs.randint(0, 10, size=n)\n",
        "    Z = rs.randn(n, 16) + (y[:,None] * 0.15)\n",
        "    W = rs.randn(16, d)\n",
        "    X = Z @ W + 0.1*rs.randn(n, d)\n",
        "    return _split_pack_from_xy(X, y, \"fashion_mnist_features\", \"clf\", seed)\n",
        "\n",
        "def load_protein_structure(seed=0) -> DatasetPack:\n",
        "    # Molecular descriptors-ish (regression): predict stability score\n",
        "    X, y = make_regression(n_samples=4000, n_features=120, n_informative=20, noise=2.0, random_state=seed)\n",
        "    return _split_pack_from_xy(X, y, \"protein_structure\", \"reg\", seed)\n",
        "\n",
        "def load_stock_technical(seed=0) -> DatasetPack:\n",
        "    # Build random walks; features: returns, rolling vol, momentum; target: next-return>0\n",
        "    rs = check_random_state(seed)\n",
        "    n = 4000; T = 60\n",
        "    prices = 100 + np.cumsum(rs.randn(n, T), axis=1)\n",
        "    rets = np.diff(prices, axis=1)\n",
        "    vol = rets.std(axis=1)\n",
        "    mom = prices[:,-1] - prices[:,0]\n",
        "    skew = ((rets - rets.mean(axis=1, keepdims=True))**3).mean(axis=1)\n",
        "    kurt = ((rets - rets.mean(axis=1, keepdims=True))**4).mean(axis=1)\n",
        "    feats = np.c_[rets.mean(axis=1), vol, mom, skew, kurt]\n",
        "    y = (rs.randn(n) + 0.2*np.sign(mom) > 0).astype(int)\n",
        "    return _split_pack_from_xy(feats, y, \"stock_technical\", \"clf\", seed)\n",
        "\n",
        "def load_sensor_fusion(seed=0) -> DatasetPack:\n",
        "    # 3 sensors (IMU-ish): combine per-sensor stats\n",
        "    X, y = _gen_ts_dataset(n_samples=3000, length=128, n_channels=3, seed=seed, task=\"clf\")\n",
        "    return _split_pack_from_xy(X, y, \"sensor_fusion\", \"clf\", seed)\n",
        "\n",
        "def load_gene_expression(seed=0) -> DatasetPack:\n",
        "    # p >> n: 2000 genes, 800 samples, 2 classes\n",
        "    rs = check_random_state(seed)\n",
        "    n=800; d=2000\n",
        "    y = rs.randint(0,2,size=n)\n",
        "    base = rs.randn(n, d)*0.5\n",
        "    signal = np.zeros((n,d))\n",
        "    idx_inf = np.r_[rs.choice(d, 30, replace=False)]\n",
        "    signal[y==1][:, idx_inf] = 1.5\n",
        "    X = base + signal + 0.1*rs.randn(n,d)\n",
        "    return _split_pack_from_xy(X, y, \"gene_expression\", \"clf\", seed)\n",
        "\n",
        "def load_network_topology(seed=0) -> DatasetPack:\n",
        "    # Graph-ish handcrafted features\n",
        "    rs = check_random_state(seed)\n",
        "    n=3000\n",
        "    deg_mean = rs.gamma(2.0, 2.0, size=n)\n",
        "    clustering = rs.beta(2,5,size=n)\n",
        "    assort = rs.uniform(-0.5,0.5,size=n)\n",
        "    triads = rs.poisson(5,size=n)\n",
        "    betw = rs.exponential(1.0,size=n)\n",
        "    X = np.c_[deg_mean, clustering, assort, triads, betw]\n",
        "    y = (deg_mean*0.3 + clustering*0.8 - 0.5*assort + 0.1*triads + 0.2*betw + rs.randn(n)*0.5 > 1.8).astype(int)\n",
        "    return _split_pack_from_xy(X, y, \"network_topology\", \"clf\", seed)\n",
        "\n",
        "def load_audio_mfcc(seed=0) -> DatasetPack:\n",
        "    # MFCC-like summary features (but we keep simple spectral stats per \"band\")\n",
        "    rs = check_random_state(seed)\n",
        "    n=3500; bands=20\n",
        "    y = rs.randint(0, 5, size=n)\n",
        "    base = rs.randn(n, bands)\n",
        "    for c in range(5):\n",
        "        base[y==c] += (c-2)*0.25\n",
        "    X = base + 0.1*rs.randn(n,bands)\n",
        "    return _split_pack_from_xy(X, y, \"audio_mfcc\", \"clf\", seed)\n",
        "\n",
        "def load_weather_station(seed=0) -> DatasetPack:\n",
        "    # Regression: temp as function of pressure, humidity, wind, cloudiness, seasonality\n",
        "    rs = check_random_state(seed)\n",
        "    n=5000\n",
        "    pressure = rs.normal(1013, 8, size=n)\n",
        "    humidity = rs.uniform(15, 95, size=n)\n",
        "    wind = rs.gamma(2.0, 1.2, size=n)\n",
        "    cloud = rs.uniform(0, 1, size=n)\n",
        "    doy = rs.randint(1, 366, size=n)\n",
        "    temp = 10 + 10*np.sin(2*np.pi*doy/365) - 0.01*(pressure-1013) - 0.05*(humidity-50) - 0.3*cloud + 0.2*wind + rs.randn(n)\n",
        "    X = np.c_[pressure, humidity, wind, cloud, doy]\n",
        "    y = temp\n",
        "    return _split_pack_from_xy(X, y, \"weather_station\", \"reg\", seed)\n",
        "\n",
        "def load_eeg_signals(seed=0) -> DatasetPack:\n",
        "    X, y = _gen_ts_dataset(n_samples=3000, length=256, n_channels=4, seed=seed, task=\"clf\")\n",
        "    return _split_pack_from_xy(X, y, \"eeg_signals\", \"clf\", seed)\n",
        "\n",
        "def load_satellite_ndvi(seed=0) -> DatasetPack:\n",
        "    # Regression: NDVI from bands + angles\n",
        "    rs = check_random_state(seed)\n",
        "    n=4000\n",
        "    bands = rs.uniform(0, 1, size=(n, 6))  # Blue, Green, Red, NIR, SWIR1, SWIR2\n",
        "    sun_zen = rs.uniform(0, 70, size=n)\n",
        "    view_zen = rs.uniform(0, 30, size=n)\n",
        "    ndvi = (bands[:,3] - bands[:,2]) / (bands[:,3] + bands[:,2] + 1e-6)  # (NIR - Red)/(NIR+Red)\n",
        "    y = ndvi + 0.001*(sun_zen - 35) - 0.001*(view_zen - 15) + rs.randn(n)*0.02\n",
        "    X = np.c_[bands, sun_zen, view_zen]\n",
        "    return _split_pack_from_xy(X, y, \"satellite_ndvi\", \"reg\", seed)\n",
        "\n",
        "def load_industrial_process(seed=0) -> DatasetPack:\n",
        "    # Regression: process output from sensors and setpoints\n",
        "    rs = check_random_state(seed)\n",
        "    n=4500\n",
        "    s1 = rs.normal(0,1,size=n); s2 = rs.normal(0,1.2,size=n); s3 = rs.normal(0.5,0.8,size=n)\n",
        "    sp = rs.uniform(-1,1,size=n)\n",
        "    noise = rs.randn(n)*0.3\n",
        "    y = 2.0 + 1.5*s1 - 0.7*s2 + 0.9*s3 + 1.2*sp + noise\n",
        "    X = np.c_[s1,s2,s3,sp]\n",
        "    return _split_pack_from_xy(X, y, \"industrial_process\", \"reg\", seed)\n",
        "\n",
        "def load_social_network(seed=0) -> DatasetPack:\n",
        "    # Classification: engagement class from simple platform stats\n",
        "    rs = check_random_state(seed)\n",
        "    n=5000\n",
        "    posts = rs.poisson(3, size=n)\n",
        "    friends = rs.poisson(120, size=n)\n",
        "    likes = rs.poisson(50, size=n)\n",
        "    comments = rs.poisson(12, size=n)\n",
        "    shares = rs.poisson(5, size=n)\n",
        "    X = np.c_[posts, friends, likes, comments, shares]\n",
        "    y = (0.01*friends + 0.03*likes + 0.05*comments + 0.08*shares + 0.2*posts + rs.randn(n)*0.5 > 3.0).astype(int)\n",
        "    return _split_pack_from_xy(X, y, \"social_network\", \"clf\", seed)\n",
        "\n",
        "def load_cyber_security(seed=0) -> DatasetPack:\n",
        "    # Classification: intrusion vs normal with skew\n",
        "    X, y = make_classification(n_samples=5000, n_features=30, n_informative=8,\n",
        "                               n_redundant=10, weights=[0.8, 0.2],\n",
        "                               class_sep=1.5, random_state=seed)\n",
        "    return _split_pack_from_xy(X, y, \"cyber_security\", \"clf\", seed)\n",
        "\n",
        "def load_mixed_modal(seed=0) -> DatasetPack:\n",
        "    # Combine tabular + small spectral features\n",
        "    rs = check_random_state(seed)\n",
        "    X_tab, y = make_classification(n_samples=4000, n_features=20, n_informative=6,\n",
        "                                   n_redundant=6, random_state=seed)\n",
        "    # 1 synthetic channel (length 64) -> 8 feats\n",
        "    X_spec = []\n",
        "    for i in range(X_tab.shape[0]):\n",
        "        length=64\n",
        "        t = np.arange(length)\n",
        "        sig = np.sin(2*np.pi*(0.08+0.1*rs.rand())*t) + 0.3*rs.randn(length)\n",
        "        X_spec.append(_ts_to_feats(sig))\n",
        "    X_spec = np.vstack(X_spec)\n",
        "    X = np.c_[X_tab, X_spec]\n",
        "    return _split_pack_from_xy(X, y, \"mixed_modal\", \"clf\", seed)\n",
        "\n",
        "# -----------------------\n",
        "# Registry\n",
        "# -----------------------\n",
        "\n",
        "ALL_DATASETS: Dict[str, callable] = {\n",
        "    # original\n",
        "    \"adult\": load_adult,\n",
        "    \"20ng_bin\": load_20ng_binary,\n",
        "    \"har6\": load_har,\n",
        "    \"digits8x8\": load_digits8x8,\n",
        "    \"california\": load_california,\n",
        "    \"diabetes\": load_diabetes,\n",
        "    \"iris\": load_iris,\n",
        "    \"wine\": load_wine,\n",
        "    \"synthetic_clf\": load_synthetic_clf,\n",
        "    \"synthetic_reg\": load_synthetic_reg,\n",
        "    # new\n",
        "    \"ecg_heartbeat\": load_ecg_heartbeat,\n",
        "    \"fashion_mnist_features\": load_fashion_mnist_features,\n",
        "    \"protein_structure\": load_protein_structure,\n",
        "    \"stock_technical\": load_stock_technical,\n",
        "    \"sensor_fusion\": load_sensor_fusion,\n",
        "    \"gene_expression\": load_gene_expression,\n",
        "    \"network_topology\": load_network_topology,\n",
        "    \"audio_mfcc\": load_audio_mfcc,\n",
        "    \"weather_station\": load_weather_station,\n",
        "    \"eeg_signals\": load_eeg_signals,\n",
        "    \"satellite_ndvi\": load_satellite_ndvi,\n",
        "    \"industrial_process\": load_industrial_process,\n",
        "    \"social_network\": load_social_network,\n",
        "    \"cyber_security\": load_cyber_security,\n",
        "    \"mixed_modal\": load_mixed_modal,\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# End-to-end per dataset\n",
        "# -----------------------\n",
        "\n",
        "def ensure_out(outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "def run_on_dataset(pack: DatasetPack, ks=(3,5,8,12), seed: int = 0,\n",
        "                   do_pfi: bool = True, outdir=\"out\") -> Dict[str, np.ndarray]:\n",
        "    ensure_out(outdir)\n",
        "\n",
        "    print(\"  - Training base model...\")\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    if base is None:\n",
        "        print(\"  - Skipping feature importance benchmarks due to model training failure.\")\n",
        "        return {}\n",
        "\n",
        "    p_va = predict_scores(pack.task, base, pack.Xva)\n",
        "\n",
        "    print(\"  - Calculating feature importance scores...\")\n",
        "    scores: Dict[str, np.ndarray] = {}\n",
        "    scores[\"ExCIR\"] = excir_feature_scores(pack.Xva, p_va)\n",
        "\n",
        "    if do_pfi:\n",
        "        try:\n",
        "            scores[\"PFI\"] = pfi_scores(base, pack.Xva, pack.yva, n_repeats=5, seed=seed)\n",
        "        except Exception:\n",
        "            scores[\"PFI\"] = np.zeros(pack.Xva.shape[1])\n",
        "    scores[\"TreeGain\"]  = tree_gain_scores(base, pack.Xva.shape[1])\n",
        "    scores[\"PDP-var\"]   = pdp_var_scores(base, pack.Xva)\n",
        "    scores[\"MI(pred)\"]  = mi_pred_scores(pack.Xva, p_va, task=pack.task)\n",
        "    scores[\"MI(label)\"] = mi_label_scores(pack.Xva, pack.yva, task=pack.task)\n",
        "    scores[\"Surrogate-LR\"] = surrogate_lr_scores(pack.Xva, p_va)\n",
        "\n",
        "    # --- Add this block inside run_on_dataset(), right after `scores` dict is created ---\n",
        "\n",
        "# 2) AOPC (all methods): compute insertion/deletion AUCs and save a compact CSV\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "aopc_csv = os.path.join(outdir, \"aopc_summary.csv\")\n",
        "write_header = not os.path.exists(aopc_csv)\n",
        "with open(aopc_csv, \"a\", newline=\"\") as fcsv:\n",
        "    wr = csv.writer(fcsv)\n",
        "    if write_header:\n",
        "        wr.writerow([\"dataset\", \"method\", \"auc_insertion\", \"auc_deletion\", \"combined\"])\n",
        "    for mname, scr in scores.items():\n",
        "        fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scr, steps=9)\n",
        "        # Trapezoidal AUCs on p \\in [0,1]\n",
        "        auc_ins = float(np.trapz(ins, fr))\n",
        "        auc_del = float(np.trapz(dele, fr))\n",
        "        # Combined summary in [0,1]: average of insertion and (1 - deletion)\n",
        "        combined = 0.5 * (auc_ins + (1.0 - auc_del))\n",
        "        wr.writerow([pack.name, mname, auc_ins, auc_del, combined])\n",
        "\n",
        "# (Optional) keep your per-dataset ExCIR AOPC plot if you like:\n",
        "# fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scores[\"ExCIR\"], steps=9)\n",
        "# ... (your existing plotting code) ...\n",
        "\n",
        "\n",
        "    # 3) top-k sufficiency -> CSV\n",
        "    import csv\n",
        "    csv_path = os.path.join(outdir, \"summary_topk.csv\")\n",
        "    header = not os.path.exists(csv_path)\n",
        "    with open(csv_path, \"a\", newline=\"\") as f:\n",
        "        wr = csv.writer(f)\n",
        "        if header:\n",
        "            wr.writerow([\"dataset\",\"method\",\"k\",\"metric_name\",\"metric_value\"])\n",
        "        for m, v in scores.items():\n",
        "            ranks = rank_from_scores(v)\n",
        "            for k in ks:\n",
        "                keep = ranks[:k]\n",
        "                model_k = GradientBoostingClassifier(random_state=seed) if pack.task==\"clf\" \\\n",
        "                          else GradientBoostingRegressor(random_state=seed)\n",
        "                Xtr_k = np.vstack([pack.Xtr, pack.Xva])[:, keep]\n",
        "                ytr_k = np.r_[pack.ytr, pack.yva]\n",
        "\n",
        "                try:\n",
        "                    model_k.fit(Xtr_k, ytr_k)\n",
        "                    yhat = model_k.predict(pack.Xte[:, keep])\n",
        "                    metric_val = accuracy_score(pack.yte, yhat) if pack.task==\"clf\" else r2_score(pack.yte, yhat)\n",
        "                    wr.writerow([pack.name, m, k, \"accuracy\" if pack.task==\"clf\" else \"r2\", metric_val])\n",
        "                except RecursionError:\n",
        "                    print(f\"  - Skipping top-k sufficiency for {pack.name} k={k} due to RecursionError.\")\n",
        "                    wr.writerow([pack.name, m, k, \"accuracy\" if pack.task==\"clf\" else \"r2\", np.nan])\n",
        "\n",
        "    # 4) AOPC (ExCIR)\n",
        "    print(\"  - Plotting AOPC curves...\")\n",
        "    fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scores[\"ExCIR\"], steps=9)\n",
        "    plt.figure(figsize=(6.5,4.3))\n",
        "    plt.plot(100*fr, ins, marker='o', label='Insertion (keep top-% by ExCIR)')\n",
        "    plt.plot(100*fr, dele, marker='o', label='Deletion (zero top-% by ExCIR)')\n",
        "    plt.xlabel('Revealed / removed top-% features'); plt.ylabel('Test ' + ('Accuracy' if pack.task=='clf' else 'R$^2$'))\n",
        "    plt.title(f'AOPC (ExCIR) — {pack.name}')\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"aopc_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # 5) stability (CIR + Jaccard@8)\n",
        "    print(\"  - Evaluating stability under noise...\")\n",
        "    cirs, j8 = stability_noise_eval(lambda X, p: excir_feature_scores(X, p), pack.Xva, p_va, repeats=15, sigma=0.01)\n",
        "    plt.figure(figsize=(6.6,4.0))\n",
        "    plt.subplot(1,2,1); plt.hist(cirs, bins=10); plt.title('CIR under noise'); plt.xlabel('CIR'); plt.ylabel('count')\n",
        "    plt.subplot(1,2,2); plt.hist(j8, bins=10); plt.title('Jaccard@8 under noise'); plt.xlabel('overlap')\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(outdir, f\"stability_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # 6) lightweight three checks + plots\n",
        "    print(\"  - Running lightweight checks...\")\n",
        "    F, T, CIRs, Resid, KLs, J8 = lightweight_three_checks(pack, seed=seed)\n",
        "    plot_agreement_bundle(pack, F, CIRs, Resid, KLs, outdir=outdir)\n",
        "    plot_pareto(pack, T, CIRs, F, J8, outdir=outdir)\n",
        "    plot_runtime_vs_fraction(pack, F, T, outdir=outdir)\n",
        "\n",
        "    # 7) digits: class-conditioned ExCIR (class 3)\n",
        "    if pack.name == \"digits8x8\":\n",
        "        print(\"  - Running special checks for digits dataset...\")\n",
        "        clf = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs', random_state=seed)\n",
        "        clf.fit(np.vstack([pack.Xtr, pack.Xva]), np.r_[pack.ytr, pack.yva])\n",
        "        Pva = clf.predict_proba(pack.Xva)\n",
        "        c = 3\n",
        "        s_c = excir_feature_scores(pack.Xva, Pva[:, c])\n",
        "        H = s_c.reshape(8,8)\n",
        "        plt.figure(figsize=(4.5,4.5)); plt.imshow(H, cmap='viridis'); plt.colorbar()\n",
        "        plt.title('Class-conditioned ExCIR (digit 3)'); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_excir_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "        fr, ins, dele = aopc_curves(\"clf\", clf, pack.Xte, pack.yte, s_c, steps=9)\n",
        "        plt.figure(figsize=(6.2,4.2))\n",
        "        plt.plot(100*fr, ins, marker='o', label='Insertion (keep top-% pixels)')\n",
        "        plt.plot(100*fr, dele, marker='o', label='Deletion (zero top-% pixels)')\n",
        "        plt.xlabel('Revealed / removed top-% pixels'); plt.ylabel('Test Accuracy')\n",
        "        plt.title('AOPC — digits (class 3 map)'); plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_aopc_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "    return scores\n",
        "\n",
        "# -----------------------\n",
        "# CLI\n",
        "# -----------------------\n",
        "\n",
        "def _run_everything(datasets, seed, ks_tuple, outdir, do_pfi):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    registry = ALL_DATASETS\n",
        "    names = [s.strip() for s in datasets.split(\",\") if s.strip()]\n",
        "    for nm in names:\n",
        "        if nm not in registry:\n",
        "            print(f\"[skip] Unknown dataset: {nm}\")\n",
        "            continue\n",
        "        try:\n",
        "            pack = registry[nm](seed=seed)\n",
        "            print(f\"=== Dataset: {pack.name} (task={pack.task}) ===\")\n",
        "            run_on_dataset(pack, ks=ks_tuple, seed=seed, do_pfi=do_pfi, outdir=outdir)\n",
        "        except Exception as e:\n",
        "            print(f\"[{nm}] ERROR: {e}\")\n",
        "    print(f\"\\nDone. See '{outdir}/' for CSVs and figures.\")\n",
        "\n",
        "def main(argv=None):\n",
        "    import argparse\n",
        "    p = argparse.ArgumentParser(description=\"Enhanced ExCIR — cross-domain benchmarks\")\n",
        "    p.add_argument(\"--datasets\", type=str, default=\"adult,20ng_bin,digits8x8,har6,california,diabetes,iris,wine,synthetic_clf,synthetic_reg,ecg_heartbeat,fashion_mnist_features,protein_structure,stock_technical,sensor_fusion,gene_expression,network_topology,audio_mfcc,weather_station,eeg_signals,satellite_ndvi,industrial_process,social_network,cyber_security,mixed_modal\",\n",
        "                   help=\"comma-separated list of datasets to run\")\n",
        "    p.add_argument(\"--seed\", type=int, default=0)\n",
        "    p.add_argument(\"--ks\", type=str, default=\"3,5,8,12\")\n",
        "    p.add_argument(\"--outdir\", type=str, default=\"out\")\n",
        "    p.add_argument(\"--no-pfi\", action=\"store_true\", help=\"disable PFI (speed)\")\n",
        "\n",
        "    if argv is None:\n",
        "        args, _ = p.parse_known_args()\n",
        "    else:\n",
        "        args = p.parse_args(argv)\n",
        "\n",
        "    ks_tuple = tuple(int(x) for x in args.ks.split(\",\"))\n",
        "    _run_everything(datasets=args.datasets,\n",
        "                    seed=args.seed,\n",
        "                    ks_tuple=ks_tuple,\n",
        "                    outdir=args.outdir,\n",
        "                    do_pfi=(not args.no_pfi))\n",
        "\n",
        "def run_bench(datasets=None, seed=0, ks=(3,5,8,12), outdir=\"out\", no_pfi=False):\n",
        "    if datasets is None:\n",
        "        datasets = \",\".join(ALL_DATASETS.keys())\n",
        "    _run_everything(datasets=datasets, seed=seed, ks_tuple=ks, outdir=outdir, do_pfi=(not no_pfi))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "GR4Sh64zyJZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== ONE TABLE for Part A + Part B =====================\n",
        "# -> Computes best efficiency, best ≤ budget, and best overall per dataset\n",
        "# -> Emits a single LaTeX (booktabs) table for BOTH parts + saves CSVs\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- match your figures ---\n",
        "PART_A = [\n",
        "    \"20ng_bin\",\"audio_mfcc\",\"california\",\n",
        "    \"cyber_security\",\"eeg_signals\",\"har6\",\n",
        "    \"industrial_process\",\"mixed_modal\",\"network_topology\",\n",
        "]\n",
        "PART_B = [\n",
        "    \"protein_structure\",\"satellite_ndvi\",\"sensor_fusion\",\"social_network\",\n",
        "    \"stock_technical\",\"synthetic_clf\",\"synthetic_reg\",\"weather_station\",\n",
        "]\n",
        "\n",
        "TIME_BUDGET = 0.50  # seconds for the \"best ≤ budget\" column\n",
        "OUTDIR = \"out\"\n",
        "\n",
        "def run_light_checks_and_collect(dataset_name, seed=0):\n",
        "    pack = ALL_DATASETS[dataset_name](seed=seed)\n",
        "    F, Times, CIRs, Resid, KLs, J8 = lightweight_three_checks(pack, seed=seed)\n",
        "    pct = (F * 100).astype(int).astype(str) + \"%\"\n",
        "    df = pd.DataFrame({\n",
        "        \"dataset\": dataset_name,\n",
        "        \"part\": (\"A\" if dataset_name in PART_A else \"B\"),\n",
        "        \"pct\": pct,\n",
        "        \"time\": Times,\n",
        "        \"cir\": CIRs,\n",
        "        \"resid\": Resid,\n",
        "        \"kl\": KLs,\n",
        "        \"j8\": J8,\n",
        "    })\n",
        "    return df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"time\",\"cir\"])\n",
        "\n",
        "def summarize_dataset(df, budget=TIME_BUDGET):\n",
        "    # Best efficiency\n",
        "    eff = df[\"cir\"] / np.clip(df[\"time\"], 1e-12, None)\n",
        "    r_eff = df.iloc[eff.values.argmax()]\n",
        "    # Best agreement under budget\n",
        "    under = df[df[\"time\"] <= budget]\n",
        "    if under.empty:\n",
        "        r_bud = {\"pct\":\"--\",\"time\":np.nan,\"cir\":np.nan}\n",
        "    else:\n",
        "        r_bud = under.iloc[under[\"cir\"].values.argmax()]\n",
        "    # Best overall\n",
        "    r_over = df.iloc[df[\"cir\"].values.argmax()]\n",
        "    return {\n",
        "        \"dataset\": df[\"dataset\"].iloc[0],\n",
        "        \"part\": df[\"part\"].iloc[0],\n",
        "        \"pct_eff\": r_eff[\"pct\"], \"time_eff\": float(r_eff[\"time\"]), \"cir_eff\": float(r_eff[\"cir\"]),\n",
        "        \"eff_best\": float((r_eff[\"cir\"] / max(r_eff[\"time\"], 1e-12))),\n",
        "        \"pct_budget\": r_bud[\"pct\"], \"time_budget\": float(r_bud[\"time\"]) if pd.notna(r_bud[\"time\"]) else np.nan,\n",
        "        \"cir_budget\": float(r_bud[\"cir\"]) if pd.notna(r_bud[\"cir\"]) else np.nan,\n",
        "        \"pct_over\": r_over[\"pct\"], \"time_over\": float(r_over[\"time\"]), \"cir_over\": float(r_over[\"cir\"]),\n",
        "    }\n",
        "\n",
        "def build_latex_one_table(df_summary, budget=TIME_BUDGET, label=\"tab:pareto_ab\"):\n",
        "    df_summary = df_summary.sort_values([\"part\",\"eff_best\"], ascending=[True, False])\n",
        "\n",
        "    def fmt(v, nd=3):\n",
        "        if pd.isna(v): return \"--\"\n",
        "        return f\"{v:.{nd}f}\"\n",
        "\n",
        "    header = r\"\"\"\\begin{table*}[t]\n",
        "\\centering\n",
        "\\small\n",
        "\\begin{tabular}{lccccccccc}\n",
        "\\toprule\n",
        "\\multirow{2}{*}{Dataset} & \\multirow{2}{*}{Part} &\n",
        "\\multicolumn{4}{c}{Best efficiency ($\\max\\,\\mathrm{CIR}/\\mathrm{time}$)} &\n",
        "\\multicolumn{3}{c}{Best agreement ($\\mathrm{time}\\le %s$ s)} &\n",
        "\\multirow{2}{*}{Best overall agr.} \\\\\n",
        "\\cmidrule(lr){3-6}\\cmidrule(lr){7-9}\n",
        " &  & \\%% kept & time (s) & CIR & eff. & \\%% kept & time (s) & CIR &  \\\\\n",
        "\\midrule\n",
        "\"\"\" % f\"{budget:.2f}\"\n",
        "\n",
        "    # add a small separator between parts\n",
        "    lines = []\n",
        "    current_part = None\n",
        "    for _, r in df_summary.iterrows():\n",
        "        if current_part is None:\n",
        "            current_part = r[\"part\"]\n",
        "        elif r[\"part\"] != current_part:\n",
        "            lines.append(r\"\\midrule\")\n",
        "            current_part = r[\"part\"]\n",
        "\n",
        "        ds = r[\"dataset\"].replace(\"_\", r\"\\_\")\n",
        "        line = (\n",
        "            f\"{ds} & {r['part']} & \"\n",
        "            f\"{r['pct_eff']} & {fmt(r['time_eff'],3)} & {fmt(r['cir_eff'],5)} & {fmt(r['eff_best'],3)} & \"\n",
        "            f\"{(r['pct_budget'] if isinstance(r['pct_budget'], str) else '--')} & {fmt(r['time_budget'],3)} & {fmt(r['cir_budget'],5)} & \"\n",
        "            f\"{fmt(r['cir_over'],5)} ({fmt(r['time_over'],3)} / {r['pct_over']}) \\\\\\\\\"\n",
        "        )\n",
        "        lines.append(line)\n",
        "\n",
        "    footer = r\"\"\"\\bottomrule\n",
        "\\end{tabular}\n",
        "\\caption{Cost--agreement summary combining Fig.~Part A and Part B. “Best efficiency” maximizes $\\mathrm{CIR}/\\mathrm{time}$; “Best agreement ($\\le %s$\\,s)” is the highest CIR not exceeding the time budget; “Best overall agr.” is the absolute highest CIR with its (time/\\%% kept).}\n",
        "\\label{%s}\n",
        "\\end{table*}\n",
        "\"\"\" % (f\"{budget:.2f}\", label)\n",
        "\n",
        "    return header + \"\\n\".join(lines) + \"\\n\" + footer\n",
        "\n",
        "# ---------------- run & output ----------------\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "all_points = []\n",
        "for ds in PART_A + PART_B:\n",
        "    pts = run_light_checks_and_collect(ds, seed=0)\n",
        "    all_points.append(pts)\n",
        "points = pd.concat(all_points, ignore_index=True)\n",
        "points.to_csv(os.path.join(OUTDIR, \"pareto_points_partAB.csv\"), index=False)\n",
        "\n",
        "summary = []\n",
        "for ds in PART_A + PART_B:\n",
        "    summary.append(summarize_dataset(points[points[\"dataset\"]==ds], budget=TIME_BUDGET))\n",
        "summary_df = pd.DataFrame(summary)\n",
        "summary_df.to_csv(os.path.join(OUTDIR, \"pareto_summary_partAB.csv\"), index=False)\n",
        "\n",
        "latex = build_latex_one_table(summary_df, budget=TIME_BUDGET, label=\"tab:pareto_partAB\")\n",
        "print(\"\\n================ LaTeX table (Part A + Part B) ================\\n\")\n",
        "print(latex)\n"
      ],
      "metadata": {
        "id": "c9AuoD6DyMTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AOPC — All datasets in one graph (solid=Insertion, dashed=Deletion)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Edit any dataset's arrays with your true numbers (0–100 step is common but not required)\n",
        "# Each has keys: pct, insertion, deletion\n",
        "datasets = {\n",
        "    \"digits8x8\": {  # smooth rise / smooth fall\n",
        "        \"pct\":       [0,10,20,30,40,50,60,70,80,90,100],\n",
        "        \"insertion\": [0.15,0.22,0.35,0.48,0.58,0.66,0.73,0.80,0.86,0.90,0.93],\n",
        "        \"deletion\":  [0.92,0.85,0.76,0.63,0.52,0.41,0.30,0.22,0.14,0.07,0.03],\n",
        "    },\n",
        "    \"audio_mfcc\": {  # moderate linear rise / fall\n",
        "        \"pct\":       [0,10,20,30,40,50,60,70,80,90,100],\n",
        "        \"insertion\": [0.20,0.24,0.28,0.31,0.35,0.38,0.41,0.44,0.47,0.50,0.53],\n",
        "        \"deletion\":  [0.52,0.49,0.45,0.41,0.37,0.33,0.29,0.25,0.20,0.15,0.10],\n",
        "    },\n",
        "    \"ecg_heartbeat\": {  # plateau then sharp improvement; deletion drops late\n",
        "        \"pct\":       [0,10,20,30,40,50,60,70,80,90,100],\n",
        "        \"insertion\": [0.55,0.56,0.56,0.57,0.58,0.59,0.60,0.78,0.82,0.84,0.85],\n",
        "        \"deletion\":  [0.78,0.78,0.78,0.78,0.77,0.77,0.76,0.35,0.22,0.12,0.08],\n",
        "    },\n",
        "    \"protein_structure (R²)\": {  # steady rise / steady fall\n",
        "        \"pct\":       [0,10,20,30,40,50,60,70,80,90,100],\n",
        "        \"insertion\": [0.05,0.12,0.20,0.28,0.36,0.45,0.53,0.61,0.70,0.78,0.86],\n",
        "        \"deletion\":  [0.60,0.50,0.41,0.34,0.27,0.21,0.15,0.11,0.08,0.05,0.03],\n",
        "    },\n",
        "    \"california (R²)\": {  # early jump then small gains; deletion mostly flat then down\n",
        "        \"pct\":       [0,10,20,30,40,50,60,70,80,90,100],\n",
        "        \"insertion\": [0.10,0.55,0.56,0.57,0.58,0.59,0.60,0.61,0.62,0.63,0.64],\n",
        "        \"deletion\":  [0.20,0.20,0.20,0.20,0.19,0.18,0.17,0.15,0.13,0.11,0.10],\n",
        "    },\n",
        "    \"satellite_ndvi (R²)\": {  # long flat then jump\n",
        "        \"pct\":       [0,10,20,30,40,50,60,70,80,90,100],\n",
        "        \"insertion\": [0.50,0.50,0.50,0.50,0.50,0.50,0.90,0.93,0.95,0.96,0.97],\n",
        "        \"deletion\":  [0.50,0.50,0.50,0.50,0.50,0.50,0.10,0.08,0.07,0.06,0.05],\n",
        "    },\n",
        "    \"gene_expression\": {  # slight dip and recovery (non-monotonic), deletion mirror-ish\n",
        "        \"pct\":       [0,10,20,30,40,50,60,70,80,90,100],\n",
        "        \"insertion\": [0.55,0.53,0.45,0.58,0.60,0.63,0.66,0.64,0.60,0.52,0.48],\n",
        "        \"deletion\":  [0.62,0.60,0.56,0.49,0.45,0.40,0.36,0.38,0.42,0.48,0.55],\n",
        "    },\n",
        "    \"digits (class 3 pixels)\": {  # steady rise / steady fall\n",
        "        \"pct\":       [0,10,20,30,40,50,60,70,80,90,100],\n",
        "        \"insertion\": [0.20,0.30,0.42,0.55,0.63,0.70,0.76,0.81,0.85,0.88,0.90],\n",
        "        \"deletion\":  [0.88,0.80,0.70,0.58,0.48,0.38,0.29,0.21,0.15,0.10,0.06],\n",
        "    },\n",
        "}\n",
        "\n",
        "normalize = True       # turn off when you paste true values on same scale\n",
        "linewidth = 2.0\n",
        "marker_every = 2\n",
        "\n",
        "def minmax(arr):\n",
        "    lo, hi = min(arr), max(arr)\n",
        "    return [(a - lo) / (hi - lo) if hi > lo else 0.0 for a in arr]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "for name, D in datasets.items():\n",
        "    x    = D[\"pct\"]\n",
        "    yin  = D[\"insertion\"]\n",
        "    ydel = D[\"deletion\"]\n",
        "\n",
        "    if normalize:\n",
        "        yin  = minmax(yin)\n",
        "        ydel = minmax(ydel)\n",
        "\n",
        "    line_in, = plt.plot(x, yin, linewidth=linewidth, label=f\"{name} – ins\")\n",
        "    plt.plot(x, ydel, \"--\", linewidth=linewidth, label=f\"{name} – del\",\n",
        "             color=line_in.get_color(), marker=\"o\", markevery=marker_every, alpha=0.95)\n",
        "\n",
        "plt.xlabel(\"Revealed / removed top-k% features\")\n",
        "plt.ylabel(\"Score (normalized)\" if normalize else \"Score\")\n",
        "plt.title(\"AOPC (Insertion vs Deletion) — All datasets in one graph\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend(ncol=2, fontsize=8, frameon=False, bbox_to_anchor=(0.5,-0.22), loc=\"upper center\")\n",
        "plt.tight_layout(rect=[0,0.05,1,1])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iGvaKn4RyPoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# toy_excir_table.py\n",
        "# Reproduces a toy comparison table for ExCIR vs KernelSHAP vs LIME\n",
        "# - Runtime (ms): includes one-time O(n^3) setup for ExCIR\n",
        "# - Suff@k: top-k sufficiency (test accuracy %) after retraining on top-k features\n",
        "# - Kendall tau@k: ranking stability after 5% Gaussian noise\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from numpy.linalg import eigh, cholesky\n",
        "from scipy.stats import kendalltau\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Optional: SHAP and LIME (graceful if absent)\n",
        "_has_shap = _has_lime = True\n",
        "try:\n",
        "    import shap\n",
        "except Exception:\n",
        "    _has_shap = False\n",
        "try:\n",
        "    from lime.lime_tabular import LimeTabularExplainer\n",
        "except Exception:\n",
        "    _has_lime = False\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# -----------------------------\n",
        "# Data generation (classification)\n",
        "# -----------------------------\n",
        "def make_correlated_gaussian(n, d, rho=0.6, block=20):\n",
        "    \"\"\"\n",
        "    Correlated Gaussian features: block-diagonal correlation with intra-block rho.\n",
        "    \"\"\"\n",
        "    blocks = d // block + (1 if d % block else 0)\n",
        "    X_list = []\n",
        "    for b in range(blocks):\n",
        "        sz = block if (b+1)*block <= d else d - b*block\n",
        "        Sigma = (1 - rho) * np.eye(sz) + rho * np.ones((sz, sz))\n",
        "        L = cholesky(Sigma)\n",
        "        Z = rng.standard_normal((n, sz))\n",
        "        Xb = Z @ L.T\n",
        "        X_list.append(Xb)\n",
        "    X = np.concatenate(X_list, axis=1)[:, :d]\n",
        "    return X\n",
        "\n",
        "def make_dataset(n=100, d=100, n_signal=10, rho=0.6, snr_db=5.0):\n",
        "    \"\"\"\n",
        "    Binary classification:\n",
        "    y = sigmoid(X @ w + noise) with w non-zero for first n_signal features.\n",
        "    \"\"\"\n",
        "    X = make_correlated_gaussian(n, d, rho=rho, block=20)\n",
        "    w = np.zeros(d)\n",
        "    w[:n_signal] = rng.normal(0, 1, size=n_signal)\n",
        "    # Scale noise to achieve approx SNR (dB) on linear score\n",
        "    linear = X @ w\n",
        "    signal_power = np.var(linear)\n",
        "    noise_power = signal_power / (10 ** (snr_db / 10.0))\n",
        "    noise = rng.normal(0, np.sqrt(noise_power), size=n)\n",
        "    logit = linear + noise\n",
        "    p = 1 / (1 + np.exp(-logit))\n",
        "    y = (p > 0.5).astype(int)\n",
        "    return X, y\n",
        "\n",
        "# -----------------------------\n",
        "# ExCIR implementation\n",
        "# -----------------------------\n",
        "def excir_calibration(n):\n",
        "    \"\"\"\n",
        "    One-time observation-only O(n^3) factorization (for the theorem):\n",
        "    Factorize H = I - 11^T/n via eigendecomposition. Return eigenvectors U.\n",
        "    (We don't need eigenvalues here; this step is to account for the O(n^3) setup.)\n",
        "    \"\"\"\n",
        "    one = np.ones((n, 1))\n",
        "    H = np.eye(n) - (one @ one.T) / n\n",
        "    # eigh on symmetric matrix ~ O(n^3)\n",
        "    eigvals, U = eigh(H)\n",
        "    return U  # not strictly needed to compute CIR, but included for realism\n",
        "\n",
        "def excir_scores(X, y_pred):\n",
        "    \"\"\"\n",
        "    CIR_i = num_i / den_i with shared mid-mean m_i = (mean(f_i)+mean(y_pred))/2\n",
        "    Works for probabilities or regression outputs.\n",
        "    Returns scores shape (d,).\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    mu_f = X.mean(axis=0)        # (d,)\n",
        "    mu_y = float(np.mean(y_pred))\n",
        "    m = 0.5 * (mu_f + mu_y)      # (d,)\n",
        "\n",
        "    # sums of squares (centered at m_i)\n",
        "    # \\sum_j (x_{ji} - m_i)^2  and \\sum_j (y_j - m_i)^2\n",
        "    # compute efficiently:\n",
        "    # sum_x2_i = \\sum_j x_{ji}^2\n",
        "    sum_x2 = np.sum(X**2, axis=0)     # (d,)\n",
        "    sum_x  = np.sum(X, axis=0)        # (d,)\n",
        "    # For y part:\n",
        "    y = y_pred\n",
        "    sum_y2 = float(np.sum(y**2))\n",
        "    sum_y  = float(np.sum(y))\n",
        "\n",
        "    # Using identity: sum (a - m)^2 = sum a^2 - 2 m sum a + n m^2\n",
        "    den_x = sum_x2 - 2*m*sum_x + n*(m**2)                  # (d,)\n",
        "    den_y = (sum_y2 - 2*m*sum_y + n*(m**2))                # (d,)\n",
        "    den = den_x + den_y                                    # (d,)\n",
        "\n",
        "    num = n * ((mu_f - m)**2 + (mu_y - m)**2)              # (d,)\n",
        "    # Avoid divide-by-zero\n",
        "    eps = 1e-12\n",
        "    cir = num / (den + eps)\n",
        "    # Bound to [0,1] numerically\n",
        "    cir = np.clip(cir, 0.0, 1.0)\n",
        "    return cir\n",
        "\n",
        "# -----------------------------\n",
        "# SHAP (KernelSHAP) and LIME global rankings\n",
        "# -----------------------------\n",
        "def global_ranking_shap(model, X_train, X_bg, X_eval, nsamples=200):\n",
        "    \"\"\"\n",
        "    KernelSHAP global ranking = mean|SHAP| over samples.\n",
        "    To keep runtime reasonable, use small background (e.g., 50) and eval (e.g., 50).\n",
        "    \"\"\"\n",
        "    if not _has_shap:\n",
        "        return None\n",
        "    f = lambda X: model.predict_proba(X)[:, 1]\n",
        "    explainer = shap.KernelExplainer(f, X_bg, link=\"logit\")\n",
        "    shap_vals = explainer.shap_values(X_eval, nsamples=nsamples)\n",
        "    # shap_vals shape: (n_eval, d)\n",
        "    mean_abs = np.mean(np.abs(shap_vals), axis=0)\n",
        "    ranking = np.argsort(mean_abs)[::-1]\n",
        "    return ranking, mean_abs\n",
        "\n",
        "def global_ranking_lime(model, X_train, y_train, X_eval, num_samples=1000, num_instances=50):\n",
        "    \"\"\"\n",
        "    Aggregate absolute LIME weights across a small batch of instances.\n",
        "    \"\"\"\n",
        "    if not _has_lime:\n",
        "        return None\n",
        "    explainer = LimeTabularExplainer(\n",
        "        X_train,\n",
        "        feature_names=[f\"f{i}\" for i in range(X_train.shape[1])],\n",
        "        class_names=[\"0\",\"1\"],\n",
        "        discretize_continuous=True,\n",
        "        verbose=False,\n",
        "        mode=\"classification\"\n",
        "    )\n",
        "    d = X_train.shape[1]\n",
        "    agg = np.zeros(d)\n",
        "    n_eval = min(num_instances, X_eval.shape[0])\n",
        "    for i in range(n_eval):\n",
        "        exp = explainer.explain_instance(\n",
        "            X_eval[i], model.predict_proba, num_features=d, labels=[1], num_samples=num_samples\n",
        "        )\n",
        "        # exp.as_list(label=1) returns list of (feature_name, weight); map back to indices\n",
        "        weights = np.zeros(d)\n",
        "        for name, w in exp.as_list(label=1):\n",
        "            # name like 'f12 <= 0.34' or 'f12 > 0.34' -> parse index\n",
        "            idx = int(name.split('f')[1].split()[0])\n",
        "            weights[idx] += abs(w)\n",
        "        agg += weights\n",
        "    mean_abs = agg / max(1, n_eval)\n",
        "    ranking = np.argsort(mean_abs)[::-1]\n",
        "    return ranking, mean_abs\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def topk_from_scores(scores, k):\n",
        "    return np.argsort(scores)[::-1][:k]\n",
        "\n",
        "def kendall_tau_at_k(rank_a, rank_b, k, d):\n",
        "    \"\"\"\n",
        "    Compute Kendall's tau on the top-k positions:\n",
        "    Take the union of top-k from both rankings, induce order, compute Kendall tau.\n",
        "    \"\"\"\n",
        "    top_a = list(rank_a[:k])\n",
        "    top_b = list(rank_b[:k])\n",
        "    # union set\n",
        "    U = list(dict.fromkeys(top_a + top_b))  # preserve order\n",
        "    # Build rankings (positions) over U\n",
        "    pos_a = {f:i for i,f in enumerate(rank_a)}\n",
        "    pos_b = {f:i for i,f in enumerate(rank_b)}\n",
        "    a_order = [pos_a[f] for f in U]\n",
        "    b_order = [pos_b[f] for f in U]\n",
        "    tau, _ = kendalltau(a_order, b_order)\n",
        "    return float(tau)\n",
        "\n",
        "def sufficiency_accuracy(model_cls, X_train, y_train, X_test, y_test, top_idx):\n",
        "    \"\"\"\n",
        "    Retrain model on top-k features, return test accuracy (0..100).\n",
        "    \"\"\"\n",
        "    clf = model_cls()\n",
        "    clf.fit(X_train[:, top_idx], y_train)\n",
        "    y_pred = clf.predict(X_test[:, top_idx])\n",
        "    acc = accuracy_score(y_test, y_pred) * 100.0\n",
        "    return acc\n",
        "\n",
        "# -----------------------------\n",
        "# Experiment runner\n",
        "# -----------------------------\n",
        "def run_once(n=100, d=100, k=10, rho=0.6, snr_db=5.0,\n",
        "             shap_bg=50, shap_eval=50, shap_nsamples=200,\n",
        "             lime_eval=30, lime_samples=800):\n",
        "    # Data\n",
        "    X, y = make_dataset(n=n, d=d, n_signal=10, rho=rho, snr_db=snr_db)\n",
        "\n",
        "    # Train/test split\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.4, random_state=123)\n",
        "\n",
        "    # Scale features (helps SHAP/LIME stability and classifier)\n",
        "    sc = StandardScaler()\n",
        "    Xtr = sc.fit_transform(Xtr)\n",
        "    Xte = sc.transform(Xte)\n",
        "\n",
        "    # Base classifier used for sufficiency test and for SHAP/LIME explanations\n",
        "    base = LogisticRegression(max_iter=200, solver='lbfgs')\n",
        "    base.fit(Xtr, ytr)\n",
        "\n",
        "    # Predictions for ExCIR (probability of class 1)\n",
        "    yhat_tr = base.predict_proba(Xtr)[:, 1]\n",
        "\n",
        "    # ExCIR ranking (with one-time O(n^3) calibration)\n",
        "    t0 = time.perf_counter()\n",
        "    _U = excir_calibration(n=Xtr.shape[0])  # O(n^3) step\n",
        "    cir_scores = excir_scores(Xtr, yhat_tr)\n",
        "    excir_rank = np.argsort(cir_scores)[::-1]\n",
        "    t_excir = (time.perf_counter() - t0) * 1000.0  # ms\n",
        "\n",
        "    # SHAP ranking\n",
        "    if _has_shap:\n",
        "        # Small background/eval subsets to manage runtime\n",
        "        bg_idx = rng.choice(Xtr.shape[0], size=min(shap_bg, Xtr.shape[0]), replace=False)\n",
        "        ev_idx = rng.choice(Xtr.shape[0], size=min(shap_eval, Xtr.shape[0]), replace=False)\n",
        "        t0 = time.perf_counter()\n",
        "        res = global_ranking_shap(base, Xtr, Xtr[bg_idx], Xtr[ev_idx], nsamples=shap_nsamples)\n",
        "        if res is None:\n",
        "            shap_rank, shap_scores, t_shap = None, None, None\n",
        "        else:\n",
        "            shap_rank, shap_scores = res\n",
        "            t_shap = (time.perf_counter() - t0) * 1000.0\n",
        "    else:\n",
        "        shap_rank, shap_scores, t_shap = None, None, None\n",
        "\n",
        "    # LIME ranking\n",
        "    if _has_lime:\n",
        "        ev_idx2 = rng.choice(Xtr.shape[0], size=min(lime_eval, Xtr.shape[0]), replace=False)\n",
        "        t0 = time.perf_counter()\n",
        "        res = global_ranking_lime(base, Xtr, ytr, Xtr[ev_idx2], num_samples=lime_samples, num_instances=lime_eval)\n",
        "        if res is None:\n",
        "            lime_rank, lime_scores, t_lime = None, None, None\n",
        "        else:\n",
        "            lime_rank, lime_scores = res\n",
        "            t_lime = (time.perf_counter() - t0) * 1000.0\n",
        "    else:\n",
        "        lime_rank, lime_scores, t_lime = None, None, None\n",
        "\n",
        "    # Sufficiency @k (use RidgeClassifier for stability)\n",
        "    ModelForSuff = RidgeClassifier\n",
        "    acc_excir = sufficiency_accuracy(ModelForSuff, Xtr, ytr, Xte, yte, excir_rank[:k])\n",
        "    acc_shap  = sufficiency_accuracy(ModelForSuff, Xtr, ytr, Xte, yte, shap_rank[:k]) if shap_rank is not None else np.nan\n",
        "    acc_lime  = sufficiency_accuracy(ModelForSuff, Xtr, ytr, Xte, yte, lime_rank[:k]) if lime_rank is not None else np.nan\n",
        "\n",
        "    # Stability: add 5% Gaussian noise to features, recompute rankings, compute Kendall tau@k\n",
        "    Xtr_noisy = Xtr + 0.05 * np.std(Xtr, axis=0, keepdims=True) * rng.standard_normal(Xtr.shape)\n",
        "    base_noisy = LogisticRegression(max_iter=200, solver='lbfgs').fit(Xtr_noisy, ytr)\n",
        "    yhat_tr_noisy = base_noisy.predict_proba(Xtr_noisy)[:, 1]\n",
        "    excir_rank_noisy = np.argsort(excir_scores(Xtr_noisy, yhat_tr_noisy))[::-1]\n",
        "    tau_excir = kendall_tau_at_k(excir_rank, excir_rank_noisy, k=k, d=d)\n",
        "\n",
        "    if _has_shap and shap_rank is not None:\n",
        "        bg_idx_n = rng.choice(Xtr_noisy.shape[0], size=min(shap_bg, Xtr_noisy.shape[0]), replace=False)\n",
        "        ev_idx_n = rng.choice(Xtr_noisy.shape[0], size=min(shap_eval, Xtr_noisy.shape[0]), replace=False)\n",
        "        res_n = global_ranking_shap(base_noisy, Xtr_noisy, Xtr_noisy[bg_idx_n], Xtr_noisy[ev_idx_n], nsamples=shap_nsamples)\n",
        "        shap_rank_noisy = res_n[0]\n",
        "        tau_shap = kendall_tau_at_k(shap_rank, shap_rank_noisy, k=k, d=d)\n",
        "    else:\n",
        "        tau_shap = np.nan\n",
        "\n",
        "    if _has_lime and lime_rank is not None:\n",
        "        ev_idx2_n = rng.choice(Xtr_noisy.shape[0], size=min(lime_eval, Xtr_noisy.shape[0]), replace=False)\n",
        "        res_n2 = global_ranking_lime(base_noisy, Xtr_noisy, ytr, Xtr_noisy[ev_idx2_n],\n",
        "                                     num_samples=lime_samples, num_instances=lime_eval)\n",
        "        lime_rank_noisy = res_n2[0]\n",
        "        tau_lime = kendall_tau_at_k(lime_rank, lime_rank_noisy, k=k, d=d)\n",
        "    else:\n",
        "        tau_lime = np.nan\n",
        "\n",
        "    return {\n",
        "        \"runtime_ms\": {\"ExCIR\": t_excir, \"KernelSHAP\": t_shap, \"LIME\": t_lime},\n",
        "        \"suff@k\":     {\"ExCIR\": acc_excir, \"KernelSHAP\": acc_shap, \"LIME\": acc_lime},\n",
        "        \"tau@k\":      {\"ExCIR\": tau_excir, \"KernelSHAP\": tau_shap, \"LIME\": tau_lime},\n",
        "    }\n",
        "\n",
        "def summarize_runs(n_values=(50,100,200), d=100, k=10, runs=5):\n",
        "    rows = []\n",
        "    for n in n_values:\n",
        "        for method in [\"ExCIR\", \"KernelSHAP\", \"LIME\"]:\n",
        "            rt, acc, tau = [], [], []\n",
        "            for r in range(runs):\n",
        "                res = run_once(n=n, d=d, k=k)\n",
        "                rt.append(res[\"runtime_ms\"][method])\n",
        "                acc.append(res[\"suff@k\"][method])\n",
        "                tau.append(res[\"tau@k\"][method])\n",
        "            rt_arr  = np.array(rt, dtype=float)\n",
        "            acc_arr = np.array(acc, dtype=float)\n",
        "            tau_arr = np.array(tau, dtype=float)\n",
        "\n",
        "            def fmt(x):\n",
        "                if np.isnan(x).any():\n",
        "                    return \"—\"\n",
        "                return f\"{np.nanmean(x):.0f} ± {np.nanstd(x):.0f}\"\n",
        "\n",
        "            def fmt_float(x, dec=1):\n",
        "                if np.isnan(x).any():\n",
        "                    return \"—\"\n",
        "                return f\"{np.nanmean(x):.{dec}f} ± {np.nanstd(x):.{dec}f}\"\n",
        "\n",
        "            rows.append([n, d, method, fmt(rt_arr), fmt_float(acc_arr, 1), fmt_float(tau_arr, 2)])\n",
        "\n",
        "    headers = [\"n\", \"d\", \"Method\", \"Runtime (ms)↓\", f\"Suff@k={k} (%)↑\", f\"Kendall τ@k={k} (5% noise)↑\"]\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"github\"))\n",
        "\n",
        "    # ---- LaTeX table (f-strings avoid % escaping headaches) ----\n",
        "    print(\"\\n\\n% -------- LaTeX table --------\")\n",
        "    print(\"\\\\begin{table}[t]\")\n",
        "    print(\"\\\\centering\")\n",
        "    print(f\"\\\\caption{{Toy empirical comparison (mean $\\\\pm$ std. over {runs} trials). \"\n",
        "          \"ExCIR includes the one-time $\\\\mathcal{{O}}(n^3)$ calibration; per-feature scoring is negligible thereafter.}}\")\n",
        "    print(\"\\\\label{tab:toy_excir}\")\n",
        "    print(\"\\\\small\")\n",
        "    print(\"\\\\begin{tabular}{@{}cccccc@{}}\")\n",
        "    print(\"\\\\toprule\")\n",
        "    print(f\"$n$ & $d$ & Method & Runtime (ms)$\\\\downarrow$ & Suff@$k={k}$ (\\\\%)$\\\\uparrow$ & \"\n",
        "          f\"Kendall $\\\\tau$@$k={k}$ (5\\\\% noise)$\\\\uparrow$ \\\\\\\\\")\n",
        "    print(\"\\\\midrule\")\n",
        "\n",
        "    for n in n_values:\n",
        "        subset = [r for r in rows if r[0] == n]\n",
        "        for idx, r in enumerate(subset):\n",
        "            if idx == 0:\n",
        "                print(f\"{r[0]} & {r[1]} & {r[2]} & {r[3]} & {r[4]} & {r[5]} \\\\\\\\\")\n",
        "            else:\n",
        "                # leave n and d cells visually empty for multirow-like look\n",
        "                print(f\"  &   & {r[2]} & {r[3]} & {r[4]} & {r[5]} \\\\\\\\\")\n",
        "        print(\"\\\\cmidrule(lr){1-6}\")\n",
        "\n",
        "    print(\"\\\\bottomrule\")\n",
        "    print(\"\\\\end{tabular}\")\n",
        "    print(\"\\\\end{table}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    summarize_runs(n_values=(50, 100, 200), d=100, k=10, runs=5)\n"
      ],
      "metadata": {
        "id": "lCpLDOmiySGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ExCIR-LW (zero-loss) vs SHAP vs LIME — runtime, fidelity, accuracy\n",
        "# Vehicular synthetic benchmark (revised: repeats + CI + robust explainers)\n",
        "# ============================================================\n",
        "\n",
        "import os, time, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Optional deps (robust fallbacks included)\n",
        "try:\n",
        "    import shap  # type: ignore\n",
        "except Exception:\n",
        "    shap = None\n",
        "try:\n",
        "    from lime.lime_tabular import LimeTabularExplainer  # type: ignore\n",
        "except Exception:\n",
        "    LimeTabularExplainer = None\n",
        "\n",
        "SEED = 42\n",
        "rng = check_random_state(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Synthetic vehicular data\n",
        "# -----------------------------\n",
        "def make_vehicular(n=6000, seed=SEED):\n",
        "    rng = check_random_state(seed)\n",
        "    feat_names = [\n",
        "        \"speed_kph\",\"rpm\",\"throttle\",\"brake\",\"steering_deg\",\"gear\",\n",
        "        \"accel_long\",\"accel_lat\",\"yaw_rate\",\"road_grade\",\n",
        "        \"ambient_temp\",\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\",\n",
        "        \"engine_load\",\"maf\",\"intake_air_temp\",\"battery_v\",\"fuel_rate\"\n",
        "    ]\n",
        "    speed = np.clip(rng.normal(80, 15, n), 0, None)\n",
        "    throttle = np.clip(rng.beta(2, 2, n), 0, 1)\n",
        "    brake = np.clip(1 - throttle + rng.normal(0, 0.15, n), 0, 1)\n",
        "    steering = rng.normal(0, 10, n)\n",
        "    gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, n), 1, 7)\n",
        "    accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, n)\n",
        "    accel_lat = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, n)\n",
        "    yaw_rate = rng.normal(steering/30 * (speed/60), 0.2, n)\n",
        "    road_grade = rng.normal(0, 2, n)\n",
        "    ambient_temp = rng.normal(20, 8, n)\n",
        "    tire_base = rng.normal(34, 1.0, (n,4))\n",
        "    low_mask = rng.uniform(0,1,n) < 0.15\n",
        "    tire_drop = rng.normal(4, 1.0, (n,4)) * low_mask[:,None]\n",
        "    tires = tire_base - tire_drop\n",
        "    engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, n), 0, 100)\n",
        "    maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,n), 0, None)\n",
        "    intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, n), -10, 80)\n",
        "    battery_v = np.clip(rng.normal(13.8, 0.3, n) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "    fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,n), 0, None)\n",
        "    rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, n), 700, 7000)\n",
        "\n",
        "    X = pd.DataFrame({\n",
        "        \"speed_kph\": speed, \"rpm\": rpm, \"throttle\": throttle, \"brake\": brake,\n",
        "        \"steering_deg\": steering, \"gear\": gear, \"accel_long\": accel_long,\n",
        "        \"accel_lat\": accel_lat, \"yaw_rate\": yaw_rate, \"road_grade\": road_grade,\n",
        "        \"ambient_temp\": ambient_temp, \"tire_fl\": tires[:,0], \"tire_fr\": tires[:,1],\n",
        "        \"tire_rl\": tires[:,2], \"tire_rr\": tires[:,3], \"engine_load\": engine_load,\n",
        "        \"maf\": maf, \"intake_air_temp\": intake_air_temp, \"battery_v\": battery_v,\n",
        "        \"fuel_rate\": fuel_rate\n",
        "    })\n",
        "    low_tire = (32 - X[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "    risk_logit = (\n",
        "        1.2*(X[\"speed_kph\"]-110)/20 + 1.1*X[\"brake\"] + 0.9*np.abs(X[\"steering_deg\"])/15\n",
        "        + 0.7*np.abs(X[\"yaw_rate\"]) + 0.8*(low_tire) + 0.7*(X[\"engine_load\"]/100)\n",
        "        + 0.3*(X[\"road_grade\"]/5) - 0.2*(X[\"battery_v\"]-13.5)\n",
        "    )\n",
        "    p = 1/(1+np.exp(-(risk_logit + rng.normal(0, 0.4, n))))\n",
        "    y = (rng.uniform(0,1,n) < p).astype(int)\n",
        "    return X, y, feat_names\n",
        "\n",
        "X, y, FEATS = make_vehicular()\n",
        "X_trv, X_te, y_trv, y_te = train_test_split(X, y, test_size=0.20, stratify=y, random_state=SEED)\n",
        "X_tr, X_va, y_tr, y_va   = train_test_split(X_trv, y_trv, test_size=0.20, stratify=y_trv, random_state=SEED)\n",
        "\n",
        "scaler = StandardScaler().fit(X_tr)\n",
        "Xs_tr = scaler.transform(X_tr)\n",
        "Xs_va = scaler.transform(X_va)\n",
        "Xs_te = scaler.transform(X_te)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Baseline (strong enough)\n",
        "# -----------------------------\n",
        "baseline = GradientBoostingClassifier(\n",
        "    random_state=SEED, n_estimators=500, learning_rate=0.05, max_depth=3, subsample=0.9\n",
        ")\n",
        "t0 = time.time()\n",
        "baseline.fit(np.vstack([Xs_tr, Xs_va]), np.hstack([y_tr, y_va]))\n",
        "base_train_time = time.time()-t0\n",
        "base_acc = accuracy_score(y_te, baseline.predict(Xs_te))\n",
        "print(f\"[Baseline] GBM test acc = {base_acc:.6f}  (fit {base_train_time:.2f}s)\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) ExCIR score (stable, bounded [0,1]): r^2(feature, ŷ)\n",
        "# -----------------------------\n",
        "def compute_cir_stable(Xs, yhat, names, eps=1e-12):\n",
        "    vals = []\n",
        "    y = yhat.astype(float); y = (y - y.mean())\n",
        "    vy = (y.var() + eps)\n",
        "    for i in range(Xs.shape[1]):\n",
        "        f = Xs[:, i].astype(float); f = f - f.mean()\n",
        "        vf = f.var() + eps\n",
        "        cov = (f*y).mean()\n",
        "        r2 = (cov**2)/(vf*vy)   # in [0,1]\n",
        "        vals.append(float(r2))\n",
        "    return pd.Series(vals, index=names).sort_values(ascending=False)\n",
        "\n",
        "# Full-data ExCIR on validation\n",
        "yh_va = baseline.predict_proba(Xs_va)[:,1]\n",
        "t0 = time.time()\n",
        "excir_full = compute_cir_stable(Xs_va, yh_va, FEATS)\n",
        "excir_full_time = time.time()-t0\n",
        "excir_full.to_csv(\"bench_excir_full.csv\")\n",
        "print(f\"[ExCIR] full val set in {excir_full_time:.3f}s\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4) ExCIR-LW with repeats (mean + CI)\n",
        "# -----------------------------\n",
        "fractions = [0.2, 0.3, 0.5]\n",
        "N_REPEATS = 7  # stabilize estimates\n",
        "run_rows = []\n",
        "for f in fractions:\n",
        "    for r in range(N_REPEATS):\n",
        "        m = max(100, int(len(y_va)*f))\n",
        "        idx = rng.choice(len(y_va), size=m, replace=False)\n",
        "        t0 = time.time()\n",
        "        excir_f = compute_cir_stable(Xs_va[idx], yh_va[idx], FEATS)\n",
        "        dt = time.time()-t0\n",
        "        rho = excir_full.rank().corr(excir_f[excir_full.index].rank(), method=\"spearman\")\n",
        "        k = 10\n",
        "        overlap = len(set(excir_full.head(k).index) & set(excir_f.head(k).index))/k\n",
        "        run_rows.append([f, r, dt, rho, overlap])\n",
        "\n",
        "runs_df = pd.DataFrame(run_rows, columns=[\"frac_val\",\"repeat\",\"sec\",\"spearman_vs_full\",\"top10_overlap\"])\n",
        "runs_df.to_csv(\"bench_excir_lightweight_runs.csv\", index=False)\n",
        "\n",
        "# aggregate\n",
        "agg = runs_df.groupby(\"frac_val\").agg(\n",
        "    sec_mean=(\"sec\",\"mean\"), sec_std=(\"sec\",\"std\"),\n",
        "    rho_mean=(\"spearman_vs_full\",\"mean\"), rho_std=(\"spearman_vs_full\",\"std\"),\n",
        "    ov_mean=(\"top10_overlap\",\"mean\"), ov_std=(\"top10_overlap\",\"std\")\n",
        ").reset_index()\n",
        "agg.to_csv(\"bench_excir_lightweight.csv\", index=False)\n",
        "print(\"[ExCIR] lightweight fractions (aggregated) saved to bench_excir_lightweight.csv\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Surrogates (restricted capacity)\n",
        "# -----------------------------\n",
        "surrogates = {\n",
        "    \"LogReg(L2,C=0.2)\": LogisticRegression(max_iter=2000, C=0.2, random_state=SEED),\n",
        "    \"DecisionTree(d=2,l=100)\": DecisionTreeClassifier(max_depth=2, min_samples_leaf=100, random_state=SEED),\n",
        "    \"TinyGBM(20x2)\": GradientBoostingClassifier(n_estimators=20, max_depth=2, learning_rate=0.2, random_state=SEED),\n",
        "    \"TinyRF(40,d=4,l=50)\": RandomForestClassifier(\n",
        "        n_estimators=40, max_depth=4, min_samples_leaf=50, max_features=0.5, random_state=SEED, n_jobs=-1\n",
        "    ),\n",
        "}\n",
        "rows = []\n",
        "for name, mdl in surrogates.items():\n",
        "    t0 = time.time()\n",
        "    mdl.fit(Xs_tr, y_tr)\n",
        "    train_t = time.time()-t0\n",
        "    acc = accuracy_score(y_te, mdl.predict(Xs_te))\n",
        "    rows.append([name, acc, base_acc-acc, train_t])\n",
        "    print(f\"[Surrogate] {name:20s} acc={acc:.6f} drop={base_acc-acc:+.6f} fit={train_t:.2f}s\")\n",
        "sur_df = pd.DataFrame(rows, columns=[\"model\",\"test_acc\",\"acc_drop_vs_baseline\",\"fit_sec\"])\n",
        "sur_df.to_csv(\"bench_surrogates_accuracy.csv\", index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Explainers (SHAP, LIME or fallbacks)\n",
        "# -----------------------------\n",
        "def topk_overlap(a_series, b_series, k=10):\n",
        "    return len(set(a_series.head(k).index) & set(b_series.head(k).index)) / float(k)\n",
        "\n",
        "def compute_shap_global_rank(model, X_train, X_val, feature_names, max_val=800):\n",
        "    \"\"\"Try shap.Explainer -> TreeExplainer -> Permutation FI (fallback).\"\"\"\n",
        "    start = time.time()\n",
        "    err = None\n",
        "    try:\n",
        "        if shap is None:\n",
        "            raise RuntimeError(\"shap not installed\")\n",
        "        # prefer TreeExplainer for trees\n",
        "        if isinstance(model, (GradientBoostingClassifier, RandomForestClassifier, DecisionTreeClassifier)):\n",
        "            expl = shap.TreeExplainer(model)\n",
        "            sv = expl.shap_values(X_val[:min(max_val, X_val.shape[0])])\n",
        "            if isinstance(sv, list):  # binary\n",
        "                sv = sv[-1]\n",
        "        else:\n",
        "            expl = shap.Explainer(model, X_train, feature_names=feature_names)\n",
        "            sv = expl(X_val[:min(max_val, X_val.shape[0])]).values\n",
        "        dur = time.time() - start\n",
        "        rank = pd.Series(np.abs(sv).mean(0), index=feature_names).sort_values(ascending=False)\n",
        "        return \"SHAP\", rank, dur, None\n",
        "    except Exception as e:\n",
        "        err = str(e)\n",
        "\n",
        "    # Fallback: permutation FI (accuracy drop)\n",
        "    start = time.time()\n",
        "    base_pred = model.predict(X_val)\n",
        "    base_acc_local = accuracy_score(y_va, base_pred)\n",
        "    imp = []\n",
        "    Xw = X_val.copy()\n",
        "    for j in range(Xw.shape[1]):\n",
        "        Xw_j = Xw.copy()\n",
        "        rng.shuffle(Xw_j[:, j])\n",
        "        acc_j = accuracy_score(y_va, model.predict(Xw_j))\n",
        "        imp.append(max(0.0, base_acc_local - acc_j))\n",
        "    dur = time.time() - start\n",
        "    rank = pd.Series(imp, index=feature_names).sort_values(ascending=False)\n",
        "    return \"SHAP(fallback=PFI)\", rank, dur, err\n",
        "\n",
        "def compute_lime_like_global_rank(model, X_train, X_val, feature_names,\n",
        "                                  n_instances=200, n_perturb=800, sigma=0.5, alpha=1e-2):\n",
        "    \"\"\"LIME-approx: local Ridge on Gaussian perturbations; aggregate |coef|.\"\"\"\n",
        "    from sklearn.linear_model import Ridge\n",
        "    start = time.time()\n",
        "    ids = rng.choice(X_val.shape[0], size=min(n_instances, X_val.shape[0]), replace=False)\n",
        "    agg = np.zeros(X_train.shape[1], dtype=float)\n",
        "    for i in ids:\n",
        "        x0 = X_val[i:i+1]\n",
        "        Z = x0 + rng.normal(0, sigma, size=(n_perturb, x0.shape[1]))\n",
        "        yhat = model.predict_proba(Z)[:,1]\n",
        "        rr = Ridge(alpha=alpha, fit_intercept=True, random_state=SEED)\n",
        "        rr.fit(Z, yhat)\n",
        "        agg += np.abs(rr.coef_)\n",
        "    w = agg / len(ids)\n",
        "    dur = time.time() - start\n",
        "    rank = pd.Series(w, index=feature_names).sort_values(ascending=False)\n",
        "    return rank, dur\n",
        "\n",
        "rank_rows = []  # model, explainer, sec, spearman_vs_ExCIR, top10_overlap, test_acc\n",
        "for name, mdl in surrogates.items():\n",
        "    model_acc = float(sur_df.loc[sur_df[\"model\"] == name, \"test_acc\"].values[0])\n",
        "\n",
        "    # LIME (real) or LIME-approx\n",
        "    if LimeTabularExplainer is not None:\n",
        "        t0 = time.time()\n",
        "        expl = LimeTabularExplainer(\n",
        "            Xs_tr, feature_names=FEATS, class_names=[\"0\",\"1\"],\n",
        "            discretize_continuous=True, mode=\"classification\", random_state=SEED\n",
        "        )\n",
        "        preds = lambda Z: mdl.predict_proba(Z)\n",
        "        ids  = rng.choice(Xs_va.shape[0], size=min(200, Xs_va.shape[0]), replace=False)\n",
        "        agg  = pd.Series(0.0, index=FEATS); cnt = pd.Series(0, index=FEATS, dtype=int)\n",
        "        for i in ids:\n",
        "            exp = expl.explain_instance(Xs_va[i], preds, num_features=min(10, len(FEATS)))\n",
        "            for fname, w in exp.as_list():\n",
        "                if fname in agg.index:\n",
        "                    agg[fname] += abs(w); cnt[fname] += 1\n",
        "        cnt[cnt==0] = 1\n",
        "        lime_rank = (agg/cnt).sort_values(ascending=False)\n",
        "        lime_t    = time.time() - t0\n",
        "        rho_lime  = excir_full.rank().corr(lime_rank[excir_full.index].rank(), method=\"spearman\")\n",
        "        ov_lime   = topk_overlap(excir_full, lime_rank, k=10)\n",
        "        rank_rows.append([name, \"LIME\", lime_t, rho_lime, ov_lime, model_acc])\n",
        "        lime_rank.to_csv(f\"bench_rank_{name}_LIME.csv\")\n",
        "    else:\n",
        "        lime_rank, lime_t = compute_lime_like_global_rank(mdl, Xs_tr, Xs_va, FEATS)\n",
        "        rho_lime  = excir_full.rank().corr(lime_rank[excir_full.index].rank(), method=\"spearman\")\n",
        "        ov_lime   = topk_overlap(excir_full, lime_rank, k=10)\n",
        "        rank_rows.append([name, \"LIME(aprx)\", lime_t, rho_lime, ov_lime, model_acc])\n",
        "        lime_rank.to_csv(f\"bench_rank_{name}_LIMEaprx.csv\")\n",
        "    print(f\"[Rank] {name} LIME*  rho={rank_rows[-1][3]:.3f}  top10={rank_rows[-1][4]:.2f}  t={rank_rows[-1][2]:.2f}s\")\n",
        "\n",
        "    # SHAP (robust with fallback)\n",
        "    method, shap_rank, shap_t, shap_err = compute_shap_global_rank(mdl, Xs_tr, Xs_va, FEATS)\n",
        "    rho_shap  = excir_full.rank().corr(shap_rank[excir_full.index].rank(), method=\"spearman\")\n",
        "    ov_shap   = topk_overlap(excir_full, shap_rank, k=10)\n",
        "    rank_rows.append([name, method, shap_t, rho_shap, ov_shap, model_acc])\n",
        "    shap_rank.to_csv(f\"bench_rank_{name}_{method.replace('/','_')}.csv\")\n",
        "    print(f\"[Rank] {name} {method}  rho={rho_shap:.3f}  top10={ov_shap:.2f}  t={shap_t:.2f}s\")\n",
        "\n",
        "rank_df = pd.DataFrame(rank_rows, columns=[\n",
        "    \"model\",\"explainer\",\"sec\",\"spearman_vs_ExCIR\",\"top10_overlap\",\"test_acc\"\n",
        "])\n",
        "rank_df[\"acc_drop_vs_baseline\"] = base_acc - rank_df[\"test_acc\"]\n",
        "rank_df.to_csv(\"bench_surrogate_rank_agreement.csv\", index=False)\n",
        "print(\"[Rank] Saved bench_surrogate_rank_agreement.csv\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Accuracy-cost table incl. ExCIR-LW (zero-loss)\n",
        "# -----------------------------\n",
        "# ExCIR rows use aggregated mean times; accuracy equals baseline; drop=0\n",
        "excir_rows = []\n",
        "for _, r in agg.iterrows():\n",
        "    label = f\"ExCIR-LW({int(r['frac_val']*100)}%)\"\n",
        "    excir_rows.append([label, base_acc, 0.0, r[\"sec_mean\"], \"explain\"])\n",
        "baseline_row = [[\"Baseline(GBM)\", base_acc, 0.0, base_train_time, \"fit\"]]\n",
        "\n",
        "sur_tmp = sur_df.copy()\n",
        "sur_tmp[\"kind\"] = \"fit\"\n",
        "sur_tmp = sur_tmp.rename(columns={\"fit_sec\":\"sec\"})[[\"model\",\"test_acc\",\"acc_drop_vs_baseline\",\"sec\",\"kind\"]]\n",
        "\n",
        "acc_cost = pd.DataFrame(baseline_row, columns=[\"model\",\"test_acc\",\"acc_drop_vs_baseline\",\"sec\",\"kind\"])\n",
        "acc_cost = pd.concat([acc_cost, pd.DataFrame(excir_rows, columns=acc_cost.columns), sur_tmp], ignore_index=True)\n",
        "acc_cost.to_csv(\"bench_accuracy_costs.csv\", index=False)\n",
        "print(\"[Accuracy-Cost] Saved bench_accuracy_costs.csv\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Unified method comparison (SHAP/LIME/ExCIR-LW)\n",
        "# -----------------------------\n",
        "excir_cmp = pd.DataFrame([\n",
        "    [\"ExCIR-LW\", f\"ExCIR-LW({int(r['frac_val']*100)}%)\", r[\"sec_mean\"], r[\"rho_mean\"], r[\"ov_mean\"], base_acc, 0.0]\n",
        "    for _, r in agg.iterrows()\n",
        "], columns=[\"method\",\"model\",\"time_sec\",\"rank_spearman\",\"top10_overlap\",\"test_acc\",\"acc_drop_vs_baseline\"])\n",
        "\n",
        "sr = pd.read_csv(\"bench_surrogate_rank_agreement.csv\").rename(columns={\n",
        "    \"explainer\":\"method\",\"sec\":\"time_sec\",\"spearman_vs_ExCIR\":\"rank_spearman\"\n",
        "})[[\"method\",\"model\",\"time_sec\",\"rank_spearman\",\"top10_overlap\",\"test_acc\",\"acc_drop_vs_baseline\"]]\n",
        "\n",
        "method_cmp = pd.concat([excir_cmp, sr], ignore_index=True)\n",
        "method_cmp.to_csv(\"bench_method_compare.csv\", index=False)\n",
        "print(\"[Compare] Saved bench_method_compare.csv\")\n",
        "\n",
        "# -----------------------------\n",
        "# 9) Plots (cleaned; with error bars for ExCIR-LW)\n",
        "# -----------------------------\n",
        "# Order for bars: Baseline → ExCIR-LW → Surrogates\n",
        "excir_order = [f\"ExCIR-LW({int(f*100)}%)\" for f in fractions]\n",
        "sur_order   = list(sur_df[\"model\"])\n",
        "labels_order = [\"Baseline(GBM)\"] + excir_order + sur_order\n",
        "acc_cost_idx = acc_cost.set_index(\"model\")\n",
        "\n",
        "# (A) Accuracy bars\n",
        "plt.figure(figsize=(9.5,5))\n",
        "accs = [acc_cost_idx.loc[m, \"test_acc\"] for m in labels_order]\n",
        "plt.bar(range(len(labels_order)), accs)\n",
        "plt.xticks(range(len(labels_order)), labels_order, rotation=20, ha=\"right\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.title(\"Predictive accuracy: Baseline vs ExCIR-LW vs Lightweight Surrogates\")\n",
        "plt.tight_layout(); plt.savefig(\"bench_acc_bars.png\", dpi=150)\n",
        "\n",
        "# (B) Accuracy drop vs baseline\n",
        "plt.figure(figsize=(9.5,5))\n",
        "drops = [acc_cost_idx.loc[m, \"acc_drop_vs_baseline\"] for m in labels_order]\n",
        "plt.bar(range(len(labels_order)), drops)\n",
        "plt.xticks(range(len(labels_order)), labels_order, rotation=20, ha=\"right\")\n",
        "plt.ylabel(\"Accuracy drop vs baseline (↓ is worse)\")\n",
        "plt.title(\"Accuracy cost of surrogates vs zero-loss ExCIR-LW\")\n",
        "plt.tight_layout(); plt.savefig(\"bench_acc_drop_bars.png\", dpi=150)\n",
        "\n",
        "# (C) ExCIR-LW agreement with error bars\n",
        "plt.figure(figsize=(8,4.6))\n",
        "plt.errorbar(agg[\"frac_val\"], agg[\"rho_mean\"], yerr=agg[\"rho_std\"], fmt=\"-o\", label=\"Spearman (rank)\")\n",
        "plt.errorbar(agg[\"frac_val\"], agg[\"ov_mean\"], yerr=agg[\"ov_std\"], fmt=\"-o\", label=\"Top-10 overlap\")\n",
        "plt.axhline(1.0, ls=\"--\", lw=1)\n",
        "plt.ylim(0,1.05); plt.xlabel(\"Fraction of validation rows used\")\n",
        "plt.ylabel(\"Agreement with full ExCIR\"); plt.legend()\n",
        "plt.title(\"ExCIR: lightweight compute keeps the same model (mean ± std over repeats)\")\n",
        "plt.tight_layout(); plt.savefig(\"bench_excir_lightweight_agreement.png\", dpi=150)\n",
        "\n",
        "# (D) Rank agreement vs time (surrogate explainers)\n",
        "ra = pd.read_csv(\"bench_surrogate_rank_agreement.csv\")\n",
        "if len(ra):\n",
        "    plt.figure(figsize=(9,5))\n",
        "    for expl in ra[\"explainer\"].unique():\n",
        "        sub = ra[ra[\"explainer\"]==expl]\n",
        "        plt.scatter(sub[\"sec\"], sub[\"spearman_vs_ExCIR\"], label=expl, s=70)\n",
        "        for _,r in sub.iterrows():\n",
        "            plt.text(r[\"sec\"]*1.02, r[\"spearman_vs_ExCIR\"]+0.01, r[\"model\"], fontsize=8)\n",
        "    plt.xlabel(\"Seconds (lower is better)\"); plt.ylabel(\"Spearman vs ExCIR (higher is better)\")\n",
        "    plt.title(\"Rank agreement vs time (surrogate explainers)\")\n",
        "    plt.legend(); plt.tight_layout(); plt.savefig(\"bench_rank_time_scatter.png\", dpi=150)\n",
        "\n",
        "# (E) Time vs Test Accuracy — SHAP vs LIME vs ExCIR-LW\n",
        "plt.figure(figsize=(10,5))\n",
        "for m in method_cmp[\"method\"].unique():\n",
        "    sub = method_cmp[method_cmp[\"method\"]==m]\n",
        "    plt.scatter(sub[\"time_sec\"], sub[\"test_acc\"], s=70, label=m)\n",
        "    for _, r in sub.iterrows():\n",
        "        plt.text(r[\"time_sec\"]*1.02, r[\"test_acc\"]+0.0008, r[\"model\"], fontsize=8)\n",
        "plt.xlabel(\"Explanation time (s)\"); plt.ylabel(\"Test accuracy of underlying model\")\n",
        "plt.title(\"Performance (time) vs Predictive Accuracy: SHAP vs LIME vs ExCIR-LW\")\n",
        "plt.legend(); plt.tight_layout(); plt.savefig(\"bench_method_time_vs_accuracy.png\", dpi=150)\n",
        "\n",
        "# (F) Time vs Rank fidelity (Spearman vs full ExCIR)\n",
        "plt.figure(figsize=(10,5))\n",
        "for m in method_cmp[\"method\"].unique():\n",
        "    sub = method_cmp[method_cmp[\"method\"]==m]\n",
        "    plt.scatter(sub[\"time_sec\"], sub[\"rank_spearman\"], s=70, label=m)\n",
        "    for _, r in sub.iterrows():\n",
        "        plt.text(r[\"time_sec\"]*1.02, r[\"rank_spearman\"]+0.01, r[\"model\"], fontsize=8)\n",
        "plt.xlabel(\"Explanation time (s)\"); plt.ylabel(\"Rank fidelity (Spearman vs full ExCIR)\")\n",
        "plt.title(\"Fidelity (rank) vs Time: SHAP vs LIME vs ExCIR-LW\")\n",
        "plt.ylim(0, 1.05); plt.legend(); plt.tight_layout(); plt.savefig(\"bench_method_time_vs_rank.png\", dpi=150)\n",
        "\n",
        "# (G) Accuracy drop by method/model (zero for ExCIR-LW)\n",
        "plt.figure(figsize=(11,5))\n",
        "disp = method_cmp.copy()\n",
        "disp[\"label\"] = disp[\"model\"].where(disp[\"method\"]==\"ExCIR-LW\", disp[\"method\"]+\"-\"+disp[\"model\"])\n",
        "plt.bar(range(len(disp)), disp[\"acc_drop_vs_baseline\"])\n",
        "plt.xticks(range(len(disp)), disp[\"label\"], rotation=25, ha=\"right\")\n",
        "plt.ylabel(\"Accuracy drop vs baseline (↓ is worse)\")\n",
        "plt.title(\"Accuracy cost: SHAP/LIME surrogates vs zero-loss ExCIR-LW\")\n",
        "plt.tight_layout(); plt.savefig(\"bench_method_accuracy_drop.png\", dpi=150)\n",
        "\n",
        "# -----------------------------\n",
        "# 10) Console sanity check\n",
        "# -----------------------------\n",
        "print(\"\\nSanity check (ExCIR-LW zero-loss):\")\n",
        "print(\"  baseline acc =\", base_acc)\n",
        "print(method_cmp[method_cmp[\"method\"]==\"ExCIR-LW\"][[\"model\",\"test_acc\",\"acc_drop_vs_baseline\",\"time_sec\",\"rank_spearman\",\"top10_overlap\"]])\n",
        "print(\"\\nDone. CSVs:\")\n",
        "print(\"  - bench_excir_full.csv\")\n",
        "print(\"  - bench_excir_lightweight_runs.csv\")\n",
        "print(\"  - bench_excir_lightweight.csv\")\n",
        "print(\"  - bench_surrogates_accuracy.csv\")\n",
        "print(\"  - bench_surrogate_rank_agreement.csv\")\n",
        "print(\"  - bench_accuracy_costs.csv\")\n",
        "print(\"  - bench_method_compare.csv\")\n",
        "print(\"PNGs:\")\n",
        "print(\"  - bench_acc_bars.png\")\n",
        "print(\"  - bench_acc_drop_bars.png\")\n",
        "print(\"  - bench_excir_lightweight_agreement.png\")\n",
        "print(\"  - bench_rank_time_scatter.png\")\n",
        "print(\"  - bench_method_time_vs_accuracy.png\")\n",
        "print(\"  - bench_method_time_vs_rank.png\")\n",
        "print(\"  - bench_method_accuracy_drop.png\")\n"
      ],
      "metadata": {
        "id": "i39TBNy-yWCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group_cir_experiment.py\n",
        "# Synthetic vehicular data -> train GBM -> ExCIR per-feature & Block (Group) ExCIR\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "SEED = 42\n",
        "rng = check_random_state(SEED)\n",
        "\n",
        "# 1) Synthetic vehicular data\n",
        "def make_vehicular(n=6000, seed=SEED):\n",
        "    rng = check_random_state(seed)\n",
        "    speed = np.clip(rng.normal(80, 15, n), 0, None)\n",
        "    throttle = np.clip(rng.beta(2, 2, n), 0, 1)\n",
        "    brake = np.clip(1 - throttle + rng.normal(0, 0.15, n), 0, 1)\n",
        "    steering = rng.normal(0, 10, n)\n",
        "    gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, n), 1, 7)\n",
        "    accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, n)\n",
        "    accel_lat = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, n)\n",
        "    yaw_rate = rng.normal(steering/30 * (speed/60), 0.2, n)\n",
        "    road_grade = rng.normal(0, 2, n)\n",
        "    ambient_temp = rng.normal(20, 8, n)\n",
        "    tire_base = rng.normal(34, 1.0, (n,4))\n",
        "    low_mask = rng.uniform(0,1,n) < 0.15\n",
        "    tire_drop = rng.normal(4, 1.0, (n,4)) * low_mask[:,None]\n",
        "    tires = tire_base - tire_drop\n",
        "    engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, n), 0, 100)\n",
        "    maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,n), 0, None)\n",
        "    intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, n), -10, 80)\n",
        "    battery_v = np.clip(rng.normal(13.8, 0.3, n) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "    fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,n), 0, None)\n",
        "    rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, n), 700, 7000)\n",
        "\n",
        "    X = pd.DataFrame({\n",
        "        \"speed_kph\": speed, \"rpm\": rpm, \"throttle\": throttle, \"brake\": brake,\n",
        "        \"steering_deg\": steering, \"gear\": gear, \"accel_long\": accel_long,\n",
        "        \"accel_lat\": accel_lat, \"yaw_rate\": yaw_rate, \"road_grade\": road_grade,\n",
        "        \"ambient_temp\": ambient_temp, \"tire_fl\": tires[:,0], \"tire_fr\": tires[:,1],\n",
        "        \"tire_rl\": tires[:,2], \"tire_rr\": tires[:,3], \"engine_load\": engine_load,\n",
        "        \"maf\": maf, \"intake_air_temp\": intake_air_temp, \"battery_v\": battery_v,\n",
        "        \"fuel_rate\": fuel_rate\n",
        "    })\n",
        "    low_tire = (32 - X[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "    risk_logit = (\n",
        "        1.2*(X[\"speed_kph\"]-110)/20 + 1.1*X[\"brake\"] + 0.9*np.abs(X[\"steering_deg\"])/15\n",
        "        + 0.7*np.abs(X[\"yaw_rate\"]) + 0.8*(low_tire) + 0.7*(X[\"engine_load\"]/100)\n",
        "        + 0.3*(X[\"road_grade\"]/5) - 0.2*(X[\"battery_v\"]-13.5)\n",
        "    )\n",
        "    p = 1/(1+np.exp(-(risk_logit + rng.normal(0, 0.4, n))))\n",
        "    y = (rng.uniform(0,1,n) < p).astype(int)\n",
        "    return X, y, list(X.columns)\n",
        "\n",
        "# 2) CIR helpers (midpoint-centered ratio)\n",
        "def compute_cir(Xs, yhat, names):\n",
        "    n = Xs.shape[0]\n",
        "    y_bar = float(np.mean(yhat))\n",
        "    vals = []\n",
        "    for i in range(Xs.shape[1]):\n",
        "        f = Xs[:, i]\n",
        "        f_bar = float(np.mean(f))\n",
        "        m = 0.5*(f_bar + y_bar)\n",
        "        num = n*((f_bar - m)**2 + (y_bar - m)**2)\n",
        "        den = float(np.sum((f - m)**2) + np.sum((yhat - m)**2))\n",
        "        vals.append(0.0 if den <= 0 else num/den)\n",
        "    return pd.Series(vals, index=names).sort_values(ascending=False)\n",
        "\n",
        "def compute_group_cir(Xs, yhat, feat_names, groups):\n",
        "    name_to_idx = {n:i for i,n in enumerate(feat_names)}\n",
        "    out = {}\n",
        "    for gname, cols in groups.items():\n",
        "        idxs = [name_to_idx[c] for c in cols if c in name_to_idx]\n",
        "        if not idxs:\n",
        "            continue\n",
        "        z = Xs[:, idxs].mean(axis=1)  # equal-weight aggregated block (on standardized features)\n",
        "        n = len(z)\n",
        "        z_bar = float(np.mean(z))\n",
        "        y_bar = float(np.mean(yhat))\n",
        "        m = 0.5*(z_bar + y_bar)\n",
        "        num = n*((z_bar - m)**2 + (y_bar - m)**2)\n",
        "        den = float(np.sum((z - m)**2) + np.sum((yhat - m)**2))\n",
        "        out[gname] = 0.0 if den <= 0 else num/den\n",
        "    return pd.Series(out).sort_values(ascending=False)\n",
        "\n",
        "# 3) Data, splits, scaling\n",
        "X, y, FEATS = make_vehicular()\n",
        "X_trv, X_te, y_trv, y_te = train_test_split(X, y, test_size=0.20, stratify=y, random_state=SEED)\n",
        "X_tr, X_va, y_tr, y_va   = train_test_split(X_trv, y_trv, test_size=0.20, stratify=y_trv, random_state=SEED)\n",
        "\n",
        "scaler = StandardScaler().fit(X_tr)\n",
        "Xs_tr = scaler.transform(X_tr)\n",
        "Xs_va = scaler.transform(X_va)\n",
        "Xs_te = scaler.transform(X_te)\n",
        "\n",
        "# 4) Baseline model: GBM on train+val\n",
        "gbm = GradientBoostingClassifier(random_state=SEED, n_estimators=200, max_depth=3)\n",
        "gbm.fit(np.vstack([Xs_tr, Xs_va]), np.hstack([y_tr, y_va]))\n",
        "base_acc = accuracy_score(y_te, gbm.predict(Xs_te))\n",
        "print(f\"Baseline GBM test accuracy = {base_acc:.3f}\")\n",
        "\n",
        "# 5) ExCIR per-feature + Group-CIR on validation\n",
        "yhat_va = gbm.predict_proba(Xs_va)[:,1]\n",
        "excir_feat = compute_cir(Xs_va, yhat_va, FEATS)\n",
        "groups = {\n",
        "    \"Tires\": [\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"],\n",
        "    \"Dynamics\": [\"steering_deg\",\"yaw_rate\",\"accel_lat\",\"accel_long\"],\n",
        "    \"Control\": [\"brake\",\"throttle\"],\n",
        "    \"Powertrain\": [\"rpm\",\"engine_load\",\"maf\",\"fuel_rate\",\"gear\"],\n",
        "    \"Speed\": [\"speed_kph\"],\n",
        "    \"Environment\": [\"road_grade\",\"ambient_temp\",\"intake_air_temp\",\"battery_v\"]\n",
        "}\n",
        "excir_group = compute_group_cir(Xs_va, yhat_va, FEATS, groups)\n",
        "\n",
        "# 6) Save results\n",
        "excir_feat.to_csv(\"veh_excir_feature.csv\")\n",
        "excir_group.to_csv(\"veh_excir_group.csv\")\n",
        "top10 = excir_feat.head(10).reset_index()\n",
        "top10.columns = [\"feature\",\"CIR\"]\n",
        "feat_to_group = {c:g for g, cols in groups.items() for c in cols}\n",
        "top10[\"group\"] = top10[\"feature\"].map(feat_to_group).fillna(\"Other\")\n",
        "top10.to_csv(\"veh_excir_top10_with_group.csv\", index=False)\n",
        "\n",
        "# 7) Plots\n",
        "plt.figure(figsize=(8,5))\n",
        "order = excir_feat.sort_values(ascending=True)\n",
        "plt.barh(range(len(order)), order.values)\n",
        "plt.yticks(range(len(order)), order.index)\n",
        "plt.xlabel(\"CIR (feature)\")\n",
        "plt.title(f\"ExCIR per-feature (validation) — Baseline GBM acc={base_acc:.3f}\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"veh_excir_feature.png\", dpi=150)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "g_order = excir_group.sort_values(ascending=True)\n",
        "plt.barh(range(len(g_order)), g_order.values)\n",
        "plt.yticks(range(len(g_order)), g_order.index)\n",
        "plt.xlabel(\"CIR (group)\")\n",
        "plt.title(\"Block (Group) ExCIR\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"veh_excir_group.png\", dpi=150)\n"
      ],
      "metadata": {
        "id": "FhdqBIkeyZgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Enhanced ExCIR — cross-domain benchmarks\n",
        "XGBoost with GPU (gpu_hist) and CPU ('hist') fallback.\n",
        "Saves ALL results to CSV:\n",
        "  out/feature_importances.csv\n",
        "  out/aopc_curves.csv\n",
        "  out/aopc_summary.csv\n",
        "  out/summary_topk.csv\n",
        "  out/stability_noise.csv\n",
        "  out/lightweight_checks.csv\n",
        "  out/base_model_metrics.csv\n",
        "  (special) out/digits_excir_class3_map.csv when digits8x8 is run\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, time, sys, csv, warnings, argparse\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde, entropy\n",
        "from scipy.signal import periodogram\n",
        "\n",
        "# --- GPU Acceleration via XGBoost ---\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance, partial_dependence\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "\n",
        "# Use prefixed sklearn loaders to avoid name clashes\n",
        "from sklearn.datasets import (\n",
        "    fetch_openml, load_digits, load_iris as skl_load_iris, load_wine as skl_load_wine,\n",
        "    fetch_20newsgroups, fetch_california_housing, load_diabetes as skl_load_diabetes,\n",
        "    make_classification, make_regression\n",
        ")\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "sys.setrecursionlimit(3000)\n",
        "\n",
        "# ===================== ExCIR core =====================\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    z = np.asarray(z, float).ravel(); s = np.asarray(s, float).ravel()\n",
        "    assert z.shape[0] == s.shape[0]\n",
        "    mu_z, mu_s = z.mean(), s.mean()\n",
        "    m = 0.5*(mu_z + mu_s)\n",
        "    num = z.shape[0]*((mu_z - m)**2 + (mu_s - m)**2)\n",
        "    den = np.sum((z - m)**2) + np.sum((s - m)**2) + eps\n",
        "    return float(num/den)\n",
        "\n",
        "def excir_feature_scores(X: np.ndarray, p: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    X = np.asarray(X, float); p = np.asarray(p, float).ravel()\n",
        "    n, d = X.shape; assert p.shape[0] == n\n",
        "    mu_p = p.mean(); mu_x = X.mean(axis=0); m_j = 0.5*(mu_x + mu_p)\n",
        "    num = n*((mu_x - m_j)**2 + (mu_p - m_j)**2)\n",
        "    den = np.sum((X - m_j)**2, axis=0) + np.sum((p[:,None] - m_j[None,:])**2, axis=0) + eps\n",
        "    return num/den\n",
        "\n",
        "# ===================== metrics & helpers =====================\n",
        "def projection_alignment_residual(y_full, y_light):\n",
        "    y = np.asarray(y_full).ravel(); yp = np.asarray(y_light).ravel()\n",
        "    n = y.shape[0]; Phi = np.c_[yp, np.ones(n)]\n",
        "    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n",
        "    yhat = Phi @ theta\n",
        "    return float(np.linalg.norm(y - yhat) / (np.linalg.norm(y) + 1e-12))\n",
        "\n",
        "def kde_kl_symmetric(y_full, y_light, grid_points=400):\n",
        "    y = np.asarray(y_full).ravel(); yp = np.asarray(y_light).ravel()\n",
        "    kde_p = gaussian_kde(y); kde_q = gaussian_kde(yp)\n",
        "    both = np.r_[y, yp]; lo, hi = np.percentile(both, 0.5), np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None); q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo)/max(grid_points-1, 1)\n",
        "    kl_pq = float(np.sum(p*(np.log(p)-np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q*(np.log(q)-np.log(p))) * dx)\n",
        "    return 0.5*(kl_pq + kl_qp)\n",
        "\n",
        "def jaccard_topk(a_scores, b_scores, k=8):\n",
        "    A = set(np.argsort(-a_scores)[:k]); B = set(np.argsort(-b_scores)[:k])\n",
        "    return len(A & B) / (len(A | B) + 1e-12)\n",
        "\n",
        "def pfi_scores(model, X, y, n_repeats=5, seed=0):\n",
        "    try:\n",
        "        return permutation_importance(model, X.astype(np.float32), y,\n",
        "                                      n_repeats=n_repeats, random_state=seed,\n",
        "                                      scoring=None).importances_mean\n",
        "    except Exception:\n",
        "        return np.zeros(X.shape[1])\n",
        "\n",
        "def tree_gain_scores(model, d):\n",
        "    try:\n",
        "        w = getattr(model, \"feature_importances_\", None)\n",
        "        return w if (w is not None and len(w)==d) else np.zeros(d)\n",
        "    except Exception:\n",
        "        return np.zeros(d)\n",
        "\n",
        "def pdp_var_scores(model, X, grid_points=15, random_state=0):\n",
        "    d = X.shape[1]; scores = np.zeros(d); rs = check_random_state(random_state)\n",
        "    idx = rs.choice(np.arange(X.shape[0]), size=min(1000, X.shape[0]), replace=False)\n",
        "    Xs = X[idx]\n",
        "    for j in range(d):\n",
        "        try:\n",
        "            pd = partial_dependence(model, Xs.astype(np.float32), features=[j],\n",
        "                                    kind='average', grid_resolution=grid_points)\n",
        "            scores[j] = np.var(pd.average[0])\n",
        "        except Exception:\n",
        "            scores[j] = 0.0\n",
        "    return scores\n",
        "\n",
        "def surrogate_lr_scores(X, teacher_scores, alpha=1.0):\n",
        "    X = np.asarray(X, float); y = np.asarray(teacher_scores, float).ravel()\n",
        "    reg = Ridge(alpha=alpha, random_state=0).fit(X, y)\n",
        "    w = np.abs(reg.coef_); w = np.linalg.norm(w, axis=0) if w.ndim>1 else w\n",
        "    return w/(w.sum()+1e-12)\n",
        "\n",
        "def mi_pred_scores(X, preds, task):\n",
        "    X = np.asarray(X, float); p = np.asarray(preds, float).ravel()\n",
        "    if task==\"clf\":\n",
        "        z = (p > np.median(p)).astype(int); return mutual_info_classif(X, z, random_state=0)\n",
        "    z = np.digitize(p, np.quantile(p, [1/3, 2/3])); return mutual_info_classif(X, z, random_state=0)\n",
        "\n",
        "def mi_label_scores(X, y, task):\n",
        "    return mutual_info_classif(X, y, random_state=0) if task==\"clf\" else mutual_info_regression(X, y, random_state=0)\n",
        "\n",
        "def rank_from_scores(scores): return np.argsort(-scores)\n",
        "\n",
        "def aopc_curves(task, model, Xte, yte, scores, steps=9):\n",
        "    idx = rank_from_scores(scores)\n",
        "    n, d = Xte.shape; fracs = np.linspace(0,1,steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f*d)); keep = idx[:k]\n",
        "        mask = np.zeros(d, bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0).astype(np.float32)\n",
        "        X_del = np.where(mask, 0.0, Xte).astype(np.float32)\n",
        "        try:\n",
        "            y_ins = model.predict(X_ins); y_del = model.predict(X_del)\n",
        "            if task==\"clf\":\n",
        "                ins.append(accuracy_score(yte, y_ins)); dele.append(accuracy_score(yte, y_del))\n",
        "            else:\n",
        "                ins.append(r2_score(yte, y_ins)); dele.append(r2_score(yte, y_del))\n",
        "        except Exception:\n",
        "            ins.append(np.nan); dele.append(np.nan)\n",
        "    return fracs, np.asarray(ins), np.asarray(dele)\n",
        "\n",
        "def stability_noise_eval(scores_fn, Xva, p_va, repeats=15, sigma=0.01):\n",
        "    base = scores_fn(Xva, p_va); cirs, j8 = [], []\n",
        "    for _ in range(repeats):\n",
        "        noise = np.random.normal(0, sigma, size=Xva.shape)\n",
        "        s_noisy = scores_fn(Xva+noise, p_va)\n",
        "        cirs.append(cir_pair(base, s_noisy)); j8.append(jaccard_topk(base, s_noisy, k=min(8, Xva.shape[1])))\n",
        "    return np.asarray(cirs), np.asarray(j8)\n",
        "\n",
        "# ===================== XGBoost training & inference =====================\n",
        "def _get_xgb_model(task, y_data, params):\n",
        "    unique_classes = np.unique(y_data)\n",
        "    if task == \"clf\":\n",
        "        if len(unique_classes) > 2:\n",
        "            return xgb.XGBClassifier(objective='multi:softmax',\n",
        "                                     num_class=len(unique_classes),\n",
        "                                     eval_metric='merror', **params)\n",
        "        else:\n",
        "            return xgb.XGBClassifier(objective='binary:logistic',\n",
        "                                     eval_metric='logloss',\n",
        "                                     use_label_encoder=False, **params)\n",
        "    else:\n",
        "        return xgb.XGBRegressor(objective='reg:squarederror',\n",
        "                                eval_metric='rmse', **params)\n",
        "\n",
        "def train_gboost(task, Xtr, ytr, Xva, yva, seed=0):\n",
        "    Xall = np.vstack([Xtr, Xva]).astype(np.float32); yall = np.r_[ytr, yva]\n",
        "    params = {\n",
        "        'n_estimators': 50, 'random_state': seed,\n",
        "        'tree_method': 'gpu_hist', 'gpu_id': 0,\n",
        "        'validate_parameters': True, 'n_jobs': 1\n",
        "    }\n",
        "    m = _get_xgb_model(task, yall, params)\n",
        "    try:\n",
        "        m.fit(Xall, yall); return m\n",
        "    except xgb.core.XGBoostError:\n",
        "        print(\"  [XGBoost] GPU failed. Retrying with tree_method='hist' (CPU).\")\n",
        "        params['tree_method'] = 'hist'; params.pop('gpu_id', None)\n",
        "        m = _get_xgb_model(task, yall, params)\n",
        "        try:\n",
        "            m.fit(Xall, yall); return m\n",
        "        except Exception as e_cpu:\n",
        "            print(f\"  [XGBoost] CPU fall-back also failed: {e_cpu}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"XGBoost training failed unexpectedly: {e}\")\n",
        "        return None\n",
        "\n",
        "def predict_scores(task, model, X):\n",
        "    if model is None: return np.zeros(X.shape[0])\n",
        "    X = X.astype(np.float32)\n",
        "    try:\n",
        "        # Use API type to decide\n",
        "        if isinstance(model, xgb.XGBClassifier):\n",
        "            proba = model.predict_proba(X)\n",
        "            return proba[:, -1] if proba.ndim==2 else proba\n",
        "        else:\n",
        "            return model.predict(X)\n",
        "    except Exception:\n",
        "        return np.zeros(X.shape[0])\n",
        "\n",
        "# ===================== dataset plumbing =====================\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str\n",
        "    task: str  # \"clf\" | \"reg\"\n",
        "    Xtr: np.ndarray; ytr: np.ndarray\n",
        "    Xva: np.ndarray; yva: np.ndarray\n",
        "    Xte: np.ndarray; yte: np.ndarray\n",
        "    feature_names: List[str]\n",
        "\n",
        "def _standardize_fit(Xtr, Xva, Xte):\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    return ss.fit_transform(Xtr), ss.transform(Xva), ss.transform(Xte)\n",
        "\n",
        "def _split_xy(X, y, task, seed=0):\n",
        "    Xtr, Xte, ytr, yte = train_test_split(\n",
        "        X, y, test_size=0.2,\n",
        "        stratify=y if task==\"clf\" else None,\n",
        "        random_state=seed\n",
        "    )\n",
        "    Xtr, Xva, ytr, yva = train_test_split(\n",
        "        Xtr, ytr, test_size=0.25,\n",
        "        stratify=ytr if task==\"clf\" else None,\n",
        "        random_state=seed\n",
        "    )\n",
        "    Xtr_s, Xva_s, Xte_s = _standardize_fit(Xtr, Xva, Xte)\n",
        "    feat_names = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "    return Xtr_s, ytr, Xva_s, yva, Xte_s, yte, feat_names\n",
        "\n",
        "# ---- loaders ----\n",
        "def load_adult(seed=0):\n",
        "    try:\n",
        "        ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "        dfX = ds.data.select_dtypes(include=[np.number]).copy()\n",
        "        y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "        X = dfX.to_numpy(float)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"adult\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,list(dfX.columns))\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_20ng_binary(seed=0):\n",
        "    try:\n",
        "        cats=['comp.graphics','sci.space']\n",
        "        tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "        te = fetch_20newsgroups(subset='test',  categories=cats, remove=('headers','footers','quotes'))\n",
        "        tf = TfidfVectorizer(max_features=3000, ngram_range=(1,2), stop_words='english')\n",
        "        Xtr_full = tf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "        Xte = tf.transform(te.data).astype(np.float32).toarray()\n",
        "        ytr_full, yte = tr.target, te.target\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2, stratify=ytr_full, random_state=seed)\n",
        "        return DatasetPack(\"20ng_bin\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,[f\"tfidf_{i}\" for i in range(Xtr.shape[1])])\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_har(seed=0):\n",
        "    try:\n",
        "        ds = fetch_openml(\"HAR\", version=1, as_frame=True)\n",
        "        X = ds.data.to_numpy(float); y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"har6\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,[f\"sensor_{i}\" for i in range(X.shape[1])])\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_digits8x8(seed=0):\n",
        "    dg = load_digits(); X = dg.data.astype(float)/16.0; y = dg.target\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"digits8x8\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,[f\"pix{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_california(seed=0):\n",
        "    try:\n",
        "        ds = fetch_california_housing()\n",
        "        X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"california\",\"reg\",Xtr,ytr,Xva,yva,Xte,yte,list(ds.feature_names))\n",
        "    except Exception:\n",
        "        return load_synthetic_reg(seed)\n",
        "\n",
        "def load_diabetes(seed=0):\n",
        "    ds = skl_load_diabetes(); X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    feat = list(getattr(ds,\"feature_names\", [f\"f{i}\" for i in range(X.shape[1])]))\n",
        "    return DatasetPack(\"diabetes\",\"reg\",Xtr,ytr,Xva,yva,Xte,yte,feat)\n",
        "\n",
        "def load_iris(seed=0):\n",
        "    ds = skl_load_iris(); X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"iris\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,list(ds.feature_names))\n",
        "\n",
        "def load_wine(seed=0):\n",
        "    ds = skl_load_wine(); X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr2, Xva2, ytr2, yva2 = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr_s, Xva_s, Xte_s = _standardize_fit(Xtr2, Xva2, Xte)\n",
        "    return DatasetPack(\"wine\",\"clf\",Xtr_s,ytr2,Xva_s,yva2,Xte_s,yte,list(ds.feature_names))\n",
        "\n",
        "def load_synthetic_clf(seed=0):\n",
        "    X, y = make_classification(n_samples=3000, n_features=40, n_informative=10,\n",
        "                               n_redundant=10, n_repeated=5, n_classes=2, random_state=seed)\n",
        "    Xtr, ytr, Xva, yva, Xte, yte, feat = _split_xy(X, y, \"clf\", seed)\n",
        "    return DatasetPack(\"synthetic_clf\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,[f\"synth_clf_{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_synthetic_reg(seed=0):\n",
        "    X, y = make_regression(n_samples=3000, n_features=40, n_informative=10, random_state=seed)\n",
        "    Xtr, ytr, Xva, yva, Xte, yte, feat = _split_xy(X, y, \"reg\", seed)\n",
        "    return DatasetPack(\"synthetic_reg\",\"reg\",Xtr,ytr,Xva,yva,Xte,yte,[f\"synth_reg_{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "# extra synthetic modalities\n",
        "def _dominant_freq(x, fs=1.0):\n",
        "    f, Pxx = periodogram(x, fs=fs)\n",
        "    return f[np.argmax(Pxx)] if len(Pxx)>0 and np.isfinite(Pxx).any() else 0.0\n",
        "def _spec_entropy(x, fs=1.0):\n",
        "    f, Pxx = periodogram(x, fs=fs); p = Pxx + 1e-12; p /= p.sum(); return float(entropy(p))\n",
        "def _ts_to_feats(T, fs=1.0):\n",
        "    T = np.asarray(T, float)\n",
        "    return np.array([T.mean(), T.std(), T.min(), T.max(),\n",
        "                     _dominant_freq(T, fs), _spec_entropy(T, fs),\n",
        "                     np.mean(np.diff(T)), np.std(np.diff(T))], float)\n",
        "def _gen_ts_dataset(n_samples=2000, length=256, n_channels=1, seed=0, task=\"clf\"):\n",
        "    rs = check_random_state(seed); fs=1.0; X_feats=[]; y=[]\n",
        "    for _ in range(n_samples):\n",
        "        label = rs.randint(0,2) if task==\"clf\" else None\n",
        "        chans=[]\n",
        "        for _c in range(n_channels):\n",
        "            freq = 0.05 + 0.2*rs.rand() + (0.1 if (task==\"clf\" and label==1) else 0.0)\n",
        "            t = np.arange(length)/fs\n",
        "            sig = np.sin(2*np.pi*freq*t)+0.5*np.sin(2*np.pi*(freq*2)*t + rs.rand()*2*np.pi) + 0.3*rs.randn(length)\n",
        "            chans.append(sig)\n",
        "        chans = np.stack(chans, axis=0)\n",
        "        feats = np.concatenate([_ts_to_feats(ch, fs) for ch in chans], 0); X_feats.append(feats)\n",
        "        y.append(label if task==\"clf\" else feats.sum()+rs.randn()*0.5)\n",
        "    X = np.vstack(X_feats); y = np.array(y, dtype=int if task==\"clf\" else float)\n",
        "    return X, y\n",
        "def _split_pack_from_xy(X, y, name, task, seed):\n",
        "    Xtr, ytr, Xva, yva, Xte, yte, feat = _split_xy(X, y, task, seed)\n",
        "    return DatasetPack(name, task, Xtr, ytr, Xva, yva, Xte, yte, [f\"{name}_f{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_ecg_heartbeat(seed=0): X,y=_gen_ts_dataset(2500,256,1,seed,\"clf\"); return _split_pack_from_xy(X,y,\"ecg_heartbeat\",\"clf\",seed)\n",
        "def load_fashion_mnist_features(seed=0):\n",
        "    rs = check_random_state(seed); n=5000; d=128; y=rs.randint(0,10,size=n)\n",
        "    Z = rs.randn(n,16) + (y[:,None]*0.15); W = rs.randn(16,d); X = Z@W + 0.1*rs.randn(n,d)\n",
        "    return _split_pack_from_xy(X,y,\"fashion_mnist_features\",\"clf\",seed)\n",
        "def load_protein_structure(seed=0): X,y=make_regression(4000,120,20,noise=2.0,random_state=seed); return _split_pack_from_xy(X,y,\"protein_structure\",\"reg\",seed)\n",
        "def load_stock_technical(seed=0):\n",
        "    rs=check_random_state(seed); n=4000; T=60\n",
        "    prices = 100 + np.cumsum(rs.randn(n,T),1); rets = np.diff(prices,1)\n",
        "    vol=rets.std(1); mom=prices[:,-1]-prices[:,0]\n",
        "    skew=((rets-rets.mean(1,keepdims=True))**3).mean(1); kurt=((rets-rets.mean(1,keepdims=True))**4).mean(1)\n",
        "    feats=np.c_[rets.mean(1),vol,mom,skew,kurt]; y=(rs.randn(n)+0.2*np.sign(mom)>0).astype(int)\n",
        "    return _split_pack_from_xy(feats,y,\"stock_technical\",\"clf\",seed)\n",
        "def load_sensor_fusion(seed=0): X,y=_gen_ts_dataset(3000,128,3,seed,\"clf\"); return _split_pack_from_xy(X,y,\"sensor_fusion\",\"clf\",seed)\n",
        "def load_gene_expression(seed=0):\n",
        "    rs=check_random_state(seed); n=800; d=2000; y=rs.randint(0,2,size=n)\n",
        "    base=rs.randn(n,d)*0.5; signal=np.zeros((n,d)); idx=rs.choice(d,30,replace=False); signal[y==1][:,idx]=1.5\n",
        "    X=base+signal+0.1*rs.randn(n,d); return _split_pack_from_xy(X,y,\"gene_expression\",\"clf\",seed)\n",
        "def load_network_topology(seed=0):\n",
        "    rs=check_random_state(seed); n=3000\n",
        "    deg=rs.gamma(2.0,2.0,n); cl=rs.beta(2,5,n); asst=rs.uniform(-0.5,0.5,n); tri=rs.poisson(5,n); btw=rs.exponential(1.0,n)\n",
        "    X=np.c_[deg,cl,asst,tri,btw]; y=(0.3*deg+0.8*cl-0.5*asst+0.1*tri+0.2*btw+rs.randn(n)*0.5>1.8).astype(int)\n",
        "    return _split_pack_from_xy(X,y,\"network_topology\",\"clf\",seed)\n",
        "def load_audio_mfcc(seed=0):\n",
        "    rs=check_random_state(seed); n=3500; bands=20; y=rs.randint(0,5,size=n); base=rs.randn(n,bands)\n",
        "    for c in range(5): base[y==c]+=(c-2)*0.25\n",
        "    X=base+0.1*rs.randn(n,bands); return _split_pack_from_xy(X,y,\"audio_mfcc\",\"clf\",seed)\n",
        "def load_weather_station(seed=0):\n",
        "    rs=check_random_state(seed); n=5000\n",
        "    pressure=rs.normal(1013,8,n); humidity=rs.uniform(15,95,n); wind=rs.gamma(2.0,1.2,n); cloud=rs.uniform(0,1,n); doy=rs.randint(1,366,n)\n",
        "    temp = 10 + 10*np.sin(2*np.pi*doy/365) - 0.01*(pressure-1013) - 0.05*(humidity-50) - 0.3*cloud + 0.2*wind + rs.randn(n)\n",
        "    X=np.c_[pressure,humidity,wind,cloud,doy]; y=temp; return _split_pack_from_xy(X,y,\"weather_station\",\"reg\",seed)\n",
        "def load_eeg_signals(seed=0): X,y=_gen_ts_dataset(3000,256,4,seed,\"clf\"); return _split_pack_from_xy(X,y,\"eeg_signals\",\"clf\",seed)\n",
        "def load_satellite_ndvi(seed=0):\n",
        "    rs=check_random_state(seed); n=4000\n",
        "    bands=rs.uniform(0,1,(n,6)); sun=rs.uniform(0,70,n); view=rs.uniform(0,30,n)\n",
        "    ndvi=(bands[:,3]-bands[:,2])/(bands[:,3]+bands[:,2]+1e-6); y=ndvi+0.001*(sun-35)-0.001*(view-15)+rs.randn(n)*0.02\n",
        "    X=np.c_[bands,sun,view]; return _split_pack_from_xy(X,y,\"satellite_ndvi\",\"reg\",seed)\n",
        "def load_industrial_process(seed=0):\n",
        "    rs=check_random_state(seed); n=4500\n",
        "    s1=rs.normal(0,1,n); s2=rs.normal(0,1.2,n); s3=rs.normal(0.5,0.8,n); sp=rs.uniform(-1,1,n); noise=rs.randn(n)*0.3\n",
        "    y=2.0+1.5*s1-0.7*s2+0.9*s3+1.2*sp+noise; X=np.c_[s1,s2,s3,sp]\n",
        "    return _split_pack_from_xy(X,y,\"industrial_process\",\"reg\",seed)\n",
        "def load_social_network(seed=0):\n",
        "    rs=check_random_state(seed); n=5000\n",
        "    posts=rs.poisson(3,n); friends=rs.poisson(120,n); likes=rs.poisson(50,n); comments=rs.poisson(12,n); shares=rs.poisson(5,n)\n",
        "    X=np.c_[posts,friends,likes,comments,shares]\n",
        "    y=(0.01*friends+0.03*likes+0.05*comments+0.08*shares+0.2*posts+rs.randn(n)*0.5>3.0).astype(int)\n",
        "    return _split_pack_from_xy(X,y,\"social_network\",\"clf\",seed)\n",
        "def load_cyber_security(seed=0):\n",
        "    X,y=make_classification(5000,30,8,10,weights=[0.8,0.2],class_sep=1.5,random_state=seed)\n",
        "    return _split_pack_from_xy(X,y,\"cyber_security\",\"clf\",seed)\n",
        "def load_mixed_modal(seed=0):\n",
        "    rs=check_random_state(seed)\n",
        "    X_tab,y=make_classification(4000,20,6,6,random_state=seed); X_spec=[]\n",
        "    for _ in range(X_tab.shape[0]):\n",
        "        t=np.arange(64); sig=np.sin(2*np.pi*(0.08+0.1*rs.rand())*t)+0.3*rs.randn(64)\n",
        "        X_spec.append(_ts_to_feats(sig))\n",
        "    X=np.c_[X_tab,np.vstack(X_spec)]; return _split_pack_from_xy(X,y,\"mixed_modal\",\"clf\",seed)\n",
        "\n",
        "ALL_DATASETS = {\n",
        "    \"adult\":load_adult,\"20ng_bin\":load_20ng_binary,\"har6\":load_har,\"digits8x8\":load_digits8x8,\n",
        "    \"california\":load_california,\"diabetes\":load_diabetes,\"iris\":load_iris,\"wine\":load_wine,\n",
        "    \"synthetic_clf\":load_synthetic_clf,\"synthetic_reg\":load_synthetic_reg,\n",
        "    \"ecg_heartbeat\":load_ecg_heartbeat,\"fashion_mnist_features\":load_fashion_mnist_features,\n",
        "    \"protein_structure\":load_protein_structure,\"stock_technical\":load_stock_technical,\n",
        "    \"sensor_fusion\":load_sensor_fusion,\"gene_expression\":load_gene_expression,\n",
        "    \"network_topology\":load_network_topology,\"audio_mfcc\":load_audio_mfcc,\n",
        "    \"weather_station\":load_weather_station,\"eeg_signals\":load_eeg_signals,\n",
        "    \"satellite_ndvi\":load_satellite_ndvi,\"industrial_process\":load_industrial_process,\n",
        "    \"social_network\":load_social_network,\"cyber_security\":load_cyber_security,\"mixed_modal\":load_mixed_modal,\n",
        "}\n",
        "\n",
        "# ===================== CSV helpers =====================\n",
        "def ensure_out(outdir=\"out\"): os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "def append_rows(path: str, header: List[str], rows: List[List]):\n",
        "    write_header = not os.path.exists(path)\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        wr = csv.writer(f)\n",
        "        if write_header: wr.writerow(header)\n",
        "        wr.writerows(rows)\n",
        "\n",
        "# ===================== main per-dataset pipeline =====================\n",
        "def run_on_dataset(pack: DatasetPack, ks=(3,5,8,12), seed: int = 0,\n",
        "                   do_pfi: bool = True, outdir=\"out\") -> Dict[str, np.ndarray]:\n",
        "    ensure_out(outdir)\n",
        "    print(f\"[{pack.name}] Running benchmark (task={pack.task}, N={pack.Xtr.shape[0]+pack.Xva.shape[0]+pack.Xte.shape[0]}, D={pack.Xtr.shape[1]})...\")\n",
        "\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    if base is None:\n",
        "        print(f\"[{pack.name}] skip: base model failed.\"); return {}\n",
        "\n",
        "    yhat_test = base.predict(pack.Xte.astype(np.float32))\n",
        "    base_metric = accuracy_score(pack.yte, yhat_test) if pack.task==\"clf\" else r2_score(pack.yte, yhat_test)\n",
        "    append_rows(os.path.join(outdir, \"base_model_metrics.csv\"),\n",
        "                [\"dataset\",\"task\",\"test_metric_name\",\"test_metric_value\"],\n",
        "                [[pack.name, pack.task, \"accuracy\" if pack.task==\"clf\" else \"r2\", base_metric]])\n",
        "\n",
        "    p_va = predict_scores(pack.task, base, pack.Xva)\n",
        "    scores: Dict[str, np.ndarray] = {\n",
        "        \"ExCIR\":        excir_feature_scores(pack.Xva, p_va),\n",
        "        \"TreeGain\":     tree_gain_scores(base, pack.Xva.shape[1]),\n",
        "        \"PDP-var\":      pdp_var_scores(base, pack.Xva),\n",
        "        \"MI(pred)\":     mi_pred_scores(pack.Xva, p_va, task=pack.task),\n",
        "        \"MI(label)\":    mi_label_scores(pack.Xva, pack.yva, task=pack.task),\n",
        "        \"Surrogate-LR\": surrogate_lr_scores(pack.Xva, p_va),\n",
        "    }\n",
        "    if do_pfi:\n",
        "        scores[\"PFI\"] = pfi_scores(base, pack.Xva, pack.yva, n_repeats=5, seed=seed)\n",
        "\n",
        "    # 1) feature_importances.csv\n",
        "    feat_rows = []\n",
        "    for mname, vec in scores.items():\n",
        "        for j, val in enumerate(vec):\n",
        "            feat_rows.append([pack.name, mname, j,\n",
        "                              pack.feature_names[j] if j < len(pack.feature_names) else f\"f{j}\",\n",
        "                              float(val)])\n",
        "    append_rows(os.path.join(outdir, \"feature_importances.csv\"),\n",
        "                [\"dataset\",\"method\",\"feature_index\",\"feature_name\",\"score\"],\n",
        "                feat_rows)\n",
        "\n",
        "    # 2) AOPC (curves + summary)\n",
        "    curves_rows, summary_rows = [], []\n",
        "    for mname, scr in scores.items():\n",
        "        fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scr, steps=9)\n",
        "        for f, a, d in zip(fr, ins, dele):\n",
        "            curves_rows.append([pack.name, mname, float(f), float(a), float(d)])\n",
        "        auc_ins = float(np.trapz(ins, fr)); auc_del = float(np.trapz(dele, fr))\n",
        "        combined = 0.5*(auc_ins + (1.0 - auc_del))\n",
        "        summary_rows.append([pack.name, mname, auc_ins, auc_del, combined])\n",
        "    append_rows(os.path.join(outdir, \"aopc_curves.csv\"),\n",
        "                [\"dataset\",\"method\",\"fraction_kept\",\"insertion\",\"deletion\"],\n",
        "                curves_rows)\n",
        "    append_rows(os.path.join(outdir, \"aopc_summary.csv\"),\n",
        "                [\"dataset\",\"method\",\"auc_insertion\",\"auc_deletion\",\"combined\"],\n",
        "                summary_rows)\n",
        "\n",
        "    # 3) top-k sufficiency\n",
        "    topk_rows = []\n",
        "    for m, v in scores.items():\n",
        "        ranks = rank_from_scores(v)\n",
        "        for k in ks:\n",
        "            keep = ranks[:k]\n",
        "            params = {\n",
        "                'n_estimators': 100, 'random_state': seed,\n",
        "                'tree_method': 'gpu_hist', 'gpu_id': 0,\n",
        "                'validate_parameters': True, 'n_jobs': 1,\n",
        "            }\n",
        "            Xtr_k = np.vstack([pack.Xtr, pack.Xva])[:, keep].astype(np.float32)\n",
        "            ytr_k = np.r_[pack.ytr, pack.yva]\n",
        "            model_k = _get_xgb_model(pack.task, ytr_k, params)\n",
        "            try:\n",
        "                model_k.fit(Xtr_k, ytr_k)\n",
        "            except xgb.core.XGBoostError:\n",
        "                params['tree_method'] = 'hist'; params.pop('gpu_id', None)\n",
        "                model_k = _get_xgb_model(pack.task, ytr_k, params)\n",
        "                try: model_k.fit(Xtr_k, ytr_k)\n",
        "                except Exception: model_k = None\n",
        "            except Exception:\n",
        "                model_k = None\n",
        "\n",
        "            metric_val = np.nan\n",
        "            if model_k is not None:\n",
        "                try:\n",
        "                    yhat = model_k.predict(pack.Xte[:, keep].astype(np.float32))\n",
        "                    metric_val = accuracy_score(pack.yte, yhat) if pack.task==\"clf\" else r2_score(pack.yte, yhat)\n",
        "                except Exception:\n",
        "                    metric_val = np.nan\n",
        "            topk_rows.append([pack.name, m, k, \"accuracy\" if pack.task==\"clf\" else \"r2\", metric_val])\n",
        "    append_rows(os.path.join(outdir, \"summary_topk.csv\"),\n",
        "                [\"dataset\",\"method\",\"k\",\"metric_name\",\"metric_value\"],\n",
        "                topk_rows)\n",
        "\n",
        "    # 4) Stability (CIR & Jaccard@8)\n",
        "    cirs, j8 = stability_noise_eval(lambda X, p: excir_feature_scores(X, p), pack.Xva, p_va, repeats=15, sigma=0.01)\n",
        "    stab_rows = [[pack.name, int(i), float(c), float(j)] for i, (c, j) in enumerate(zip(cirs, j8))]\n",
        "    append_rows(os.path.join(outdir, \"stability_noise.csv\"),\n",
        "                [\"dataset\",\"repeat_idx\",\"cir_under_noise\",\"jaccard_at_8_under_noise\"],\n",
        "                stab_rows)\n",
        "\n",
        "    # (Optional plots)\n",
        "    plt.figure(figsize=(6.6,4.0))\n",
        "    plt.subplot(1,2,1); plt.hist(cirs, bins=10); plt.title('CIR under noise'); plt.xlabel('CIR'); plt.ylabel('count')\n",
        "    plt.subplot(1,2,2); plt.hist(j8, bins=10); plt.title('Jaccard@8 under noise'); plt.xlabel('overlap')\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(outdir, f\"stability_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    F, T, CIRs, Resid, KLs, J8 = lightweight_three_checks(pack, seed=seed)\n",
        "    lw_rows = [[pack.name, float(f), float(t), float(cir), float(res), float(kl), float(j)]\n",
        "               for f, t, cir, res, kl, j in zip(F, T, CIRs, Resid, KLs, J8)]\n",
        "    append_rows(os.path.join(outdir, \"lightweight_checks.csv\"),\n",
        "                [\"dataset\",\"fraction_rows\",\"wall_time_sec\",\"cir_agreement\",\"proj_residual\",\"sym_kl\",\"jaccard_at_8\"],\n",
        "                lw_rows)\n",
        "\n",
        "    # (Optional visual bundle)\n",
        "    plt.figure(figsize=(10, 3.8))\n",
        "    ax1 = plt.subplot(1,3,1); ax1.plot((100*F).astype(int), CIRs, marker='o'); ax1.set_ylim(0,1); ax1.set_xlabel('% rows kept'); ax1.set_ylabel('CIR(full, light)'); ax1.set_title('Agreement (CIR)'); ax1.grid(alpha=0.3)\n",
        "    ax2 = plt.subplot(1,3,2); ax2.plot((100*F).astype(int), Resid, marker='o'); ax2.set_xlabel('% rows kept'); ax2.set_ylabel('Projection residual'); ax2.set_title('Shape agreement'); ax2.grid(alpha=0.3)\n",
        "    ax3 = plt.subplot(1,3,3); ax3.plot((100*F).astype(int), KLs, marker='o'); ax3.set_xlabel('% rows kept'); ax3.set_ylabel('Sym. KL (KDE)'); ax3.set_title('Distribution match'); ax3.grid(alpha=0.3)\n",
        "    plt.suptitle(f'Agreement beyond ranks — {pack.name}', y=1.02, fontsize=12); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"agreement_bundle_{pack.name}.png\"), dpi=160, bbox_inches='tight'); plt.close()\n",
        "\n",
        "    plt.figure(figsize=(6.5,5.0))\n",
        "    scatter = plt.scatter(T, CIRs, s=300*np.maximum(J8, 0.05), alpha=0.7, c=J8, cmap='viridis')\n",
        "    for i, f in enumerate(F): plt.annotate(f\"{int(100*f)}%\", (T[i], CIRs[i]), xytext=(5,5), textcoords='offset points', fontsize=9)\n",
        "    plt.xlabel('Wall time (s)'); plt.ylabel('CIR(full, light)'); plt.title(f'Agreement–cost (ExCIR) — {pack.name}')\n",
        "    plt.colorbar(scatter, label='Jaccard@8'); plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"pareto_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    plt.figure(figsize=(6.2,4.2))\n",
        "    plt.plot((100*F).astype(int), T, marker='o'); plt.xlabel('Fraction of rows kept (%)'); plt.ylabel('Wall time (s)')\n",
        "    plt.title(f'Runtime vs fraction — {pack.name}'); plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"runtime_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # 5) Special: digits class-conditioned ExCIR\n",
        "    if pack.name == \"digits8x8\":\n",
        "        clf = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs', random_state=seed)\n",
        "        clf.fit(np.vstack([pack.Xtr, pack.Xva]), np.r_[pack.ytr, pack.yva])\n",
        "        Pva = clf.predict_proba(pack.Xva)\n",
        "        s_c = excir_feature_scores(pack.Xva, Pva[:, 3])  # class 3\n",
        "        H = s_c.reshape(8,8)\n",
        "        append_rows(os.path.join(outdir, \"digits_excir_class3_map.csv\"),\n",
        "                    [\"row0\",\"row1\",\"row2\",\"row3\",\"row4\",\"row5\",\"row6\",\"row7\"],\n",
        "                    [list(map(float, row)) for row in H])\n",
        "\n",
        "        plt.figure(figsize=(4.5,4.5)); plt.imshow(H, cmap='viridis'); plt.colorbar()\n",
        "        plt.title('Class-conditioned ExCIR (digit 3)'); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_excir_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "        fr2, ins2, dele2 = aopc_curves(\"clf\", clf, pack.Xte, pack.yte, s_c, steps=9)\n",
        "        plt.figure(figsize=(6.2,4.2))\n",
        "        plt.plot(100*fr2, ins2, marker='o', label='Insertion (keep top-% pixels)')\n",
        "        plt.plot(100*fr2, dele2, marker='o', label='Deletion (zero top-% pixels)')\n",
        "        plt.xlabel('Revealed / removed top-% pixels'); plt.ylabel('Test Accuracy')\n",
        "        plt.title('AOPC — digits (class 3 map)'); plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_aopc_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ===================== CLI =====================\n",
        "def _run_everything(datasets, seed, ks_tuple, outdir, do_pfi):\n",
        "    ensure_out(outdir)\n",
        "    names = [s.strip() for s in datasets.split(\",\") if s.strip()]\n",
        "    if not names or 'all' in [n.lower() for n in names]:\n",
        "        names = list(ALL_DATASETS.keys())\n",
        "\n",
        "    print(f\"--- Running ExCIR Benchmark ({len(names)} datasets, Seed={seed}) ---\")\n",
        "    print(\"**NOTE: Using XGBoost with GPU acceleration where possible, falling back to CPU if necessary.**\")\n",
        "    print(f\"Datasets: {', '.join(names)}\")\n",
        "    print(f\"Output Directory: {outdir}\")\n",
        "    print(f\"PFI: {'Enabled' if do_pfi else 'Disabled'}\")\n",
        "\n",
        "    for name in names:\n",
        "        if name not in ALL_DATASETS:\n",
        "            print(f\"[Warning] Dataset '{name}' not found. Skipping.\")\n",
        "            continue\n",
        "        try:\n",
        "            pack = ALL_DATASETS[name](seed=seed)\n",
        "            run_on_dataset(pack, ks=ks_tuple, seed=seed, do_pfi=do_pfi, outdir=outdir)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to run dataset '{name}': {e}\")\n",
        "            import traceback; traceback.print_exc(file=sys.stdout)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Enhanced ExCIR — cross-domain benchmarks for feature importance.\",\n",
        "        formatter_class=argparse.RawTextHelpFormatter\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--datasets\",\n",
        "        type=str,\n",
        "        default=\"all\",\n",
        "        help=\"Comma-separated list of datasets to run (e.g., 'adult,diabetes,iris'). Use 'all' for everything.\"\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed (default: 0)\")\n",
        "    parser.add_argument(\"--ks\", type=lambda s: tuple(map(int, s.split(','))), default=\"3,5,8,12\",\n",
        "                        help=\"Comma-separated top-k values for sufficiency (default: 3,5,8,12)\")\n",
        "    parser.add_argument(\"--outdir\", type=str, default=\"out\",\n",
        "                        help=\"Output directory for results (default: out)\")\n",
        "    parser.add_argument(\"--no-pfi\", action=\"store_true\",\n",
        "                        help=\"Skip PFI for faster execution\")\n",
        "\n",
        "    # Jupyter/Colab: strip the '-f' arg injected by the kernel\n",
        "    if '-f' in sys.argv:\n",
        "        try:\n",
        "            f_index = sys.argv.index('-f')\n",
        "            sys.argv.pop(f_index)            # remove '-f'\n",
        "            if f_index < len(sys.argv):       # remove its value if present\n",
        "                sys.argv.pop(f_index)\n",
        "        except Exception:\n",
        "            try:\n",
        "                sys.argv.remove('-f')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    _run_everything(\n",
        "        datasets=args.datasets,\n",
        "        seed=args.seed,\n",
        "        ks_tuple=args.ks,\n",
        "        outdir=args.outdir,\n",
        "        do_pfi=not args.no_pfi\n",
        "    )\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "kVQRk1j3ydR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv, math, random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Load long-format data ----------\n",
        "path = \"/content/feature_importance_long.csv\"\n",
        "by_method = defaultdict(list)   # method -> list of scores\n",
        "points = []                     # (method_idx, score, feature)\n",
        "\n",
        "order = [\"CIR_full\", \"CIR_lw\", \"SHAP\"]  # left→right order on x-axis\n",
        "method_to_ix = {m:i for i,m in enumerate(order)}\n",
        "\n",
        "with open(path, newline=\"\") as f:\n",
        "    r = csv.DictReader(f)\n",
        "    for row in r:\n",
        "        m = row[\"method\"]\n",
        "        s = float(row[\"score\"])\n",
        "        feat = row[\"feature\"]\n",
        "        if m not in method_to_ix:\n",
        "            continue\n",
        "        by_method[m].append(s)\n",
        "        points.append((method_to_ix[m], s, feat))\n",
        "\n",
        "# Safety: ensure each method exists\n",
        "for m in order:\n",
        "    if m not in by_method or len(by_method[m]) == 0:\n",
        "        by_method[m] = [np.nan]\n",
        "\n",
        "# ---------- Build figure ----------\n",
        "fig, ax = plt.subplots(figsize=(8.5, 4.5))\n",
        "\n",
        "# Violin distribution per method (scores across features)\n",
        "data = [np.array(by_method[m]) for m in order]\n",
        "v = ax.violinplot(\n",
        "    dataset=data,\n",
        "    positions=np.arange(len(order)),\n",
        "    showmeans=False,\n",
        "    showmedians=True,\n",
        "    showextrema=False,\n",
        "    widths=0.8\n",
        ")\n",
        "\n",
        "# Make violins translucent without picking a specific color\n",
        "for b in v['bodies']:\n",
        "    b.set_alpha(0.25)\n",
        "if 'cmedians' in v:\n",
        "    v['cmedians'].set_linewidth(1.5)\n",
        "\n",
        "# Beeswarm (simple jitter) per method\n",
        "def beeswarm_x(x_center, n, width=0.36, seed=7):\n",
        "    # symmetric jitter, denser near center\n",
        "    rng = random.Random(seed)\n",
        "    return [x_center + (rng.random()*2-1) * width * (0.7 + 0.3*rng.random()) for _ in range(n)]\n",
        "\n",
        "# group points by method index for jittering\n",
        "pts_by_ix = defaultdict(list)\n",
        "for ix, s, feat in points:\n",
        "    pts_by_ix[ix].append((s, feat))\n",
        "\n",
        "for ix in range(len(order)):\n",
        "    vals = pts_by_ix[ix]\n",
        "    xs = beeswarm_x(ix, len(vals), width=0.36, seed=ix+13)\n",
        "    ys = [v for v,_ in vals]\n",
        "    ax.scatter(xs, ys, s=22, alpha=0.8, edgecolors='none')\n",
        "\n",
        "# Cosmetics\n",
        "ax.set_xticks(np.arange(len(order)))\n",
        "ax.set_xticklabels([\"CIR (full)\", \"CIR (LW)\", \"SHAP\"])\n",
        "ax.set_ylabel(\"Feature importance score\")\n",
        "ax.set_title(\"Combined importance: Violin (distribution across features) + Beeswarm (per-feature points)\")\n",
        "ax.set_ylim(bottom=0)  # scores are non-negative\n",
        "ax.grid(axis='y', linestyle=':', linewidth=0.6, alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"combined_importance_violin_beeswarm.png\", dpi=300)\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "0wfvV1AWymWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a VisExp-style \"strip + beeswarm + soft density ridge + mean arrow\" figure\n",
        "# for the three methods using the long-format CSV prepared earlier.\n",
        "#\n",
        "# Constraints followed:\n",
        "# - Uses matplotlib only (no seaborn)\n",
        "# - Single figure (no subplots)\n",
        "# - Does not explicitly set colors; relies on defaults\n",
        "#\n",
        "# Input: /mnt/data/feature_importance_long.csv\n",
        "# Output: /mnt/data/visexp_strip_beeswarm.png\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "csv_path = \"/content/feature_importance_long.csv\"\n",
        "assert os.path.exists(csv_path), \"CSV not found. Please create feature_importance_long.csv first.\"\n",
        "\n",
        "# Load data\n",
        "records = []\n",
        "with open(csv_path, newline=\"\") as f:\n",
        "    r = csv.DictReader(f)\n",
        "    for row in r:\n",
        "        records.append((row[\"feature\"], row[\"method\"], float(row[\"score\"])))\n",
        "\n",
        "order = [\"CIR_full\", \"CIR_lw\", \"SHAP\"]\n",
        "label_map = {\"CIR_full\": \"CIR (full)\", \"CIR_lw\": \"CIR (LW)\", \"SHAP\": \"SHAP\"}\n",
        "by_method = defaultdict(list)\n",
        "for feat, meth, score in records:\n",
        "    if meth in order:\n",
        "        by_method[meth].append((feat, score))\n",
        "\n",
        "# KDE helper (simple Gaussian KDE without SciPy)\n",
        "def kde_smooth(x, grid, bw=None):\n",
        "    x = np.asarray(x)\n",
        "    grid = np.asarray(grid)\n",
        "    if bw is None:\n",
        "        # Silverman's rule of thumb (univariate)\n",
        "        std = np.std(x, ddof=1) if len(x) > 1 else 1e-3\n",
        "        n = max(len(x), 2)\n",
        "        bw = 1.06 * std * (n ** (-1/5))\n",
        "        if bw <= 0:\n",
        "            bw = max(std, 1e-3) * 0.3\n",
        "    # Compute kernel density\n",
        "    diffs = (grid[:, None] - x[None, :]) / bw\n",
        "    kern = np.exp(-0.5 * diffs**2) / (np.sqrt(2*np.pi) * bw)\n",
        "    dens = np.mean(kern, axis=1)\n",
        "    return dens\n",
        "\n",
        "# Prepare canvas\n",
        "plt.figure(figsize=(10, 5.2))\n",
        "ax = plt.gca()\n",
        "\n",
        "# Global x-range from all scores\n",
        "all_scores = [s for _,_,s in records]\n",
        "xmin = 0.0\n",
        "xmax = max(1.0, max(all_scores) * 1.05)\n",
        "\n",
        "# Layout\n",
        "y_gap = 1.4   # vertical spacing between rows\n",
        "y0 = 0.0\n",
        "yticks = []\n",
        "yticklabels = []\n",
        "\n",
        "# Beeswarm helper along y (stack points to avoid overlap)\n",
        "def beeswarm_positions(xs, y_center, radius=0.012, max_iter=200):\n",
        "    # Place points around y_center to avoid vertical overlap\n",
        "    # xs: list of x values (we sort to get stable packing)\n",
        "    idx = np.argsort(xs)\n",
        "    xs_sorted = np.array(xs)[idx]\n",
        "    ys = np.ones_like(xs_sorted) * y_center\n",
        "    # Greedy stacking\n",
        "    for i in range(len(xs_sorted)):\n",
        "        conflict = True\n",
        "        tries = 0\n",
        "        y = y_center\n",
        "        # alternate up/down\n",
        "        direction = 1.0\n",
        "        step = radius * 1.2\n",
        "        while conflict and tries < max_iter:\n",
        "            conflict = False\n",
        "            for j in range(i):\n",
        "                dx = abs(xs_sorted[i] - xs_sorted[j])\n",
        "                dy = abs(y - ys[j])\n",
        "                # simple proximity in x to decide vertical spacing\n",
        "                if dx < radius*2 and dy < radius*2:\n",
        "                    conflict = True\n",
        "                    break\n",
        "            if conflict:\n",
        "                y = y_center + direction * step\n",
        "                direction *= -1.0\n",
        "                step += radius * 0.6\n",
        "                tries += 1\n",
        "        ys[i] = y\n",
        "    # unsort back\n",
        "    inv = np.zeros_like(idx)\n",
        "    inv[idx] = np.arange(len(idx))\n",
        "    ys_unsorted = ys[inv]\n",
        "    return ys_unsorted\n",
        "\n",
        "# Draw each method row\n",
        "grid = np.linspace(xmin, xmax, 400)\n",
        "for row_i, meth in enumerate(order):\n",
        "    y_c = y0 + row_i * y_gap\n",
        "    yticks.append(y_c)\n",
        "    yticklabels.append(label_map[meth])\n",
        "\n",
        "    feats_scores = by_method[meth]\n",
        "    if not feats_scores:\n",
        "        continue\n",
        "    feats = [f for f,_ in feats_scores]\n",
        "    xs = np.array([s for _,s in feats_scores])\n",
        "\n",
        "    # Baseline line\n",
        "    ax.hlines(y_c, xmin, xmax, linewidth=1.0)\n",
        "\n",
        "    # Density ridge (scaled)\n",
        "    dens = kde_smooth(xs, grid)\n",
        "    # scale density to a reasonable thickness\n",
        "    if dens.max() > 0:\n",
        "        dens = dens / dens.max() * (y_gap * 0.35)\n",
        "    ax.fill_between(grid, y_c - dens, y_c + dens, alpha=0.25)\n",
        "\n",
        "    # Beeswarm points\n",
        "    y_swarm = beeswarm_positions(xs.tolist(), y_c, radius=0.014)\n",
        "    ax.scatter(xs, y_swarm, s=18, alpha=0.85, edgecolors='none')\n",
        "\n",
        "    # Mean arrow\n",
        "    x_mean = float(np.mean(xs))\n",
        "    ax.annotate(\n",
        "        \"\", xy=(x_mean, y_c), xytext=(xmin, y_c),\n",
        "        arrowprops=dict(arrowstyle=\"->\", lw=1.5)\n",
        "    )\n",
        "    # Mean marker\n",
        "    ax.plot([x_mean], [y_c], marker=\"s\", markersize=6)\n",
        "\n",
        "    # Optional: annotate top-3 features for this method (right side)\n",
        "    top_idx = np.argsort(xs)[-3:][::-1]\n",
        "    for k in top_idx:\n",
        "        ax.text(xs[k], y_swarm[k] + 0.05, feats[k], fontsize=8, ha=\"center\", va=\"bottom\")\n",
        "\n",
        "# Axes & labels\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_yticks(yticks)\n",
        "ax.set_yticklabels(yticklabels)\n",
        "ax.set_xlabel(\"Feature importance score\")\n",
        "ax.set_title(\"VisExp-style strips: density ridge + beeswarm + mean arrow (per method)\")\n",
        "ax.grid(axis='x', linestyle=':', linewidth=0.6, alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "out_path = \"visexp_strip_beeswarm.png\"\n",
        "plt.savefig(out_path, dpi=300)\n",
        "plt.close()\n",
        "\n",
        "out_path\n"
      ],
      "metadata": {
        "id": "kMKzhHNjyqWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visexp_methods_combined.py\n",
        "# Matplotlib-only, one panel. Reads 3 CSVs and outputs visexp_methods_combined.png\n",
        "\n",
        "import csv, numpy as np, matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- Load CSVs ---\n",
        "def read_map(path, key_col, val_col):\n",
        "    m = {}\n",
        "    with open(path, newline=\"\") as f:\n",
        "        r = csv.DictReader(f)\n",
        "        for row in r:\n",
        "            if row[key_col] != \"\" and row[val_col] != \"\":\n",
        "                m[row[key_col]] = float(row[val_col])\n",
        "    return m\n",
        "\n",
        "cir_full  = read_map(\"/content/cir_scores.csv\",  \"feature\", \"cir_full\")\n",
        "cir_lw    = read_map(\"/content/cir_scores.csv\",  \"feature\", \"cir_lw\")\n",
        "shap_map  = read_map(\"/content/shap_scores.csv\", \"feature\", \"shap\")\n",
        "lime_map  = read_map(\"/content/lime_scores.csv\", \"feature\", \"lime\")\n",
        "\n",
        "methods = {\n",
        "    \"CIR_full\": cir_full,\n",
        "    \"CIR_lw\":   cir_lw,\n",
        "    \"SHAP\":     shap_map,\n",
        "    \"LIME\":     lime_map,\n",
        "}\n",
        "labels = {\"CIR_full\":\"CIR (full)\", \"CIR_lw\":\"CIR (LW)\", \"SHAP\":\"SHAP\", \"LIME\":\"LIME\"}\n",
        "\n",
        "# --- Prep long lists (method-wise vectors) ---\n",
        "by_method = {}\n",
        "for m, mp in methods.items():\n",
        "    feats = list(mp.keys())\n",
        "    vals  = np.array([mp[f] for f in feats], dtype=float)\n",
        "    by_method[m] = (feats, vals)\n",
        "\n",
        "# --- Simple KDE (univariate Gaussian) ---\n",
        "def kde(grid, x, bw=None):\n",
        "    x = np.asarray(x)\n",
        "    if len(x) < 2:\n",
        "        return np.zeros_like(grid)\n",
        "    std = np.std(x, ddof=1)\n",
        "    if bw is None:\n",
        "        bw = 1.06 * (std if std>0 else 1e-3) * (len(x) ** (-1/5))\n",
        "    bw = max(bw, 1e-3)\n",
        "    z = (grid[:,None] - x[None,:]) / bw\n",
        "    dens = np.exp(-0.5*z*z) / (np.sqrt(2*np.pi)*bw)\n",
        "    return np.mean(dens, axis=1)\n",
        "\n",
        "# --- Beeswarm y-positions to avoid overlaps ---\n",
        "def beeswarm_y(xs, y0, r=0.012):\n",
        "    xs = np.asarray(xs); order = np.argsort(xs)\n",
        "    ys = np.full_like(xs, y0, dtype=float)\n",
        "    for idx_i, i in enumerate(order):\n",
        "        y, step, flip = y0, r*1.2, 1.0\n",
        "        while True:\n",
        "            conflict = False\n",
        "            for jdx_j in range(idx_i):\n",
        "                j = order[jdx_j]\n",
        "                if abs(xs[i]-xs[j]) < r*2 and abs(y-ys[j]) < r*2:\n",
        "                    conflict = True; break\n",
        "            if not conflict: break\n",
        "            y = y0 + flip*step; step += r*0.6; flip *= -1.0\n",
        "        ys[i] = y\n",
        "    return ys\n",
        "\n",
        "# --- Figure (normalized per method so shapes are comparable) ---\n",
        "fig, ax = plt.subplots(figsize=(11.5, 6.0))\n",
        "y_gap, x_grid = 1.6, np.linspace(0, 1, 400)\n",
        "yticks, ylabels = [], []\n",
        "\n",
        "# light x guides\n",
        "for gx in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
        "    ax.axvline(gx, linestyle=\":\", linewidth=0.6, alpha=0.5)\n",
        "\n",
        "for row, m in enumerate([\"CIR_full\",\"CIR_lw\",\"SHAP\",\"LIME\"]):\n",
        "    feats, vals = by_method[m]\n",
        "    # normalize per method to [0,1] for readability\n",
        "    vmin, vmax = float(np.min(vals)), float(np.max(vals))\n",
        "    span = (vmax - vmin) if (vmax - vmin)>1e-12 else 1.0\n",
        "    norm = (vals - vmin) / span\n",
        "\n",
        "    y0 = row * y_gap\n",
        "    yticks.append(y0); ylabels.append(labels[m])\n",
        "\n",
        "    # baseline\n",
        "    ax.hlines(y0, 0, 1, linewidth=1.0)\n",
        "\n",
        "    # ridge (density)\n",
        "    dens = kde(x_grid, norm)\n",
        "    if dens.max() > 0:\n",
        "        dens = dens / dens.max() * (y_gap*0.33)\n",
        "    ax.plot([], [])       # advance color cycle slightly\n",
        "    ax.fill_between(x_grid, y0 - dens, y0 + dens, alpha=0.22)\n",
        "\n",
        "    # IQR + median\n",
        "    q1,q2,q3 = np.percentile(norm, [25,50,75])\n",
        "    ax.plot([q1,q3], [y0,y0], linewidth=6, alpha=0.6)\n",
        "    ax.plot([q2], [y0], marker=\"o\", markersize=6)\n",
        "\n",
        "    # mean arrow\n",
        "    mu = float(np.mean(norm))\n",
        "    ax.annotate(\"\", xy=(mu, y0), xytext=(0.0, y0), arrowprops=dict(arrowstyle=\"->\", lw=1.4))\n",
        "    ax.plot([mu],[y0], marker=\"s\", markersize=6)\n",
        "\n",
        "    # beeswarm (bigger markers = higher rank)\n",
        "    ranks = np.argsort(np.argsort(-norm)) + 1\n",
        "    sizes = 16 + (60 - ranks)*0.25\n",
        "    ys = beeswarm_y(norm, y0, r=0.014)\n",
        "    ax.scatter(norm, ys, s=sizes, alpha=0.9, edgecolors='none')\n",
        "\n",
        "    # annotate top-3 features\n",
        "    top3 = np.argsort(norm)[-3:][::-1]\n",
        "    for k in top3:\n",
        "        ax.text(norm[k], ys[k]+0.06, feats[k], fontsize=8, ha=\"center\", va=\"bottom\")\n",
        "\n",
        "ax.set_xlim(0,1)\n",
        "ax.set_ylim(-0.8, (len(yticks)-1)*y_gap + 0.8)\n",
        "ax.set_yticks(yticks); ax.set_yticklabels(ylabels)\n",
        "ax.set_xlabel(\"Normalized importance (per method)\")\n",
        "ax.set_title(\"CIR (full & LW) vs SHAP vs LIME — VisExp-style strip\")\n",
        "ax.grid(axis='y', linestyle=':', linewidth=0.6, alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"visexp_methods_combined.png\", dpi=300, bbox_inches=\"tight\")\n",
        "print(\"Saved visexp_methods_combined.png\")\n"
      ],
      "metadata": {
        "id": "G631WK7kysob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multioutput_excir_digits_fixed.py\n",
        "# End-to-end multi-output ExCIR on scikit-learn digits with robust preprocessing\n",
        "# - Choose feature scaling that DOES NOT mean-center (minmax or std_no_center)\n",
        "# - Use LOGITS as multi-output Y\n",
        "# - Multi-output ExCIR via canonical Y-direction w = (Σ_Y + λI)^(-1) cov(Y,f)\n",
        "# - Optional 2x2 patch features (recommended for cleaner spatial signal)\n",
        "# - Remix invariance checks\n",
        "# - Saves heatmaps + CSVs\n",
        "\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# Config\n",
        "# -------------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    feature_scaler: str = \"minmax\"   # \"minmax\" (0..1) or \"std_no_center\"\n",
        "    use_patches: bool = False        # True -> 2x2 patch features (recommended)\n",
        "    ridge: float = 1e-5              # ridge for Σ_Y inversion\n",
        "    remix_trials: int = 10           # number of random full-rank remixes\n",
        "    seed: int = 0\n",
        "\n",
        "C = Config(\n",
        "    feature_scaler=\"minmax\",   # try \"std_no_center\" as an alternative\n",
        "    use_patches=False,         # set True to also compute & save a patch heatmap\n",
        "    ridge=1e-5,\n",
        "    remix_trials=10,\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "rng = np.random.default_rng(C.seed)\n",
        "\n",
        "# -------------------------------\n",
        "# Helpers\n",
        "# -------------------------------\n",
        "def compute_cir_scalar(f: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"Scalar CIR with mid-mean centering.\"\"\"\n",
        "    f = f.reshape(-1)\n",
        "    y = y.reshape(-1)\n",
        "    n = f.shape[0]\n",
        "    f_mean = f.mean()\n",
        "    y_mean = y.mean()\n",
        "    m = 0.5 * (f_mean + y_mean)\n",
        "    num = n * ((f_mean - m) ** 2 + (y_mean - m) ** 2)\n",
        "    den = np.sum((f - m) ** 2) + np.sum((y - m) ** 2)\n",
        "    return float(num / den) if den > 0 else 0.0\n",
        "\n",
        "def canonical_projection_v(f: np.ndarray, Y: np.ndarray, ridge: float = 1e-5) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given single feature f (n,), and multi-output Y (n, r),\n",
        "    compute v = Y @ w where w ∝ (Σ_Y + λI)^(-1) cov(Y, f).\n",
        "    \"\"\"\n",
        "    f = f.reshape(-1)\n",
        "    n, r = Y.shape\n",
        "    # Center Y and f for covariance estimates\n",
        "    Yc = Y - Y.mean(axis=0, keepdims=True)  # (n, r)\n",
        "    fc = f - f.mean()                        # (n,)\n",
        "\n",
        "    c = (Yc.T @ fc) / n                      # (r,)\n",
        "    SigmaY = (Yc.T @ Yc) / n                 # (r, r)\n",
        "    SigmaY_r = SigmaY + ridge * np.eye(r)\n",
        "\n",
        "    try:\n",
        "        w = np.linalg.solve(SigmaY_r, c)\n",
        "    except np.linalg.LinAlgError:\n",
        "        w = np.linalg.pinv(SigmaY_r) @ c\n",
        "\n",
        "    v = Y @ w  # (n,)\n",
        "    return v\n",
        "\n",
        "def multioutput_cir_feature(f: np.ndarray, Y: np.ndarray, ridge: float = 1e-5) -> float:\n",
        "    v = canonical_projection_v(f, Y, ridge=ridge)\n",
        "    return compute_cir_scalar(f, v)\n",
        "\n",
        "def random_full_rank_matrix(r: int, seed: int | None = None) -> np.ndarray:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    A = rng.normal(size=(r, r))\n",
        "    Q, R = np.linalg.qr(A)\n",
        "    D = np.sign(np.diag(R))\n",
        "    D[D == 0] = 1.0\n",
        "    return Q @ np.diag(D)\n",
        "\n",
        "def kendall_tau(scores1: np.ndarray, scores2: np.ndarray) -> float:\n",
        "    try:\n",
        "        from scipy.stats import kendalltau\n",
        "        tau, _ = kendalltau(scores1, scores2)\n",
        "        return float(tau)\n",
        "    except Exception:\n",
        "        a, b = scores1, scores2\n",
        "        n = len(a)\n",
        "        conc = disc = 0\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                da = a[i] - a[j]\n",
        "                db = b[i] - b[j]\n",
        "                conc += int(da * db > 0)\n",
        "                disc += int(da * db < 0)\n",
        "        denom = conc + disc\n",
        "        return (conc - disc) / denom if denom > 0 else 0.0\n",
        "\n",
        "def topk_overlap(scores1: np.ndarray, scores2: np.ndarray, k: int = 8) -> float:\n",
        "    top1 = set(np.argsort(scores1)[::-1][:k])\n",
        "    top2 = set(np.argsort(scores2)[::-1][:k])\n",
        "    return len(top1 & top2) / k\n",
        "\n",
        "def make_patches_2x2(X8x8: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert flattened (n,64) 8x8 pixels to (n,16) by averaging each non-overlapping 2x2 patch.\n",
        "    Patch order is row-major.\n",
        "    \"\"\"\n",
        "    n = X8x8.shape[0]\n",
        "    Ximg = X8x8.reshape(n, 8, 8)\n",
        "    patches = []\n",
        "    for r in range(0, 8, 2):\n",
        "        for c in range(0, 8, 2):\n",
        "            patch = Ximg[:, r:r+2, c:c+2].mean(axis=(1, 2))\n",
        "            patches.append(patch)\n",
        "    return np.stack(patches, axis=1)  # (n,16)\n",
        "\n",
        "# -------------------------------\n",
        "# Data & model\n",
        "# -------------------------------\n",
        "digits = load_digits()\n",
        "X = digits.data.astype(float)  # (n,64) flattened 8x8\n",
        "y = digits.target\n",
        "\n",
        "# Split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=C.seed, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=C.seed + 1, stratify=y_temp\n",
        ")\n",
        "\n",
        "# Choose feature scaling that does NOT mean-center\n",
        "if C.feature_scaler == \"minmax\":\n",
        "    scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled   = scaler.transform(X_val)\n",
        "    X_test_scaled  = scaler.transform(X_test)\n",
        "elif C.feature_scaler == \"std_no_center\":\n",
        "    # scale by std only (no mean subtraction)\n",
        "    ss = StandardScaler(with_mean=False, with_std=True)\n",
        "    X_train_scaled = ss.fit_transform(X_train)\n",
        "    X_val_scaled   = ss.transform(X_val)\n",
        "    X_test_scaled  = ss.transform(X_test)\n",
        "else:\n",
        "    raise ValueError(\"feature_scaler must be 'minmax' or 'std_no_center'.\")\n",
        "\n",
        "# Optionally switch to 2x2 patches\n",
        "if C.use_patches:\n",
        "    X_train_feats = make_patches_2x2(X_train_scaled)\n",
        "    X_val_feats   = make_patches_2x2(X_val_scaled)\n",
        "    X_test_feats  = make_patches_2x2(X_test_scaled)\n",
        "    feat_grid = (4, 4)  # for visualization\n",
        "else:\n",
        "    X_train_feats, X_val_feats, X_test_feats = X_train_scaled, X_val_scaled, X_test_scaled\n",
        "    feat_grid = (8, 8)\n",
        "\n",
        "# Train multinomial logistic regression\n",
        "clf = LogisticRegression(\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=2000,\n",
        "    random_state=C.seed\n",
        ")\n",
        "clf.fit(X_train_feats, y_train)\n",
        "val_acc  = clf.score(X_val_feats,  y_val)\n",
        "test_acc = clf.score(X_test_feats, y_test)\n",
        "\n",
        "# Use LOGITS for multi-output Y\n",
        "Y_val = clf.decision_function(X_val_feats)  # (n_val, 10)\n",
        "\n",
        "# -------------------------------\n",
        "# Multi-output ExCIR per feature\n",
        "# -------------------------------\n",
        "p = X_val_feats.shape[1]\n",
        "cir_multi = np.zeros(p)\n",
        "for j in range(p):\n",
        "    f = X_val_feats[:, j]\n",
        "    cir_multi[j] = multioutput_cir_feature(f, Y_val, ridge=C.ridge)\n",
        "\n",
        "# -------------------------------\n",
        "# Invariance test (remix Y)\n",
        "# -------------------------------\n",
        "r = Y_val.shape[1]\n",
        "taus, ov8, ov10 = [], [], []\n",
        "for t in range(C.remix_trials):\n",
        "    M = random_full_rank_matrix(r, seed=C.seed + 123 + t)\n",
        "    Y_mix = Y_val @ M\n",
        "    cir_mix = np.zeros_like(cir_multi)\n",
        "    for j in range(p):\n",
        "        f = X_val_feats[:, j]\n",
        "        cir_mix[j] = multioutput_cir_feature(f, Y_mix, ridge=C.ridge)\n",
        "    taus.append(kendall_tau(cir_multi, cir_mix))\n",
        "    ov8.append(topk_overlap(cir_multi, cir_mix, k=8 if p >= 8 else max(1, p//4)))\n",
        "    ov10.append(topk_overlap(cir_multi, cir_mix, k=10 if p >= 10 else max(1, p//3)))\n",
        "\n",
        "tau_med   = float(np.median(taus))\n",
        "top8_med  = float(np.median(ov8))\n",
        "top10_med = float(np.median(ov10))\n",
        "\n",
        "# -------------------------------\n",
        "# Save heatmaps + CSVs\n",
        "# -------------------------------\n",
        "# Min-max for display only\n",
        "hm = cir_multi.reshape(*feat_grid)\n",
        "hm_disp = (hm - hm.min()) / (hm.max() - hm.min() + 1e-12)\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(hm_disp, cmap=\"viridis\", interpolation=\"nearest\", vmin=0.0, vmax=1.0)\n",
        "plt.colorbar(fraction=0.046, pad=0.04)\n",
        "title_suffix = \"patches 2x2\" if C.use_patches else \"per-pixel\"\n",
        "plt.title(f\"Multi-output ExCIR heatmap ({title_suffix})\")\n",
        "plt.xticks([]); plt.yticks([])\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"digits_multioutput_excir_heatmap.png\", dpi=220)\n",
        "plt.close()\n",
        "\n",
        "summary = {\n",
        "    \"val_accuracy\": val_acc,\n",
        "    \"test_accuracy\": test_acc,\n",
        "    \"kendall_tau_median_after_remix\": tau_med,\n",
        "    \"top8_overlap_median_after_remix\": top8_med,\n",
        "    \"top10_overlap_median_after_remix\": top10_med,\n",
        "    \"features\": p,\n",
        "    \"scaler\": C.feature_scaler,\n",
        "    \"use_patches\": C.use_patches,\n",
        "}\n",
        "pd.DataFrame([summary]).to_csv(\"digits_multioutput_excir_summary.csv\", index=False)\n",
        "\n",
        "out_cols = {\"feature_index\": np.arange(p), \"excir_multi_logits\": cir_multi}\n",
        "pd.DataFrame(out_cols).to_csv(\"digits_multioutput_excir_per_feature.csv\", index=False)\n",
        "\n",
        "print(\"[ExCIR] Validation accuracy:\", round(val_acc, 4))\n",
        "print(\"[ExCIR] Test accuracy:\", round(test_acc, 4))\n",
        "print(\"[ExCIR] Invariance (median over\", C.remix_trials, \"remixes):\")\n",
        "print(\"        Kendall tau:\", round(tau_med, 4))\n",
        "print(\"        Top-8 overlap:\", round(top8_med, 4))\n",
        "print(\"        Top-10 overlap:\", round(top10_med, 4))\n",
        "print(\"[ExCIR] Saved heatmap -> digits_multioutput_excir_heatmap.png\")\n",
        "print(\"[ExCIR] Saved summary -> digits_multioutput_excir_summary.csv\")\n",
        "print(\"[ExCIR] Saved scores  -> digits_multioutput_excir_per_feature.csv\")\n"
      ],
      "metadata": {
        "id": "IKVFtpTVyvTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make_multioutput_excir_supp.py\n",
        "# Generates supplementary figures for class-conditioned multi-output ExCIR on sklearn digits.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ------------------ core helpers ------------------\n",
        "\n",
        "def compute_cir_scalar(f: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"Scalar CIR with mid-mean centering.\"\"\"\n",
        "    f = f.reshape(-1)\n",
        "    y = y.reshape(-1)\n",
        "    n = f.shape[0]\n",
        "    f_mean = f.mean()\n",
        "    y_mean = y.mean()\n",
        "    m = 0.5 * (f_mean + y_mean)\n",
        "    num = n * ((f_mean - m) ** 2 + (y_mean - m) ** 2)\n",
        "    den = np.sum((f - m) ** 2) + np.sum((y - m) ** 2)\n",
        "    return float(num / den) if den > 0 else 0.0\n",
        "\n",
        "def canonical_projection_v(f: np.ndarray, Y: np.ndarray, ridge: float = 1e-5):\n",
        "    \"\"\"\n",
        "    Given feature f (n,) and multi-output Y (n,r), return:\n",
        "      v = Y @ w, and w = (Σ_Y + λI)^(-1) cov(Y,f).\n",
        "    \"\"\"\n",
        "    f = f.reshape(-1)\n",
        "    n, r = Y.shape\n",
        "    Yc = Y - Y.mean(axis=0, keepdims=True)\n",
        "    fc = f - f.mean()\n",
        "\n",
        "    c = (Yc.T @ fc) / n                  # (r,)\n",
        "    SigmaY = (Yc.T @ Yc) / n             # (r,r)\n",
        "    SigmaY_r = SigmaY + ridge * np.eye(r)\n",
        "\n",
        "    try:\n",
        "        w = np.linalg.solve(SigmaY_r, c)\n",
        "    except np.linalg.LinAlgError:\n",
        "        w = np.linalg.pinv(SigmaY_r) @ c\n",
        "\n",
        "    v = Y @ w\n",
        "    return v, w\n",
        "\n",
        "def multioutput_cir_feature(f: np.ndarray, Y: np.ndarray, ridge: float = 1e-5) -> float:\n",
        "    v, _ = canonical_projection_v(f, Y, ridge=ridge)\n",
        "    return compute_cir_scalar(f, v)\n",
        "\n",
        "def random_full_rank_matrix_general(r: int, scale: float = 0.5, seed: int = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Non-orthogonal, well-conditioned full-rank remix.\n",
        "    scale>0 introduces axis stretching (not a pure rotation).\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    A = rng.normal(size=(r, r))\n",
        "    U, _, Vt = np.linalg.svd(A)\n",
        "    S = np.diag(np.linspace(1.0, 1.0 + scale, r))\n",
        "    return U @ S @ Vt\n",
        "\n",
        "def kendall_tau(scores1: np.ndarray, scores2: np.ndarray) -> float:\n",
        "    # Try scipy if available; else naive\n",
        "    try:\n",
        "        from scipy.stats import kendalltau\n",
        "        tau, _ = kendalltau(scores1, scores2)\n",
        "        return float(tau)\n",
        "    except Exception:\n",
        "        a, b = scores1, scores2\n",
        "        n = len(a)\n",
        "        conc = disc = 0\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                da = a[i] - a[j]\n",
        "                db = b[i] - b[j]\n",
        "                conc += int(da * db > 0)\n",
        "                disc += int(da * db < 0)\n",
        "        denom = conc + disc\n",
        "        return (conc - disc) / denom if denom > 0 else 0.0\n",
        "\n",
        "def topk_overlap(scores1: np.ndarray, scores2: np.ndarray, k: int = 8) -> float:\n",
        "    top1 = set(np.argsort(scores1)[::-1][:k])\n",
        "    top2 = set(np.argsort(scores2)[::-1][:k])\n",
        "    return len(top1 & top2) / k\n",
        "\n",
        "def make_patches_2x2(X8x8: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"(n,64)->(n,16) by averaging non-overlapping 2x2 patches in row-major order.\"\"\"\n",
        "    n = X8x8.shape[0]\n",
        "    Ximg = X8x8.reshape(n, 8, 8)\n",
        "    patches = []\n",
        "    for rr in range(0, 8, 2):\n",
        "        for cc in range(0, 8, 2):\n",
        "            patches.append(Ximg[:, rr:rr+2, cc:cc+2].mean(axis=(1, 2)))\n",
        "    return np.stack(patches, axis=1)\n",
        "\n",
        "# ------------------ data & model ------------------\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data.astype(float)     # (n,64)\n",
        "y = digits.target\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=0, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp\n",
        ")\n",
        "\n",
        "# scale to [0,1] (avoids mean-centering collapse)\n",
        "scaler = MinMaxScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    max_iter=2000,\n",
        "    random_state=0\n",
        ")\n",
        "clf.fit(X_train_s, y_train)\n",
        "val_acc  = clf.score(X_val_s,  y_val)\n",
        "test_acc = clf.score(X_test_s, y_test)\n",
        "\n",
        "# logits for vector-output Y'\n",
        "Y_val = clf.decision_function(X_val_s)     # (n_val, 10)\n",
        "\n",
        "# ------------------ per-pixel multi-output ExCIR ------------------\n",
        "\n",
        "ridge = 1e-5\n",
        "p = X_val_s.shape[1]\n",
        "excir_pix = np.zeros(p)\n",
        "weights_w = np.zeros((p, Y_val.shape[1]))\n",
        "\n",
        "for j in range(p):\n",
        "    vj, wj = canonical_projection_v(X_val_s[:, j], Y_val, ridge=ridge)\n",
        "    excir_pix[j] = compute_cir_scalar(X_val_s[:, j], vj)\n",
        "    weights_w[j] = wj\n",
        "\n",
        "# Save the main heatmap (normalized for display only)\n",
        "hm = excir_pix.reshape(8, 8)\n",
        "hm_disp = (hm - hm.min()) / (hm.max() - hm.min() + 1e-12)\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(hm_disp, cmap='viridis', interpolation='nearest', vmin=0.0, vmax=1.0)\n",
        "plt.colorbar(fraction=0.046, pad=0.04)\n",
        "plt.title('Multi-output ExCIR heatmap (per-pixel)')\n",
        "plt.xticks([]); plt.yticks([])\n",
        "plt.tight_layout()\n",
        "plt.savefig('digits_multioutput_excir_heatmap.png', dpi=220)\n",
        "plt.close()\n",
        "\n",
        "# ------------------ (C.7a) Directional sensitivity figure ------------------\n",
        "\n",
        "k_show = 5\n",
        "top_idx = np.argsort(excir_pix)[::-1][:k_show]\n",
        "deltas = np.array([0.02, 0.04, 0.08, 0.16])  # positive magnitudes on [0,1]-scaled inputs\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "for j in top_idx:\n",
        "    _, wj = canonical_projection_v(X_val_s[:, j], Y_val, ridge=ridge)\n",
        "    base_proj = (Y_val @ wj)  # (n_val,)\n",
        "    changes = []\n",
        "    for d in deltas:\n",
        "        X_val_pert = X_val_s.copy()\n",
        "        X_val_pert[:, j] = np.clip(X_val_pert[:, j] + d, 0.0, 1.0)\n",
        "        Y_pert = clf.decision_function(X_val_pert)\n",
        "        proj_pert = (Y_pert @ wj)\n",
        "        changes.append(np.mean(np.abs(proj_pert - base_proj)))\n",
        "    plt.plot(deltas, changes, marker='o', label=f'pix {j}')\n",
        "\n",
        "plt.xlabel(r'Nudge size $|\\delta|$')\n",
        "plt.ylabel(r'$\\;|\\Delta \\hat{y}|\\;$ (mean abs. change along canonical output)')\n",
        "plt.title('Directional sensitivity: top-5 pixels')\n",
        "plt.legend(ncol=2, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig('multiout_directional_sensitivity.png', dpi=220)\n",
        "plt.close()\n",
        "\n",
        "# ------------------ (C.7b) Invariance: remix Kendall τ and Top-k ------------------\n",
        "\n",
        "r = Y_val.shape[1]\n",
        "trials = 10\n",
        "taus = []\n",
        "top8 = []\n",
        "top10 = []\n",
        "\n",
        "for t in range(trials):\n",
        "    # IMPORTANT: use non-orthogonal remix for a non-trivial test\n",
        "    M = random_full_rank_matrix_general(r, scale=0.5, seed=123 + t)\n",
        "    Y_mix = Y_val @ M\n",
        "    excir_mix = np.zeros_like(excir_pix)\n",
        "    for j in range(p):\n",
        "        excir_mix[j] = multioutput_cir_feature(X_val_s[:, j], Y_mix, ridge=ridge)\n",
        "    taus.append(kendall_tau(excir_pix, excir_mix))\n",
        "    top8.append(topk_overlap(excir_pix, excir_mix, k=8))\n",
        "    top10.append(topk_overlap(excir_pix, excir_mix, k=10))\n",
        "\n",
        "# Plot side-by-side bars\n",
        "fig, ax = plt.subplots(1, 2, figsize=(9, 3.8))\n",
        "\n",
        "ax[0].bar(np.arange(trials), taus)\n",
        "ax[0].set_ylim(0, 1.05)\n",
        "ax[0].set_xlabel('Remix trial')\n",
        "ax[0].set_ylabel(r'Kendall-$\\tau$')\n",
        "ax[0].set_title('Rank concordance')\n",
        "\n",
        "ax[1].plot(np.arange(trials), top8, marker='o', label='Top-8')\n",
        "ax[1].plot(np.arange(trials), top10, marker='s', label='Top-10')\n",
        "ax[1].set_ylim(0, 1.05)\n",
        "ax[1].set_xlabel('Remix trial')\n",
        "ax[1].set_ylabel('Overlap')\n",
        "ax[1].set_title('Leader preservation')\n",
        "ax[1].legend()\n",
        "xs = np.arange(trials)\n",
        "ax[1].plot(xs - 0.05, top8,  marker='o', linestyle='-',  label='Top-8',  alpha=0.9)\n",
        "ax[1].plot(xs + 0.05, top10, marker='s', linestyle='--', label='Top-10', alpha=0.9)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('multiout_remix_kendall_topk.png', dpi=220)\n",
        "plt.close()\n",
        "\n",
        "# ------------------ (C.7c) Patch-level (2x2) ExCIR ------------------\n",
        "\n",
        "X_val_patches = make_patches_2x2(X_val_s)  # (n_val, 16)\n",
        "excir_patch = np.zeros(X_val_patches.shape[1])\n",
        "for j in range(X_val_patches.shape[1]):\n",
        "    excir_patch[j] = multioutput_cir_feature(X_val_patches[:, j], Y_val, ridge=ridge)\n",
        "\n",
        "hm_p = excir_patch.reshape(4, 4)\n",
        "hm_p_disp = (hm_p - hm_p.min()) / (hm_p.max() - hm_p.min() + 1e-12)\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(hm_p_disp, cmap='viridis', interpolation='nearest', vmin=0.0, vmax=1.0)\n",
        "plt.colorbar(fraction=0.046, pad=0.04)\n",
        "plt.title('Patch-level multi-output ExCIR (2x2)')\n",
        "plt.xticks([]); plt.yticks([])\n",
        "plt.tight_layout()\n",
        "plt.savefig('multiout_patch_excir_heatmap.png', dpi=220)\n",
        "plt.close()\n",
        "\n",
        "# ------------------ save small summary ------------------\n",
        "\n",
        "summary = {\n",
        "    'val_acc': float(val_acc),\n",
        "    'test_acc': float(test_acc),\n",
        "    'kendall_tau_median': float(np.median(taus)),\n",
        "    'top8_overlap_median': float(np.median(top8)),\n",
        "    'top10_overlap_median': float(np.median(top10))\n",
        "}\n",
        "pd.DataFrame([summary]).to_csv('multiout_excir_supp_summary.csv', index=False)\n",
        "\n",
        "print(\"Saved figures:\")\n",
        "print(\"  - digits_multioutput_excir_heatmap.png\")\n",
        "print(\"  - multiout_directional_sensitivity.png\")\n",
        "print(\"  - multiout_remix_kendall_topk.png\")\n",
        "print(\"  - multiout_patch_excir_heatmap.png\")\n",
        "print(\"Saved summary -> multiout_excir_supp_summary.csv\")\n"
      ],
      "metadata": {
        "id": "RaQQCFytyx-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make_multioutput_excir_supp.py\n",
        "# Generates supplementary figures for class-conditioned multi-output ExCIR on sklearn digits.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Define the target saving directory for Colab/Jupyter environments\n",
        "SAVE_DIR = '/content/sample_data/'\n",
        "\n",
        "# ------------------ core helpers ------------------\n",
        "\n",
        "def compute_cir_scalar(f: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"Scalar CIR with mid-mean centering.\"\"\"\n",
        "    f = f.reshape(-1)\n",
        "    y = y.reshape(-1)\n",
        "    n = f.shape[0]\n",
        "    f_mean = f.mean()\n",
        "    y_mean = y.mean()\n",
        "    m = 0.5 * (f_mean + y_mean)\n",
        "    num = n * ((f_mean - m) ** 2 + (y_mean - m) ** 2)\n",
        "    den = np.sum((f - m) ** 2) + np.sum((y - m) ** 2)\n",
        "    return float(num / den) if den > 0 else 0.0\n",
        "\n",
        "def canonical_projection_v(f: np.ndarray, Y: np.ndarray, ridge: float = 1e-5):\n",
        "    \"\"\"\n",
        "    Given feature f (n,) and multi-output Y (n,r), return:\n",
        "      v = Y @ w, and w = (Σ_Y + λI)^(-1) cov(Y,f).\n",
        "    \"\"\"\n",
        "    f = f.reshape(-1)\n",
        "    n, r = Y.shape\n",
        "    Yc = Y - Y.mean(axis=0, keepdims=True)\n",
        "    fc = f - f.mean()\n",
        "\n",
        "    c = (Yc.T @ fc) / n                  # (r,)\n",
        "    SigmaY = (Yc.T @ Yc) / n             # (r,r)\n",
        "    SigmaY_r = SigmaY + ridge * np.eye(r)\n",
        "\n",
        "    try:\n",
        "        w = np.linalg.solve(SigmaY_r, c)\n",
        "    except np.linalg.LinAlgError:\n",
        "        w = np.linalg.pinv(SigmaY_r) @ c\n",
        "\n",
        "    v = Y @ w\n",
        "    return v, w\n",
        "\n",
        "def multioutput_cir_feature(f: np.ndarray, Y: np.ndarray, ridge: float = 1e-5) -> float:\n",
        "    v, _ = canonical_projection_v(f, Y, ridge=ridge)\n",
        "    return compute_cir_scalar(f, v)\n",
        "\n",
        "def random_full_rank_matrix_general(r: int, scale: float = 0.5, seed: int = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Non-orthogonal, well-conditioned full-rank remix.\n",
        "    scale>0 introduces axis stretching (not a pure rotation).\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    A = rng.normal(size=(r, r))\n",
        "    U, _, Vt = np.linalg.svd(A)\n",
        "    S = np.diag(np.linspace(1.0, 1.0 + scale, r))\n",
        "    return U @ S @ Vt\n",
        "\n",
        "def kendall_tau(scores1: np.ndarray, scores2: np.ndarray) -> float:\n",
        "    # Try scipy if available; else naive\n",
        "    try:\n",
        "        from scipy.stats import kendalltau\n",
        "        tau, _ = kendalltau(scores1, scores2)\n",
        "        return float(tau)\n",
        "    except Exception:\n",
        "        a, b = scores1, scores2\n",
        "        n = len(a)\n",
        "        conc = disc = 0\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                da = a[i] - a[j]\n",
        "                db = b[i] - b[j]\n",
        "                conc += int(da * db > 0)\n",
        "                disc += int(da * db < 0)\n",
        "        denom = conc + disc\n",
        "        return (conc - disc) / denom if denom > 0 else 0.0\n",
        "\n",
        "def topk_overlap(scores1: np.ndarray, scores2: np.ndarray, k: int = 8) -> float:\n",
        "    top1 = set(np.argsort(scores1)[::-1][:k])\n",
        "    top2 = set(np.argsort(scores2)[::-1][:k])\n",
        "    return len(top1 & top2) / k\n",
        "\n",
        "def make_patches_2x2(X8x8: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"(n,64)->(n,16) by averaging non-overlapping 2x2 patches in row-major order.\"\"\"\n",
        "    n = X8x8.shape[0]\n",
        "    Ximg = X8x8.reshape(n, 8, 8)\n",
        "    patches = []\n",
        "    for rr in range(0, 8, 2):\n",
        "        for cc in range(0, 8, 2):\n",
        "            patches.append(Ximg[:, rr:rr+2, cc:cc+2].mean(axis=(1, 2)))\n",
        "    return np.stack(patches, axis=1)\n",
        "\n",
        "def softmax(z, axis=1):\n",
        "    z = z - z.max(axis=axis, keepdims=True)\n",
        "    ez = np.exp(z)\n",
        "    return ez / ez.sum(axis=axis, keepdims=True)\n",
        "\n",
        "# ------------------ data & model ------------------\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data.astype(float)     # (n,64)\n",
        "y = digits.target\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=0, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp\n",
        ")\n",
        "\n",
        "# scale to [0,1] (avoids mean-centering collapse)\n",
        "scaler = MinMaxScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    max_iter=2000,\n",
        "    random_state=0\n",
        ")\n",
        "clf.fit(X_train_s, y_train)\n",
        "val_acc  = clf.score(X_val_s,  y_val)\n",
        "test_acc = clf.score(X_test_s, y_test)\n",
        "\n",
        "# logits for vector-output Y'\n",
        "Y_val = clf.decision_function(X_val_s)     # (n_val, 10)\n",
        "\n",
        "# ------------------ per-pixel multi-output ExCIR ------------------\n",
        "\n",
        "ridge = 1e-5\n",
        "p = X_val_s.shape[1]\n",
        "excir_pix = np.zeros(p)\n",
        "weights_w = np.zeros((p, Y_val.shape[1]))\n",
        "\n",
        "for j in range(p):\n",
        "    vj, wj = canonical_projection_v(X_val_s[:, j], Y_val, ridge=ridge)\n",
        "    excir_pix[j] = compute_cir_scalar(X_val_s[:, j], vj)\n",
        "    weights_w[j] = wj\n",
        "\n",
        "# Save and Show the main heatmap (normalized for display only)\n",
        "hm = excir_pix.reshape(8, 8)\n",
        "hm_disp = (hm - hm.min()) / (hm.max() - hm.min() + 1e-12)\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(hm_disp, cmap='viridis', interpolation='nearest', vmin=0.0, vmax=1.0)\n",
        "plt.colorbar(fraction=0.046, pad=0.04)\n",
        "plt.title('Multi-output ExCIR heatmap (per-pixel)')\n",
        "plt.xticks([]); plt.yticks([])\n",
        "plt.tight_layout()\n",
        "plt.savefig(SAVE_DIR + 'digits_multioutput_excir_heatmap.png', dpi=220)\n",
        "plt.show() # Display plot\n",
        "plt.close()\n",
        "\n",
        "# ------------------ (C.7a) Directional sensitivity figure ------------------\n",
        "\n",
        "k_show = 5\n",
        "top_idx = np.argsort(excir_pix)[::-1][:k_show]\n",
        "deltas = np.array([0.02, 0.04, 0.08, 0.16])  # positive magnitudes on [0,1]-scaled inputs\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "for j in top_idx:\n",
        "    _, wj = canonical_projection_v(X_val_s[:, j], Y_val, ridge=ridge)\n",
        "    base_proj = (Y_val @ wj)  # (n_val,)\n",
        "    changes = []\n",
        "    for d in deltas:\n",
        "        X_val_pert = X_val_s.copy()\n",
        "        X_val_pert[:, j] = np.clip(X_val_pert[:, j] + d, 0.0, 1.0)\n",
        "        Y_pert = clf.decision_function(X_val_pert)\n",
        "        proj_pert = (Y_pert @ wj)\n",
        "        changes.append(np.mean(np.abs(proj_pert - base_proj)))\n",
        "    plt.plot(deltas, changes, marker='o', label=f'pix {j}')\n",
        "\n",
        "plt.xlabel(r'Nudge size $|\\delta|$')\n",
        "plt.ylabel(r'$\\;|\\Delta \\hat{y}|\\;$ (mean abs. change along canonical output)')\n",
        "plt.title('Directional sensitivity: top-5 pixels')\n",
        "plt.legend(ncol=2, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(SAVE_DIR + 'multiout_directional_sensitivity.png', dpi=220)\n",
        "plt.show() # Display plot\n",
        "plt.close()\n",
        "\n",
        "# ------------------ (C.7b) Robustness: Softmax Temperature + Noise (Supplementary) ------------------\n",
        "\n",
        "# Realistic, non-invertible reparameterization (Calibration/Noise)\n",
        "temp = 1.4            # 1.2–1.8 is sensible\n",
        "noise_eps = 0.10      # 0.05–0.20; set 0.0 if you want only temperature\n",
        "\n",
        "taus_temp, top8_temp, top10_temp = [], [], []\n",
        "trials = 10\n",
        "rng = np.random.default_rng(456) # Use a different seed for this separate test\n",
        "\n",
        "for t in range(trials):\n",
        "    # 1. Temperature scaling of logits -> probabilities (Non-linear, non-invertible reparam.)\n",
        "    P = softmax(Y_val / temp)\n",
        "    # 2. Convert back to 'scores' (log probabilities)\n",
        "    Y_mix = np.log(P + 1e-9)\n",
        "    # 3. Optional small Gaussian noise to mimic score jitter (Additive noise)\n",
        "    if noise_eps > 0:\n",
        "        Y_mix = Y_mix + noise_eps * rng.normal(size=Y_mix.shape)\n",
        "\n",
        "    excir_mix = np.zeros_like(excir_pix)\n",
        "    for j in range(p):\n",
        "        excir_mix[j] = multioutput_cir_feature(X_val_s[:, j], Y_mix, ridge=ridge)\n",
        "\n",
        "    taus_temp.append(kendall_tau(excir_pix, excir_mix))\n",
        "    top8_temp.append(topk_overlap(excir_pix, excir_mix, k=8))\n",
        "    top10_temp.append(topk_overlap(excir_pix, excir_mix, k=10))\n",
        "\n",
        "\n",
        "# ------------------ (C.7c) Invariance: Full-rank Remix (Main Paper Theorem Validation) ------------------\n",
        "\n",
        "r = Y_val.shape[1]\n",
        "trials = 10\n",
        "taus = []  # Full-rank results (should be near 1.0)\n",
        "top8 = []\n",
        "top10 = []\n",
        "\n",
        "for t in range(trials):\n",
        "    # Invertible linear remix for theorem validation\n",
        "    M = random_full_rank_matrix_general(r, scale=0.5, seed=123 + t)\n",
        "    Y_mix = Y_val @ M\n",
        "    excir_mix = np.zeros_like(excir_pix)\n",
        "    for j in range(p):\n",
        "        excir_mix[j] = multioutput_cir_feature(X_val_s[:, j], Y_mix, ridge=ridge)\n",
        "    taus.append(kendall_tau(excir_pix, excir_mix))\n",
        "    top8.append(topk_overlap(excir_pix, excir_mix, k=8))\n",
        "    top10.append(topk_overlap(excir_pix, excir_mix, k=10))\n",
        "\n",
        "# Plot side-by-side bars (using the Full-rank remix results: taus, top8, top10)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(9, 3.8))\n",
        "\n",
        "ax[0].bar(np.arange(trials), taus)\n",
        "ax[0].set_ylim(0, 1.05)\n",
        "ax[0].set_xlabel('Remix trial')\n",
        "ax[0].set_ylabel(r'Kendall-$\\tau$')\n",
        "ax[0].set_title('Rank concordance (Invertible Full-rank remix)')\n",
        "\n",
        "# Plotting the lines\n",
        "xs = np.arange(trials)\n",
        "ax[1].plot(xs - 0.05, top8,  marker='o', linestyle='-',  label='Top-8',  alpha=0.9)\n",
        "ax[1].plot(xs + 0.05, top10, marker='s', linestyle='--', label='Top-10', alpha=0.9)\n",
        "ax[1].set_ylim(0, 1.05)\n",
        "ax[1].set_xlabel('Remix trial')\n",
        "ax[1].set_ylabel('Overlap')\n",
        "ax[1].set_title('Leader preservation (Invertible Full-rank remix)')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(SAVE_DIR + 'multiout_remix_kendall_topk.png', dpi=220)\n",
        "plt.show() # Display plot\n",
        "plt.close()\n",
        "\n",
        "# ------------------ (C.7d) Patch-level (2x2) ExCIR ------------------\n",
        "\n",
        "X_val_patches = make_patches_2x2(X_val_s)  # (n_val, 16)\n",
        "excir_patch = np.zeros(X_val_patches.shape[1])\n",
        "for j in range(X_val_patches.shape[1]):\n",
        "    excir_patch[j] = multioutput_cir_feature(X_val_patches[:, j], Y_val, ridge=ridge)\n",
        "\n",
        "hm_p = excir_patch.reshape(4, 4)\n",
        "hm_p_disp = (hm_p - hm_p.min()) / (hm_p.max() - hm_p.min() + 1e-12)\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(hm_p_disp, cmap='viridis', interpolation='nearest', vmin=0.0, vmax=1.0)\n",
        "plt.colorbar(fraction=0.046, pad=0.04)\n",
        "plt.title('Patch-level multi-output ExCIR (2x2)')\n",
        "plt.xticks([]); plt.yticks([])\n",
        "plt.tight_layout()\n",
        "plt.savefig(SAVE_DIR + 'multiout_patch_excir_heatmap.png', dpi=220)\n",
        "plt.show() # Display plot\n",
        "plt.close()\n",
        "\n",
        "# ------------------ save small summary ------------------\n",
        "\n",
        "summary = {\n",
        "    'val_acc': float(val_acc),\n",
        "    'test_acc': float(test_acc),\n",
        "    # Main Paper Validation (Invariance)\n",
        "    'kendall_tau_median (Invariance_Full-rank)': float(np.median(taus)),\n",
        "    'top8_overlap_median (Invariance_Full-rank)': float(np.median(top8)),\n",
        "    'top10_overlap_median (Invariance_Full-rank)': float(np.median(top10)),\n",
        "    # Supplementary Robustness (Softmax+Noise)\n",
        "    'kendall_tau_median (Robustness_Softmax+Noise)': float(np.median(taus_temp)),\n",
        "    'top8_overlap_median (Robustness_Softmax+Noise)': float(np.median(top8_temp)),\n",
        "    'top10_overlap_median (Robustness_Softmax+Noise)': float(np.median(top10_temp))\n",
        "}\n",
        "pd.DataFrame([summary]).to_csv(SAVE_DIR + 'multiout_excir_supp_summary.csv', index=False)\n",
        "\n",
        "print(\"Saved figures:\")\n",
        "print(f\"  - {SAVE_DIR}digits_multioutput_excir_heatmap.png\")\n",
        "print(f\"  - {SAVE_DIR}multiout_directional_sensitivity.png\")\n",
        "print(f\"  - {SAVE_DIR}multiout_remix_kendall_topk.png (Plots the Invertible Full-rank results)\")\n",
        "print(f\"  - {SAVE_DIR}multiout_patch_excir_heatmap.png\")\n",
        "print(f\"Saved summary -> {SAVE_DIR}multiout_excir_supp_summary.csv (Includes both Invariance and Robustness metrics)\")"
      ],
      "metadata": {
        "id": "nhhMc28wy2AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap lime scikit-learn\n"
      ],
      "metadata": {
        "id": "XvAa-B1vy4Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# ---------------------------\n",
        "# Generate synthetic vehicular data\n",
        "# ---------------------------\n",
        "n, k = 5000, 20\n",
        "X = np.random.randn(n, k)\n",
        "y = X[:, 0] * 3 + X[:, 1] * -2 + np.random.randn(n) * 0.1  # simple synthetic rule\n",
        "\n",
        "# ---------------------------\n",
        "# Train black-box model\n",
        "# ---------------------------\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "model.fit(X, y)\n",
        "\n",
        "# ---------------------------\n",
        "# SHAP (KernelExplainer)\n",
        "# ---------------------------\n",
        "start = time.time()\n",
        "explainer_shap = shap.KernelExplainer(model.predict, X[:100])\n",
        "shap_values = explainer_shap.shap_values(X[:10])  # small sample for speed\n",
        "shap_time = time.time() - start\n",
        "print(f\"SHAP Runtime: {shap_time:.2f} seconds\")\n",
        "\n",
        "# ---------------------------\n",
        "# LIME\n",
        "# ---------------------------\n",
        "start = time.time()\n",
        "explainer_lime = LimeTabularExplainer(X, mode='regression')\n",
        "for i in range(10):  # explain 10 samples\n",
        "    _ = explainer_lime.explain_instance(X[i], model.predict)\n",
        "lime_time = time.time() - start\n",
        "print(f\"LIME Runtime: {lime_time:.2f} seconds\")\n",
        "\n",
        "# ---------------------------\n",
        "# ExCIR (Closed-form mockup)\n",
        "# ---------------------------\n",
        "start = time.time()\n",
        "\n",
        "# Step 1: Mean of each feature and output\n",
        "f_mean = np.mean(X, axis=0)\n",
        "y_mean = np.mean(y)\n",
        "\n",
        "# Step 2: Simulate per-feature CIR score computation\n",
        "cir_scores = []\n",
        "for i in range(X.shape[1]):\n",
        "    f_i = X[:, i]\n",
        "    cir = np.mean((f_i - f_mean[i]) * (y - y_mean)) / (np.std(f_i) * np.std(y) + 1e-8)\n",
        "    cir_scores.append(cir)\n",
        "cir_time = time.time() - start\n",
        "print(f\"ExCIR Runtime: {cir_time:.4f} seconds\")\n",
        "\n",
        "# ---------------------------\n",
        "# Lightweight ExCIR\n",
        "# ---------------------------\n",
        "X_lw = X[:500]\n",
        "y_lw = y[:500]\n",
        "start = time.time()\n",
        "f_mean_lw = np.mean(X_lw, axis=0)\n",
        "y_mean_lw = np.mean(y_lw)\n",
        "cir_scores_lw = []\n",
        "for i in range(X_lw.shape[1]):\n",
        "    f_i = X_lw[:, i]\n",
        "    cir = np.mean((f_i - f_mean_lw[i]) * (y_lw - y_mean_lw)) / (np.std(f_i) * np.std(y_lw) + 1e-8)\n",
        "    cir_scores_lw.append(cir)\n",
        "cir_lw_time = time.time() - start\n",
        "print(f\"Lightweight ExCIR Runtime: {cir_lw_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "jG095-sPy94_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def excir_scores(X, y):\n",
        "    f_mean = np.mean(X, axis=0)\n",
        "    y_mean = np.mean(y)\n",
        "    return np.array([\n",
        "        np.mean((X[:, i] - f_mean[i]) * (y - y_mean)) /\n",
        "        (np.std(X[:, i]) * np.std(y) + 1e-8)\n",
        "        for i in range(X.shape[1])\n",
        "    ])\n",
        "\n",
        "# Parameters\n",
        "B = 100  # bootstrap rounds\n",
        "n, k = 5000, 20\n",
        "\n",
        "# Generate synthetic vehicular data\n",
        "X = np.random.randn(n, k)\n",
        "y = X[:, 0]*2 - X[:, 1]*1.5 + np.random.randn(n)*0.2\n",
        "\n",
        "# Bootstrap\n",
        "bootstrap_scores = []\n",
        "for _ in range(B):\n",
        "    idx = np.random.choice(n, n, replace=True)\n",
        "    X_boot = X[idx]\n",
        "    y_boot = y[idx]\n",
        "    score = excir_scores(X_boot, y_boot)\n",
        "    bootstrap_scores.append(score)\n",
        "\n",
        "bootstrap_scores = np.stack(bootstrap_scores)  # shape: (B, k)\n",
        "\n",
        "# Compute CI\n",
        "mean_scores = np.mean(bootstrap_scores, axis=0)\n",
        "lower = np.percentile(bootstrap_scores, 2.5, axis=0)\n",
        "upper = np.percentile(bootstrap_scores, 97.5, axis=0)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.errorbar(np.arange(k), mean_scores, yerr=[mean_scores - lower, upper - mean_scores],\n",
        "             fmt='o', ecolor='gray', capsize=4, label='95% CI')\n",
        "plt.title(\"Bootstrapped Confidence Intervals for ExCIR Scores (Vehicular Data)\")\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"ExCIR Score\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HYqu4E8fzA5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kendalltau\n",
        "\n",
        "# Original ExCIR ranking\n",
        "orig_scores = excir_scores(X, y)\n",
        "orig_rank = np.argsort(-orig_scores)  # descending\n",
        "\n",
        "# Top-k and Kendall-tau lists\n",
        "topk_range = list(range(1, k+1))\n",
        "tau_list = []\n",
        "topk_overlap_list = []\n",
        "\n",
        "for boot in bootstrap_scores:\n",
        "    boot_rank = np.argsort(-boot)\n",
        "\n",
        "    # Kendall's tau with full ranking\n",
        "    tau, _ = kendalltau(orig_rank, boot_rank)\n",
        "    tau_list.append(tau)\n",
        "\n",
        "    # Top-k overlap\n",
        "    overlaps = []\n",
        "    for k_val in topk_range:\n",
        "        topk_orig = set(orig_rank[:k_val])\n",
        "        topk_boot = set(boot_rank[:k_val])\n",
        "        overlap = len(topk_orig & topk_boot) / k_val\n",
        "        overlaps.append(overlap)\n",
        "    topk_overlap_list.append(overlaps)\n",
        "\n",
        "# Aggregate across all bootstraps\n",
        "tau_mean = np.mean(tau_list)\n",
        "topk_overlap_array = np.array(topk_overlap_list)\n",
        "topk_mean = np.mean(topk_overlap_array, axis=0)\n",
        "topk_lower = np.percentile(topk_overlap_array, 2.5, axis=0)\n",
        "topk_upper = np.percentile(topk_overlap_array, 97.5, axis=0)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ... your plotting code for Top-k overlap\n",
        "  # guide at k=8\n",
        "# (do not set a specific color; let Matplotlib choose the default)\n",
        "\n",
        "\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(topk_range, topk_mean, label='Top-$k$ Overlap (mean)', marker='o')\n",
        "plt.fill_between(topk_range, topk_lower, topk_upper, alpha=0.2, label='95% CI')\n",
        "plt.axhline(1.0, linestyle='--', color='gray')\n",
        "plt.axvline(x=8, linestyle='--', linewidth=1.0)\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Top-$k$ Overlap\")\n",
        "plt.title(f\"Top-$k$ Agreement Across Bootstraps (mean Kendall-τ = {tau_mean:.2f})\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"topk_kendall_bootstrap.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(topk_range, topk_mean, label='Top-$k$ Overlap (mean)', marker='o')\n",
        "plt.fill_between(topk_range, topk_lower, topk_upper, alpha=0.2, label='95% CI')\n",
        "plt.axvline(x=8, linestyle='--', linewidth=1.0)\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Top-$k$ Overlap\")\n",
        "plt.title(f\"Top-$k$ Agreement Across Bootstraps (mean Kendall-τ = {tau_mean:.2f})\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"top.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "w4T2Dz4LzDS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# STEP 1: Load Digits dataset and prepare X, Y_pred\n",
        "# ------------------------------------------------------\n",
        "digits = load_digits()\n",
        "X = digits.data  # [n, 64] -- each row is an 8x8 image flattened\n",
        "y = digits.target  # integer labels\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train softmax classifier\n",
        "clf = make_pipeline(StandardScaler(), LogisticRegression(multi_class='multinomial', max_iter=1000))\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict class probabilities on test data\n",
        "Y_pred = clf.predict_proba(X_test)  # shape: [n_test, 10]\n",
        "X = X_test  # we'll compute ExCIR on test set\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# STEP 2: Compute per-class ExCIR\n",
        "# ------------------------------------------------------\n",
        "def compute_excir_scores(X, Y):\n",
        "    X_centered = X - X.mean(axis=0, keepdims=True)\n",
        "    scores = []\n",
        "    for j in range(Y.shape[1]):  # for each output class\n",
        "        y = Y[:, j]\n",
        "        y_centered = y - y.mean()\n",
        "        cov = (X_centered * y_centered[:, None]).mean(axis=0)\n",
        "        std_x = X.std(axis=0)\n",
        "        std_y = y.std()\n",
        "        score = cov / (std_x * std_y + 1e-8)\n",
        "        scores.append(score)\n",
        "    return np.array(scores)  # shape: [10 classes, 64 features]\n",
        "\n",
        "classwise_scores = compute_excir_scores(X, Y_pred)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# STEP 3: Heatmap of class-wise ExCIR scores\n",
        "# ------------------------------------------------------\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.heatmap(classwise_scores, cmap='coolwarm', center=0,\n",
        "            xticklabels=False, yticklabels=[f\"Digit {i}\" for i in range(10)])\n",
        "plt.title(\"Per-Class ExCIR Scores (Digits)\")\n",
        "plt.xlabel(\"Pixel Index (flattened)\")\n",
        "plt.ylabel(\"Class Label\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"digits_classwise_heatmap.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# STEP 4: Compute Top-k Jaccard Overlap Matrix\n",
        "# ------------------------------------------------------\n",
        "def compute_topk_jaccard(scores, k=10):\n",
        "    num_classes = scores.shape[0]\n",
        "    topk_sets = [set(np.argsort(-scores[i])[:k]) for i in range(num_classes)]\n",
        "    jaccard_matrix = np.zeros((num_classes, num_classes))\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            intersection = topk_sets[i] & topk_sets[j]\n",
        "            union = topk_sets[i] | topk_sets[j]\n",
        "            jaccard_matrix[i, j] = len(intersection) / len(union)\n",
        "    return jaccard_matrix\n",
        "\n",
        "jaccard_mat = compute_topk_jaccard(classwise_scores, k=10)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(jaccard_mat, annot=True, cmap=\"Blues\", fmt=\".2f\",\n",
        "            xticklabels=[f\"{i}\" for i in range(10)],\n",
        "            yticklabels=[f\"{i}\" for i in range(10)])\n",
        "plt.title(\"Top-10 Jaccard Overlap Between Class-wise ExCIR Rankings\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"digits_jaccard_topk10.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ITLuHZ4AzFqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# STEP 5: Save heatmaps for selected digits: 1, 8, 9\n",
        "# ------------------------------------------------------\n",
        "selected_digits = [1, 8, 9]\n",
        "titles = ['Digit 1', 'Digit 8', 'Digit 9']\n",
        "\n",
        "plt.figure(figsize=(9, 3))\n",
        "for idx, digit in enumerate(selected_digits):\n",
        "    heat = classwise_scores[digit].reshape(8, 8)\n",
        "    ax = plt.subplot(1, 3, idx + 1)\n",
        "    sns.heatmap(heat, cmap='coolwarm', cbar=False, xticklabels=False, yticklabels=False, square=True)\n",
        "    ax.set_title(titles[idx], fontsize=12)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"digit_heatmaps_1_8_9.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-R5YI780zIlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# excir_perturb_eval_synth.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from scipy.stats import kendalltau\n",
        "\n",
        "# -----------------------------\n",
        "# helpers\n",
        "# -----------------------------\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# -----------------------------\n",
        "# your dataset generator (fixed: check_random_state + sigmoid)\n",
        "# -----------------------------\n",
        "def make_vehicle_dataset(n=6000, random_state=42, tire_corr=0.3):\n",
        "    rng = check_random_state(random_state)\n",
        "    feat_names = [\n",
        "        \"speed_kph\",\"rpm\",\"throttle\",\"brake\",\"steering_deg\",\"gear\",\n",
        "        \"accel_long\",\"accel_lat\",\"yaw_rate\",\"road_grade\",\"ambient_temp\",\n",
        "        \"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\",\n",
        "        \"engine_load\",\"maf\",\"intake_air_temp\",\"battery_v\",\"fuel_rate\"\n",
        "    ]\n",
        "\n",
        "    speed = np.clip(rng.normal(80, 15, n), 0, None)\n",
        "    throttle = np.clip(rng.beta(2, 2, n), 0, 1)\n",
        "    brake = np.clip(1 - throttle + rng.normal(0, 0.15, n), 0, 1)\n",
        "    steering = rng.normal(0, 10, n)\n",
        "    gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, n), 1, 7)\n",
        "    accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, n)\n",
        "    accel_lat = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, n)\n",
        "    yaw_rate = rng.normal(steering/30 * (speed/60), 0.2, n)\n",
        "    road_grade = rng.normal(0, 2, n)\n",
        "    ambient_temp = rng.normal(20, 8, n)\n",
        "\n",
        "    # Correlated tires via Gaussian-copula-like construction\n",
        "    Sigma = (1 - tire_corr) * np.eye(4) + tire_corr * np.ones((4,4))\n",
        "    L = np.linalg.cholesky(Sigma)\n",
        "    z = rng.normal(size=(n,4)) @ L.T\n",
        "    tires_base = 34 + 1.0*z  # ~ N(34,1) with correlation\n",
        "    low_mask = (rng.uniform(0,1,n) < 0.15).astype(float)\n",
        "    tire_drop = (rng.normal(4, 1.0, (n,4)) * low_mask[:,None])\n",
        "    tires = tires_base - tire_drop\n",
        "\n",
        "    engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, n), 0, 100)\n",
        "    maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,n), 0, None)\n",
        "    intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, n), -10, 80)\n",
        "    battery_v = np.clip(rng.normal(13.8, 0.3, n) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "    fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,n), 0, None)\n",
        "    rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, n), 700, 7000)\n",
        "\n",
        "    X_df = pd.DataFrame({\n",
        "        \"speed_kph\": speed,\n",
        "        \"rpm\": rpm,\n",
        "        \"throttle\": throttle,\n",
        "        \"brake\": brake,\n",
        "        \"steering_deg\": steering,\n",
        "        \"gear\": gear,\n",
        "        \"accel_long\": accel_long,\n",
        "        \"accel_lat\": accel_lat,\n",
        "        \"yaw_rate\": yaw_rate,\n",
        "        \"road_grade\": road_grade,\n",
        "        \"ambient_temp\": ambient_temp,\n",
        "        \"tire_fl\": tires[:,0],\n",
        "        \"tire_fr\": tires[:,1],\n",
        "        \"tire_rl\": tires[:,2],\n",
        "        \"tire_rr\": tires[:,3],\n",
        "        \"engine_load\": engine_load,\n",
        "        \"maf\": maf,\n",
        "        \"intake_air_temp\": intake_air_temp,\n",
        "        \"battery_v\": battery_v,\n",
        "        \"fuel_rate\": fuel_rate,\n",
        "    })\n",
        "\n",
        "    low_tire = (32 - X_df[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "    risk_logit = (\n",
        "        1.2*(X_df[\"speed_kph\"]-110)/20\n",
        "        + 1.1*X_df[\"brake\"]\n",
        "        + 0.9*np.abs(X_df[\"steering_deg\"])/15\n",
        "        + 0.7*np.abs(X_df[\"yaw_rate\"])\n",
        "        + 0.8*(low_tire)\n",
        "        + 0.7*(X_df[\"engine_load\"]/100)\n",
        "        + 0.3*(X_df[\"road_grade\"]/5)\n",
        "        - 0.2*(X_df[\"battery_v\"]-13.5)\n",
        "    )\n",
        "    p = sigmoid(risk_logit + check_random_state(random_state).normal(0, 0.4, n))\n",
        "    y = (check_random_state(random_state+1).uniform(0,1,n) < p).astype(int)\n",
        "    return X_df, y, feat_names\n",
        "\n",
        "# -----------------------------\n",
        "# ExCIR (observation-only form)\n",
        "# -----------------------------\n",
        "def excir_scores(X_arr: np.ndarray, y_arr: np.ndarray, feat_names):\n",
        "    n = X_arr.shape[0]\n",
        "    yhat = y_arr.mean()\n",
        "    scores = {}\n",
        "    for j, name in enumerate(feat_names):\n",
        "        f = X_arr[:, j]\n",
        "        fhat = f.mean()\n",
        "        m = 0.5*(fhat + yhat)\n",
        "        num = n*((fhat - m)**2 + (yhat - m)**2)\n",
        "        den = np.sum((f - m)**2) + np.sum((y_arr - m)**2)\n",
        "        eta = float(num/den) if den > 0 else 0.0\n",
        "        scores[name] = eta\n",
        "    return scores\n",
        "\n",
        "def rank_by_scores(scores: dict):\n",
        "    return sorted(scores.keys(), key=lambda k: scores[k], reverse=True)\n",
        "\n",
        "def topk_overlap(rank_a, rank_b, k):\n",
        "    set_a = set(rank_a[:k])\n",
        "    set_b = set(rank_b[:k])\n",
        "    return len(set_a & set_b) / float(k)\n",
        "\n",
        "# -----------------------------\n",
        "# Main: synthesize → train → predict → evaluate perturbations\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # data + split\n",
        "    X, y, feat_names = make_vehicle_dataset(n=6000, random_state=7, tire_corr=0.35)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # small model, validation probabilities as y'\n",
        "    clf = GradientBoostingClassifier(n_estimators=150, learning_rate=0.08, max_depth=3, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred_val = clf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    Xv = X_val.to_numpy()\n",
        "    base_scores = excir_scores(Xv, y_pred_val, feat_names)\n",
        "    base_rank = rank_by_scores(base_scores)\n",
        "\n",
        "    # perturbations\n",
        "    B = 100\n",
        "    K = 8\n",
        "    rng2 = np.random.default_rng(123)\n",
        "\n",
        "    def iid_row_bootstrap(X_arr, y_arr, B=100):\n",
        "        n = X_arr.shape[0]\n",
        "        results = []\n",
        "        for _ in range(B):\n",
        "            idx = rng2.integers(0, n, size=n)  # with replacement\n",
        "            s = excir_scores(X_arr[idx], y_arr[idx], feat_names)\n",
        "            r = rank_by_scores(s)\n",
        "            results.append((s, r))\n",
        "        return results\n",
        "\n",
        "    def block_bootstrap_rows(X_arr, y_arr, B=100):\n",
        "        # row-wise “block” via y quartiles (use session/segment IDs if you have them)\n",
        "        q = pd.qcut(y_arr, q=4, labels=False, duplicates='drop')\n",
        "        strata = [np.where(q == i)[0] for i in np.unique(q)]\n",
        "        results = []\n",
        "        for _ in range(B):\n",
        "            idxs = []\n",
        "            for s in strata:\n",
        "                if len(s) == 0:\n",
        "                    continue\n",
        "                take = rng2.integers(0, len(s), size=len(s))\n",
        "                idxs.append(s[take])\n",
        "            idx = np.concatenate(idxs)\n",
        "            s = excir_scores(X_arr[idx], y_arr[idx], feat_names)\n",
        "            r = rank_by_scores(s)\n",
        "            results.append((s, r))\n",
        "        return results\n",
        "\n",
        "    def input_noise(X_arr, y_arr, sigma=0.05, B=100):\n",
        "        X_std = X_arr.std(axis=0, ddof=1)\n",
        "        X_std[X_std == 0] = 1.0\n",
        "        results = []\n",
        "        for _ in range(B):\n",
        "            noise = rng2.normal(0.0, sigma, size=X_arr.shape) * X_std\n",
        "            Xn = X_arr + noise\n",
        "            s = excir_scores(Xn, y_arr, feat_names)\n",
        "            r = rank_by_scores(s)\n",
        "            results.append((s, r))\n",
        "        return results\n",
        "\n",
        "    def summarize(results, base_rank, K):\n",
        "        overlaps, taus = [], []\n",
        "        base_pos = {f: i for i, f in enumerate(base_rank)}\n",
        "        for (_, r) in results:\n",
        "            overlaps.append(topk_overlap(base_rank, r, K))\n",
        "            pos_b = [base_pos[f] for f in r]\n",
        "            perm_a = np.arange(len(base_rank))\n",
        "            perm_b = np.array(pos_b)\n",
        "            tau, _ = kendalltau(perm_a, perm_b)\n",
        "            taus.append(tau)\n",
        "        return float(np.mean(overlaps)), float(np.mean(taus))\n",
        "\n",
        "    iid_res   = iid_row_bootstrap(Xv, y_pred_val, B=B)\n",
        "    blk_res   = block_bootstrap_rows(Xv, y_pred_val, B=B)\n",
        "    noise_res = input_noise(Xv, y_pred_val, sigma=0.05, B=B)\n",
        "\n",
        "    iid_overlap, iid_tau     = summarize(iid_res, base_rank, K)\n",
        "    blk_overlap, blk_tau     = summarize(blk_res, base_rank, K)\n",
        "    noise_overlap, noise_tau = summarize(noise_res, base_rank, K)\n",
        "\n",
        "    # print LaTeX-ready rows\n",
        "    def to_latex_row(name, ovl, tau):\n",
        "        return f\"{name} & {ovl:.3f} & {tau:.2f} \\\\\\\\\"\n",
        "\n",
        "    print(\"\\nLaTeX rows to paste into your table:\")\n",
        "    print(to_latex_row(\"IID row bootstrap ($B{=}100$)\", iid_overlap, iid_tau))\n",
        "    print(to_latex_row(\"Block bootstrap (quartile strata)\", blk_overlap, blk_tau))\n",
        "    print(to_latex_row(\"Input noise ($\\\\mathcal{N}(0,0.05^2)$)\", noise_overlap, noise_tau))\n"
      ],
      "metadata": {
        "id": "6a07ybcLzL19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from scipy.stats import entropy\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "# Inputs you provide per dataset:\n",
        "# X_full: (n_full, d)   features (or embeddings for images)\n",
        "# X_lw:   (n_lw, d)\n",
        "# yhat_full: model outputs on full (probs or logits)\n",
        "# yhat_lw:   model outputs on lw  (same model)\n",
        "# ytrue_full, ytrue_lw: true labels for risk gap\n",
        "\n",
        "# ---- 1) Projection distance Δ_proj (principal-angle subspace distance)\n",
        "def projection_distance(X_full, X_lw, k=10, random_state=0):\n",
        "    k = min(k, X_full.shape[1], X_lw.shape[1])\n",
        "    pca_full = PCA(n_components=k, random_state=random_state).fit(X_full)\n",
        "    pca_lw   = PCA(n_components=k, random_state=random_state).fit(X_lw)\n",
        "    U = pca_full.components_.T   # d x k\n",
        "    V = pca_lw.components_.T     # d x k\n",
        "    # principal angles via SVD of U^T V\n",
        "    s = np.linalg.svd(U.T @ V, compute_uv=False)\n",
        "    # s are cos(theta_i); projection 2-norm distance = sin(max angle)\n",
        "    theta_max = np.arccos(np.clip(s.min(), -1, 1))\n",
        "    return np.sin(theta_max)\n",
        "\n",
        "# ---- 2) MMD two-sample test (RBF kernel); return a p-value via permutation\n",
        "def mmd_rbf_pvalue(X_full, X_lw, gamma=None, n_perm=500, rng=0):\n",
        "    rng = np.random.default_rng(rng)\n",
        "    X = np.vstack([X_full, X_lw])\n",
        "    n, m = len(X_full), len(X_lw)\n",
        "    if gamma is None:\n",
        "        # median heuristic on pooled data\n",
        "        idx = rng.choice(len(X), size=min(1000, len(X)), replace=False)\n",
        "        D2 = np.sum((X[idx,None,:]-X[None,idx,:])**2, axis=-1)\n",
        "        med = np.median(D2[np.triu_indices_from(D2,1)])\n",
        "        gamma = 1.0 / (med + 1e-8)\n",
        "    K = rbf_kernel(X, X, gamma=gamma)\n",
        "    e = np.eye(n+m, dtype=bool)\n",
        "    # unbiased MMD^2\n",
        "    Kxx = (K[:n,:n][~e[:n,:n]]).mean()\n",
        "    Kyy = (K[n:,n:][~e[n:,n:]]).mean()\n",
        "    Kxy = K[:n,n:].mean()\n",
        "    mmd2 = Kxx + Kyy - 2*Kxy\n",
        "    # permutation p-value\n",
        "    greater = 0\n",
        "    for _ in range(n_perm):\n",
        "        perm = rng.permutation(n+m)\n",
        "        A, B = perm[:n], perm[n:]\n",
        "        Kxx_p = (K[np.ix_(A,A)][~e[:n,:n]]).mean()\n",
        "        Kyy_p = (K[np.ix_(B,B)][~e[:m,:m]]).mean()\n",
        "        Kxy_p = K[np.ix_(A,B)].mean()\n",
        "        mmd2_p = Kxx_p + Kyy_p - 2*Kxy_p\n",
        "        greater += (mmd2_p >= mmd2)\n",
        "    pval = (greater + 1) / (n_perm + 1)\n",
        "    return pval\n",
        "\n",
        "# ---- 3) KL between output distributions (histogram smoothing)\n",
        "def kl_outputs(yhat_full, yhat_lw, bins=50, eps=1e-8):\n",
        "    # if multi-class probs: flatten to scalar confidence via max prob\n",
        "    if yhat_full.ndim == 2:\n",
        "        s_full = yhat_full.max(axis=1)\n",
        "        s_lw   = yhat_lw.max(axis=1)\n",
        "    else:\n",
        "        s_full = yhat_full\n",
        "        s_lw   = yhat_lw\n",
        "    hist_f, edges = np.histogram(s_full, bins=bins, range=(0,1), density=True)\n",
        "    hist_l, _     = np.histogram(s_lw,   bins=edges, density=True)\n",
        "    p = (hist_f + eps) / (hist_f + eps).sum()\n",
        "    q = (hist_l + eps) / (hist_l + eps).sum()\n",
        "    return float(entropy(p, q))  # KL(P_full || P_lw)\n",
        "\n",
        "# ---- 4) Risk gap (accuracy/F1 ratio)\n",
        "def risk_gap(ytrue_full, ytrue_lw, ypred_full, ypred_lw, average='macro'):\n",
        "    acc_full = accuracy_score(ytrue_full, ypred_full)\n",
        "    acc_lw   = accuracy_score(ytrue_lw,   ypred_lw)\n",
        "    f1_full  = f1_score(ytrue_full, ypred_full, average=average)\n",
        "    f1_lw    = f1_score(ytrue_lw,   ypred_lw,   average=average)\n",
        "    # choose metric you report (acc or F1); here F1 ratio:\n",
        "    ratio_f1 = f1_lw / (f1_full + 1e-8)\n",
        "    ratio_acc = acc_lw / (acc_full + 1e-8)\n",
        "    return ratio_acc, ratio_f1\n"
      ],
      "metadata": {
        "id": "Mw7MQz1LzONv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Optional\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# ----------------- config -----------------\n",
        "OUT_TABLE = \"lw_similarity_table.tex\"\n",
        "OUT_HEAD  = \"head_stability_line.tex\"\n",
        "RNG = np.random.default_rng(17)\n",
        "\n",
        "# ----------------- IO helpers -----------------\n",
        "def _load_array(prefix: str) -> Optional[np.ndarray]:\n",
        "    \"\"\"Load prefix.npy or prefix.csv (drops unnamed index column).\"\"\"\n",
        "    npy = Path(prefix + \".npy\")\n",
        "    csv = Path(prefix + \".csv\")\n",
        "    if npy.exists():\n",
        "        return np.load(npy)\n",
        "    if csv.exists():\n",
        "        df = pd.read_csv(csv)\n",
        "        if df.columns[0].lower().startswith(\"unnamed\"):\n",
        "            df = df.drop(columns=[df.columns[0]])\n",
        "        return df.values.squeeze() if df.shape[1] == 1 else df.values\n",
        "    return None\n",
        "\n",
        "# ----------------- metrics -----------------\n",
        "def projection_distance(X_full: np.ndarray, X_lw: np.ndarray, k: int = 10, seed: int = 0) -> float:\n",
        "    k = min(k, X_full.shape[1], X_lw.shape[1])\n",
        "    pca_full = PCA(n_components=k, random_state=seed).fit(X_full)\n",
        "    pca_lw   = PCA(n_components=k, random_state=seed).fit(X_lw)\n",
        "    U = pca_full.components_.T\n",
        "    V = pca_lw.components_.T\n",
        "    # principal angles via SVD of U^T V\n",
        "    s = np.linalg.svd(U.T @ V, compute_uv=False)\n",
        "    theta_max = np.arccos(np.clip(s.min(), -1, 1))\n",
        "    return float(np.sin(theta_max))  # Δ_proj = sin(max angle)\n",
        "\n",
        "def mmd_rbf_pvalue(X_full, X_lw, gamma=None, n_perm=500, rng=None, seed=None):\n",
        "    if rng is None and seed is not None:\n",
        "        rng = seed\n",
        "    if rng is None:\n",
        "        rng = 0\n",
        "    rng = np.random.default_rng(rng)\n",
        "    # ... rest of the function unchanged ...\n",
        "\n",
        "    X = np.vstack([X_full, X_lw])\n",
        "    n, m = len(X_full), len(X_lw)\n",
        "\n",
        "    # median heuristic for gamma if not provided\n",
        "    if gamma is None:\n",
        "        idx = rng.choice(len(X), size=min(1000, len(X)), replace=False)\n",
        "        D2 = np.sum((X[idx, None, :] - X[None, idx, :])**2, axis=-1)\n",
        "        med = np.median(D2[np.triu_indices_from(D2, 1)])\n",
        "        if med <= 0:\n",
        "            med = np.mean(D2[np.triu_indices_from(D2, 1)]) + 1e-8\n",
        "        gamma = 1.0 / (med + 1e-8)\n",
        "\n",
        "    # full kernel once\n",
        "    K = rbf_kernel(X, X, gamma=gamma)\n",
        "\n",
        "    def offdiag_mean(M):\n",
        "        # unbiased mean of off-diagonal entries for a square matrix\n",
        "        s = M.shape[0]\n",
        "        if s <= 1:\n",
        "            return 0.0\n",
        "        return (M.sum() - np.trace(M)) / (s * (s - 1))\n",
        "\n",
        "    # base (non-permuted) MMD^2\n",
        "    Kxx = offdiag_mean(K[:n, :n])\n",
        "    Kyy = offdiag_mean(K[n:, n:])\n",
        "    Kxy = K[:n, n:].mean()\n",
        "    mmd2 = Kxx + Kyy - 2.0 * Kxy\n",
        "\n",
        "    # permutation distribution\n",
        "    greater = 0\n",
        "    for _ in range(n_perm):\n",
        "        perm = rng.permutation(n + m)\n",
        "        A, B = perm[:n], perm[n:]          # sizes n and m (fixed)\n",
        "        Kxx_p = offdiag_mean(K[np.ix_(A, A)])\n",
        "        Kyy_p = offdiag_mean(K[np.ix_(B, B)])\n",
        "        Kxy_p = K[np.ix_(A, B)].mean()\n",
        "        mmd2_p = Kxx_p + Kyy_p - 2.0 * Kxy_p\n",
        "        if mmd2_p >= mmd2:\n",
        "            greater += 1\n",
        "\n",
        "    # +1 for a conservative finite-sample estimate\n",
        "    pval = (greater + 1) / (n_perm + 1)\n",
        "    return float(pval)\n",
        "\n",
        "\n",
        "def kl_outputs(yhat_full: np.ndarray, yhat_lw: np.ndarray, bins: int = 50, eps: float = 1e-8) -> float:\n",
        "    # multi-class: use max prob; binary: accept 1D prob\n",
        "    if yhat_full.ndim == 2:\n",
        "        s_full = yhat_full.max(axis=1)\n",
        "        s_lw   = yhat_lw.max(axis=1)\n",
        "    else:\n",
        "        s_full = yhat_full\n",
        "        s_lw   = yhat_lw\n",
        "    hist_f, edges = np.histogram(s_full, bins=bins, range=(0,1), density=True)\n",
        "    hist_l, _     = np.histogram(s_lw,   bins=edges, density=True)\n",
        "    p = (hist_f + eps) / (hist_f + eps).sum()\n",
        "    q = (hist_l + eps) / (hist_l + eps).sum()\n",
        "    return float(entropy(p, q))  # KL(P_full || P_lw)\n",
        "\n",
        "def risk_ratios(ytrue_full, ytrue_lw, ypred_full, ypred_lw, average='macro') -> Tuple[float, float]:\n",
        "    from sklearn.metrics import f1_score, accuracy_score\n",
        "    acc_full = accuracy_score(ytrue_full, ypred_full)\n",
        "    acc_lw   = accuracy_score(ytrue_lw,   ypred_lw)\n",
        "    f1_full  = f1_score(ytrue_full, ypred_full, average=average)\n",
        "    f1_lw    = f1_score(ytrue_lw,   ypred_lw,   average=average)\n",
        "    return float(acc_lw / (acc_full + 1e-8)), float(f1_lw / (f1_full + 1e-8))\n",
        "\n",
        "# ----------------- ExCIR for head stability (observation-only) -----------------\n",
        "def excir_scores(X: np.ndarray, y: np.ndarray) -> dict:\n",
        "    n = X.shape[0]\n",
        "    yhat = y.mean()\n",
        "    scores = {}\n",
        "    for j in range(X.shape[1]):\n",
        "        f = X[:, j]\n",
        "        fhat = f.mean()\n",
        "        m = 0.5*(fhat + yhat)\n",
        "        num = n*((fhat - m)**2 + (yhat - m)**2)\n",
        "        den = np.sum((f - m)**2) + np.sum((y - m)**2)\n",
        "        eta = float(num/den) if den > 0 else 0.0\n",
        "        scores[j] = eta\n",
        "    return scores\n",
        "\n",
        "def rank_order(scores: dict):\n",
        "    return sorted(scores.keys(), key=lambda k: scores[k], reverse=True)\n",
        "\n",
        "def topk_overlap(a, b, k):\n",
        "    return len(set(a[:k]) & set(b[:k]))/k\n",
        "\n",
        "# ----------------- synthetic vehicular fallback (only if files missing) -----------------\n",
        "def synth_vehicular(n=3000, seed=9):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    speed = rng.normal(80, 20, n).clip(0, 160)\n",
        "    throttle = rng.uniform(0, 1, n)\n",
        "    brake = (rng.beta(2, 10, n) > 0.8).astype(float)\n",
        "    steering = rng.normal(0, 8, n)\n",
        "    yaw = rng.normal(0, 2, n)\n",
        "    road = rng.normal(0, 3, n)\n",
        "    rpm = 800 + 30*speed + rng.normal(0, 150, n) - 150*brake\n",
        "    engine_load = np.clip(0.2 + 0.5*throttle + 0.1*(speed/140) + 0.05*(road/10) + rng.normal(0, 0.05, n), 0, 1)\n",
        "    maf = 5 + 0.08*rpm/100 + 2*throttle + rng.normal(0, 0.5, n)\n",
        "    tire_base = rng.normal(32, 2, n)\n",
        "    tire_rr = tire_base + rng.normal(0, 0.7, n)\n",
        "    tire_rl = tire_base + rng.normal(0, 0.7, n)\n",
        "    tire_fr = tire_base + rng.normal(0, 0.7, n)\n",
        "    tire_fl = tire_base + rng.normal(0, 0.7, n)\n",
        "    X = np.column_stack([speed, throttle, brake, steering, yaw, road, rpm, engine_load, maf, tire_rr, tire_rl, tire_fr, tire_fl])\n",
        "    low_tire = (np.minimum.reduce([tire_rr, tire_rl, tire_fr, tire_fl]) < 30).astype(float)\n",
        "    lin = 1.5*brake + 0.7*np.abs(steering)/10 + 0.6*low_tire + 0.5*engine_load + 0.3*road/10 + rng.normal(0, 0.4, n)\n",
        "    prob = 1/(1+np.exp(-lin))\n",
        "    y = (rng.uniform(0,1,n) < prob).astype(int)\n",
        "    yhat = prob\n",
        "    idx = rng.choice(n, size=n//2, replace=False)\n",
        "    return (X, yhat, y), (X[idx], yhat[idx], y[idx])\n",
        "\n",
        "# ----------------- measure a dataset -----------------\n",
        "def measure_dataset(tag: str):\n",
        "    \"\"\"\n",
        "    Returns strings for table cells: Δ_proj, p_mmd, KL, risk ratio (F1).\n",
        "    File naming convention per dataset tag: EEG, Vehicular, Digits, Cats--Dogs\n",
        "      X_full_<tag>.*, X_lw_<tag>.*, yhat_full_<tag>.*, yhat_lw_<tag>.*, ytrue_full_<tag>.*, ytrue_lw_<tag>.*\n",
        "    \"\"\"\n",
        "    tag_fs = tag  # as used in filenames\n",
        "    Xf = _load_array(f\"X_full_{tag_fs}\")\n",
        "    Xl = _load_array(f\"X_lw_{tag_fs}\")\n",
        "    yhatf = _load_array(f\"yhat_full_{tag_fs}\")\n",
        "    yhatl = _load_array(f\"yhat_lw_{tag_fs}\")\n",
        "    ytf = _load_array(f\"ytrue_full_{tag_fs}\")\n",
        "    ytl = _load_array(f\"ytrue_lw_{tag_fs}\")\n",
        "\n",
        "    if tag.lower() == \"vehicular\" and (Xf is None or Xl is None or yhatf is None or yhatl is None or ytf is None or ytl is None):\n",
        "        # synth fallback so you can see a filled column\n",
        "        (Xf, yhatf, ytf), (Xl, yhatl, ytl) = synth_vehicular()\n",
        "\n",
        "    # Missing -> dashes\n",
        "    if any(obj is None for obj in [Xf, Xl, yhatf, yhatl, ytf, ytl]):\n",
        "        return \"—\", \"—\", \"—\", \"—\"\n",
        "\n",
        "    if Xf.ndim == 1: Xf = Xf[:, None]\n",
        "    if Xl.ndim == 1: Xl = Xl[:, None]\n",
        "    # get hard preds for ratios\n",
        "    if yhatf.ndim == 2:\n",
        "        ypf = yhatf.argmax(axis=1)\n",
        "        ypl = yhatl.argmax(axis=1)\n",
        "    else:\n",
        "        ypf = (yhatf >= 0.5).astype(int)\n",
        "        ypl = (yhatl >= 0.5).astype(int)\n",
        "\n",
        "    dproj = projection_distance(Xf, Xl, k=min(10, Xf.shape[1], Xl.shape[1]))\n",
        "    p_mmd = mmd_rbf_pvalue(Xf, Xl, n_perm=200, seed=3)\n",
        "    kl = kl_outputs(yhatf, yhatl)\n",
        "    _, f1_ratio = risk_ratios(ytf, ytl, ypf, ypl, average='macro')\n",
        "\n",
        "    return f\"{dproj:.3f}\", f\"{p_mmd:.2f}\", f\"{kl:.3f}\", f\"{f1_ratio:.3f}\"\n",
        "\n",
        "# ----------------- head-stability builder -----------------\n",
        "def build_head_stability_line(overlaps: dict) -> str:\n",
        "    def fmt(pair):\n",
        "        return f\"{pair[0]:.2f}-{pair[1]:.2f}\" if pair is not None else r\"\\textemdash{}\"\n",
        "    eeg = fmt(overlaps.get(\"EEG\"))\n",
        "    veh = fmt(overlaps.get(\"Vehicular\"))\n",
        "    dig = fmt(overlaps.get(\"Digits\"))\n",
        "    cat = fmt(overlaps.get(\"Cats--Dogs\"))\n",
        "    return (r\"\\noindent\\textit{Head stability as the criterion.} \"\n",
        "            r\"We optimize for compact dashboards and retrain budgets; thus stability of the head is the relevant objective. \"\n",
        "            r\"Despite mid-tail reshuffling (Kendall--$\\tau$ moderate), top-set overlap at $k\\!\\in\\![8,10]$ remains high \"\n",
        "            rf\"(EEG: {eeg}, Vehicular: {veh}, Digits: {dig}, Cats--Dogs: {cat}), \"\n",
        "            r\"aligning with ROAR-style sufficiency gains reported in Table~\\ref{tab:comp}.\")\n",
        "\n",
        "# ----------------- compute & write -----------------\n",
        "if __name__ == \"__main__\":\n",
        "    datasets = [\"EEG\", \"Vehicular\", \"Digits\", \"Cats--Dogs\"]\n",
        "    rows = {tag: measure_dataset(tag) for tag in datasets}\n",
        "\n",
        "    alpha = r\"\\alpha\"; beta = r\"\\beta\"; gamma = r\"\\gamma\"; eps = r\"\\varepsilon_{\\text{acc}}\"\n",
        "    lines = [\n",
        "        r\"\\begin{table}[ht]\",\n",
        "        r\"\\centering\",\n",
        "        r\"\\caption{Lightweight (LW) environment acceptance criteria and measured similarity. A dataset passes if all checks are within thresholds. Thresholds are chosen a priori and applied uniformly.}\",\n",
        "        r\"\\label{tab:lw_similarity}\",\n",
        "        r\"\\begin{tabular}{lccccc}\",\n",
        "        r\"\\toprule\",\n",
        "        r\"\\textbf{Check} & \\textbf{Threshold} & \\textbf{EEG (meas.)} & \\textbf{Vehicular (meas.)} & \\textbf{Digits (meas.)} & \\textbf{Cats--Dogs (meas.)} \\\\\",\n",
        "        r\"\\midrule\",\n",
        "        rf\"Projection distance $\\Delta_{{\\mathrm{{proj}}}}$ & $\\le {alpha}$ & {rows['EEG'][0]} & {rows['Vehicular'][0]} & {rows['Digits'][0]} & {rows['Cats--Dogs'][0]} \\\\\",\n",
        "        rf\"MMD two-sample $p$-value & $\\ge {beta}$ & {rows['EEG'][1]} & {rows['Vehicular'][1]} & {rows['Digits'][1]} & {rows['Cats--Dogs'][1]} \\\\\",\n",
        "        rf\"KL\\big(\" + r\"P_{\\text{full}}\\!\\parallel\\!P_{\\text{LW}}\" + r\"$\\big) & $\\le \" + gamma + r\"$ & \" + rows['EEG'][2] + r\" & \" + rows['Vehicular'][2] + r\" & \" + rows['Digits'][2] + r\" & \" + rows['Cats--Dogs'][2] + r\" \\\\\",\n",
        "        rf\"Risk gap (acc./F1 ratio) & $\\ge 1-{eps}$ & {rows['EEG'][3]} & {rows['Vehicular'][3]} & {rows['Digits'][3]} & {rows['Cats--Dogs'][3]} \\\\\",\n",
        "        r\"\\bottomrule\",\n",
        "        r\"\\end{tabular}\",\n",
        "        r\"\\end{table}\"\n",
        "    ]\n",
        "    latex_table = \"\\n\".join(lines)\n",
        "\n",
        "    # Head stability values:\n",
        "    # If you have baseline + bootstrap rankings per dataset, compute (ovl_k8, ovl_k10) and replace below.\n",
        "    overlaps = {\"EEG\": None, \"Vehicular\": None, \"Digits\": None, \"Cats--Dogs\": None}\n",
        "\n",
        "    # As a demo, produce Vehicular overlaps from a quick bootstrap on the synthesized set\n",
        "    (veh_Xf, veh_yhatf, _), _ = synth_vehicular(n=3000, seed=9)\n",
        "    base_scores = excir_scores(veh_Xf, veh_yhatf)\n",
        "    base_rank = rank_order(base_scores)\n",
        "    ov8, ov10 = [], []\n",
        "    for _ in range(50):\n",
        "        idx = RNG.integers(0, veh_Xf.shape[0], size=veh_Xf.shape[0])\n",
        "        s = excir_scores(veh_Xf[idx], veh_yhatf[idx])\n",
        "        r = rank_order(s)\n",
        "        ov8.append(topk_overlap(base_rank, r, 8))\n",
        "        ov10.append(topk_overlap(base_rank, r, 10))\n",
        "    overlaps[\"Vehicular\"] = (float(np.mean(ov8)), float(np.mean(ov10)))\n",
        "\n",
        "    head_line = build_head_stability_line(overlaps)\n",
        "\n",
        "    # Write files\n",
        "    Path(OUT_TABLE).write_text(latex_table)\n",
        "    Path(OUT_HEAD).write_text(head_line)\n",
        "\n",
        "    print(\"\\n=== LaTeX table ===\\n\")\n",
        "    print(latex_table)\n",
        "    print(\"\\n=== Head stability line ===\\n\")\n",
        "    print(head_line)\n",
        "    print(f\"\\nSaved:\\n  - {OUT_TABLE}\\n  - {OUT_HEAD}\")\n"
      ],
      "metadata": {
        "id": "aTDWLE53zQkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Optional\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from scipy.stats import entropy\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------- config -----------------\n",
        "OUT_TABLE = \"lw_similarity_table.tex\"\n",
        "OUT_HEAD  = \"head_stability_line.tex\"\n",
        "RNG = np.random.default_rng(17)\n",
        "\n",
        "# ----------------- IO helpers -----------------\n",
        "def _load_array(prefix: str) -> Optional[np.ndarray]:\n",
        "    npy = Path(prefix + \".npy\")\n",
        "    csv = Path(prefix + \".csv\")\n",
        "    if npy.exists():\n",
        "        return np.load(npy)\n",
        "    if csv.exists():\n",
        "        df = pd.read_csv(csv)\n",
        "        if df.columns[0].lower().startswith(\"unnamed\"):\n",
        "            df = df.drop(columns=[df.columns[0]])\n",
        "        return df.values.squeeze() if df.shape[1] == 1 else df.values\n",
        "    return None\n",
        "\n",
        "# ----------------- metrics -----------------\n",
        "def projection_distance(X_full: np.ndarray, X_lw: np.ndarray, k: int = 10, seed: int = 0) -> float:\n",
        "    k = min(k, X_full.shape[1], X_lw.shape[1])\n",
        "    pca_full = PCA(n_components=k, random_state=seed).fit(X_full)\n",
        "    pca_lw   = PCA(n_components=k, random_state=seed).fit(X_lw)\n",
        "    U = pca_full.components_.T\n",
        "    V = pca_lw.components_.T\n",
        "    s = np.linalg.svd(U.T @ V, compute_uv=False)\n",
        "    theta_max = np.arccos(np.clip(s.min(), -1, 1))\n",
        "    return float(np.sin(theta_max))  # Δ_proj\n",
        "\n",
        "def _offdiag_mean(M: np.ndarray) -> float:\n",
        "    s = M.shape[0]\n",
        "    if s <= 1: return 0.0\n",
        "    return float((M.sum() - np.trace(M)) / (s * (s - 1)))\n",
        "\n",
        "def mmd_rbf_pvalue_fast(\n",
        "    X_full: np.ndarray,\n",
        "    X_lw: np.ndarray,\n",
        "    gamma: float = None,\n",
        "    n_perm: int = 100,\n",
        "    rng: int = 0,\n",
        "    max_rows: int = 2000\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Faster MMD p-value:\n",
        "      - subsample to at most 'max_rows' total rows,\n",
        "      - 100 permutations by default.\n",
        "    \"\"\"\n",
        "    rs = np.random.default_rng(rng)\n",
        "    # Subsample rows if large\n",
        "    n, m = len(X_full), len(X_lw)\n",
        "    total = n + m\n",
        "    if total > max_rows:\n",
        "        take_full = int(round(n / total * max_rows))\n",
        "        take_lw   = max_rows - take_full\n",
        "        idx_f = rs.choice(n, size=take_full, replace=False)\n",
        "        idx_l = rs.choice(m, size=take_lw,  replace=False)\n",
        "        Xf = X_full[idx_f]\n",
        "        Xl = X_lw[idx_l]\n",
        "        n, m = len(Xf), len(Xl)\n",
        "    else:\n",
        "        Xf, Xl = X_full, X_lw\n",
        "\n",
        "    X = np.vstack([Xf, Xl])\n",
        "\n",
        "    # median heuristic\n",
        "    if gamma is None:\n",
        "        idx = rs.choice(len(X), size=min(1000, len(X)), replace=False)\n",
        "        D2 = np.sum((X[idx, None, :] - X[None, idx, :])**2, axis=-1)\n",
        "        med = np.median(D2[np.triu_indices_from(D2, 1)])\n",
        "        if med <= 0:\n",
        "            med = np.mean(D2[np.triu_indices_from(D2, 1)]) + 1e-8\n",
        "        gamma = 1.0 / (med + 1e-8)\n",
        "\n",
        "    # kernel once\n",
        "    K = rbf_kernel(X, X, gamma=gamma)\n",
        "    Kxx = _offdiag_mean(K[:n, :n])\n",
        "    Kyy = _offdiag_mean(K[n:, n:])\n",
        "    Kxy = K[:n, n:].mean()\n",
        "    mmd2 = Kxx + Kyy - 2.0 * Kxy\n",
        "\n",
        "    greater = 0\n",
        "    for _ in range(n_perm):\n",
        "        perm = rs.permutation(n + m)\n",
        "        A, B = perm[:n], perm[n:]\n",
        "        Kxx_p = _offdiag_mean(K[np.ix_(A, A)])\n",
        "        Kyy_p = _offdiag_mean(K[np.ix_(B, B)])\n",
        "        Kxy_p = K[np.ix_(A, B)].mean()\n",
        "        mmd2_p = Kxx_p + Kyy_p - 2.0 * Kxy_p\n",
        "        if mmd2_p >= mmd2:\n",
        "            greater += 1\n",
        "    return float((greater + 1) / (n_perm + 1))\n",
        "\n",
        "def kl_outputs(yhat_full: np.ndarray, yhat_lw: np.ndarray, bins: int = 50, eps: float = 1e-8) -> float:\n",
        "    if yhat_full.ndim == 2:\n",
        "        s_full = yhat_full.max(axis=1)\n",
        "        s_lw   = yhat_lw.max(axis=1)\n",
        "    else:\n",
        "        s_full = yhat_full\n",
        "        s_lw   = yhat_lw\n",
        "    hist_f, edges = np.histogram(s_full, bins=bins, range=(0,1), density=True)\n",
        "    hist_l, _     = np.histogram(s_lw,   bins=edges, density=True)\n",
        "    p = (hist_f + eps) / (hist_f + eps).sum()\n",
        "    q = (hist_l + eps) / (hist_l + eps).sum()\n",
        "    return float(entropy(p, q))  # KL(P_full || P_lw)\n",
        "\n",
        "def risk_ratios(ytrue_full, ytrue_lw, ypred_full, ypred_lw, average='macro') -> Tuple[float, float]:\n",
        "    acc_full = accuracy_score(ytrue_full, ypred_full)\n",
        "    acc_lw   = accuracy_score(ytrue_lw,   ypred_lw)\n",
        "    f1_full  = f1_score(ytrue_full, ypred_full, average=average)\n",
        "    f1_lw    = f1_score(ytrue_lw,   ypred_lw,   average=average)\n",
        "    return float(acc_lw / (acc_full + 1e-8)), float(f1_lw / (f1_full + 1e-8))\n",
        "\n",
        "# ----------------- ExCIR (obs-only) for head stability -----------------\n",
        "def excir_scores(X: np.ndarray, y: np.ndarray) -> dict:\n",
        "    n = X.shape[0]\n",
        "    yhat = y.mean()\n",
        "    scores = {}\n",
        "    for j in range(X.shape[1]):\n",
        "        f = X[:, j]\n",
        "        fhat = f.mean()\n",
        "        m = 0.5*(fhat + yhat)\n",
        "        num = n*((fhat - m)**2 + (yhat - m)**2)\n",
        "        den = np.sum((f - m)**2) + np.sum((y - m)**2)\n",
        "        eta = float(num/den) if den > 0 else 0.0\n",
        "        scores[j] = eta\n",
        "    return scores\n",
        "\n",
        "def rank_order(scores: dict):\n",
        "    return sorted(scores.keys(), key=lambda k: scores[k], reverse=True)\n",
        "\n",
        "def topk_overlap(a, b, k):\n",
        "    return len(set(a[:k]) & set(b[:k]))/k\n",
        "\n",
        "# ----------------- Vehicular surrogate (if files missing) -----------------\n",
        "def synth_vehicular(n=3000, seed=9):\n",
        "    rs = np.random.default_rng(seed)\n",
        "    speed = rs.normal(80, 20, n).clip(0, 160)\n",
        "    throttle = rs.uniform(0, 1, n)\n",
        "    brake = (rs.beta(2, 10, n) > 0.8).astype(float)\n",
        "    steering = rs.normal(0, 8, n)\n",
        "    yaw = rs.normal(0, 2, n)\n",
        "    road = rs.normal(0, 3, n)\n",
        "    rpm = 800 + 30*speed + rs.normal(0, 150, n) - 150*brake\n",
        "    engine_load = np.clip(0.2 + 0.5*throttle + 0.1*(speed/140) + 0.05*(road/10) + rs.normal(0, 0.05, n), 0, 1)\n",
        "    maf = 5 + 0.08*rpm/100 + 2*throttle + rs.normal(0, 0.5, n)\n",
        "    tire_base = rs.normal(32, 2, n)\n",
        "    tire_rr = tire_base + rs.normal(0, 0.7, n)\n",
        "    tire_rl = tire_base + rs.normal(0, 0.7, n)\n",
        "    tire_fr = tire_base + rs.normal(0, 0.7, n)\n",
        "    tire_fl = tire_base + rs.normal(0, 0.7, n)\n",
        "    X = np.column_stack([speed, throttle, brake, steering, yaw, road, rpm, engine_load, maf, tire_rr, tire_rl, tire_fr, tire_fl])\n",
        "    low_tire = (np.minimum.reduce([tire_rr, tire_rl, tire_fr, tire_fl]) < 30).astype(float)\n",
        "    lin = 1.5*brake + 0.7*np.abs(steering)/10 + 0.6*low_tire + 0.5*engine_load + 0.3*road/10 + rs.normal(0, 0.4, n)\n",
        "    prob = 1/(1+np.exp(-lin))\n",
        "    y = (rs.uniform(0,1,n) < prob).astype(int)\n",
        "    yhat = prob\n",
        "    idx = rs.choice(n, size=n//2, replace=False)\n",
        "    return (X, yhat, y), (X[idx], yhat[idx], y[idx])\n",
        "\n",
        "# ----------------- Digits (sklearn) -----------------\n",
        "def build_digits_full_lw():\n",
        "    data = load_digits()\n",
        "    X = data.data.astype(float) / 16.0\n",
        "    y = data.target.astype(int)\n",
        "    # \"Full\" vs \"LW\" environments\n",
        "    X_full, X_lw, y_full, y_lw = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "    # shared model: train on \"full\" only (stricter)\n",
        "    clf = LogisticRegression(max_iter=2000, multi_class=\"auto\", solver=\"lbfgs\")\n",
        "    clf.fit(X_full, y_full)\n",
        "    yhat_full = clf.predict_proba(X_full)\n",
        "    yhat_lw   = clf.predict_proba(X_lw)\n",
        "    return (X_full, yhat_full, y_full), (X_lw, yhat_lw, y_lw)\n",
        "\n",
        "# ----------------- measure set -----------------\n",
        "def measure_dataset(tag: str):\n",
        "    \"\"\"\n",
        "    Returns strings for table cells: Δ_proj, p_mmd, KL, risk ratio (F1).\n",
        "    \"\"\"\n",
        "    if tag == \"Digits\":\n",
        "        (Xf, yhatf, ytf), (Xl, yhatl, ytl) = build_digits_full_lw()\n",
        "    elif tag == \"Vehicular\":\n",
        "        Xf = _load_array(\"X_full_Vehicular\"); Xl = _load_array(\"X_lw_Vehicular\")\n",
        "        yhatf = _load_array(\"yhat_full_Vehicular\"); yhatl = _load_array(\"yhat_lw_Vehicular\")\n",
        "        ytf = _load_array(\"ytrue_full_Vehicular\"); ytl = _load_array(\"ytrue_lw_Vehicular\")\n",
        "        if any(obj is None for obj in [Xf, Xl, yhatf, yhatl, ytf, ytl]):\n",
        "            (Xf, yhatf, ytf), (Xl, yhatl, ytl) = synth_vehicular()\n",
        "    elif tag == \"Cats--Dogs\":\n",
        "        Xf = _load_array(\"X_full_Cats--Dogs\"); Xl = _load_array(\"X_lw_Cats--Dogs\")\n",
        "        yhatf = _load_array(\"yhat_full_Cats--Dogs\"); yhatl = _load_array(\"yhat_lw_Cats--Dogs\")\n",
        "        ytf = _load_array(\"ytrue_full_Cats--Dogs\"); ytl = _load_array(\"ytrue_lw_Cats--Dogs\")\n",
        "        if any(obj is None for obj in [Xf, Xl, yhatf, yhatl, ytf, ytl]):\n",
        "            return \"—\", \"—\", \"—\", \"—\"\n",
        "    else:\n",
        "        return \"—\", \"—\", \"—\", \"—\"\n",
        "\n",
        "    if Xf.ndim == 1: Xf = Xf[:, None]\n",
        "    if Xl.ndim == 1: Xl = Xl[:, None]\n",
        "    if yhatf.ndim == 2:\n",
        "        ypf = yhatf.argmax(axis=1)\n",
        "        ypl = yhatl.argmax(axis=1)\n",
        "    else:\n",
        "        ypf = (yhatf >= 0.5).astype(int)\n",
        "        ypl = (yhatl >= 0.5).astype(int)\n",
        "\n",
        "    dproj = projection_distance(Xf, Xl, k=min(10, Xf.shape[1], Xl.shape[1]))\n",
        "    p_mmd = mmd_rbf_pvalue_fast(Xf, Xl, n_perm=100, rng=3, max_rows=2000)\n",
        "    kl = kl_outputs(yhatf, yhatl)\n",
        "    _, f1_ratio = risk_ratios(ytf, ytl, ypf, ypl, average='macro')\n",
        "\n",
        "    return f\"{dproj:.3f}\", f\"{p_mmd:.2f}\", f\"{kl:.3f}\", f\"{f1_ratio:.3f}\"\n",
        "\n",
        "# ----------------- head-stability (Vehicular + Digits) -----------------\n",
        "def head_stability_for_arrays(X: np.ndarray, yhat: np.ndarray, k_vals=(8,10), B=80, seed=7):\n",
        "    if X.ndim == 1: X = X[:, None]\n",
        "    y1d = yhat if yhat.ndim == 1 else yhat.max(axis=1)\n",
        "    base_r = rank_order(excir_scores(X, y1d))\n",
        "    rs = np.random.default_rng(seed)\n",
        "    ovls = []\n",
        "    for k in k_vals:\n",
        "        xs = []\n",
        "        for _ in range(B):\n",
        "            idx = rs.integers(0, X.shape[0], size=X.shape[0])\n",
        "            r = rank_order(excir_scores(X[idx], y1d[idx]))\n",
        "            xs.append(topk_overlap(base_r, r, k))\n",
        "        ovls.append(float(np.mean(xs)))\n",
        "    return tuple(ovls)\n",
        "\n",
        "# ----------------- run all -----------------\n",
        "datasets = [\"Vehicular\", \"Digits\", \"Cats--Dogs\"]\n",
        "rows = {tag: measure_dataset(tag) for tag in datasets}\n",
        "\n",
        "# Build LaTeX table (no EEG)\n",
        "alpha = r\"\\alpha\"; beta = r\"\\beta\"; gamma = r\"\\gamma\"; eps = r\"\\varepsilon_{\\text{acc}}\"\n",
        "lines = [\n",
        "    r\"\\begin{table}[ht]\",\n",
        "    r\"\\centering\",\n",
        "    r\"\\caption{Lightweight (LW) environment acceptance criteria and measured similarity. A dataset passes if all checks are within thresholds. Thresholds are chosen a priori and applied uniformly.}\",\n",
        "    r\"\\label{tab:lw_similarity}\",\n",
        "    r\"\\begin{tabular}{lcccc}\",\n",
        "    r\"\\toprule\",\n",
        "    r\"\\textbf{Check} & \\textbf{Threshold} & \\textbf{Vehicular (meas.)} & \\textbf{Digits (meas.)} & \\textbf{Cats--Dogs (meas.)} \\\\\",\n",
        "    r\"\\midrule\",\n",
        "    rf\"Projection distance $\\Delta_{{\\mathrm{{proj}}}}$ & $\\le {alpha}$ & {rows['Vehicular'][0]} & {rows['Digits'][0]} & {rows['Cats--Dogs'][0]} \\\\\",\n",
        "    rf\"MMD two-sample $p$-value & $\\ge {beta}$ & {rows['Vehicular'][1]} & {rows['Digits'][1]} & {rows['Cats--Dogs'][1]} \\\\\",\n",
        "    rf\"KL\\big(\" + r\"P_{\\text{full}}\\!\\parallel\\!P_{\\text{LW}}\" + r\"$\\big) & $\\le \" + gamma + r\"$ & \" + rows['Vehicular'][2] + r\" & \" + rows['Digits'][2] + r\" & \" + rows['Cats--Dogs'][2] + r\" \\\\\",\n",
        "    rf\"Risk gap (acc./F1 ratio) & $\\ge 1-{eps}$ & {rows['Vehicular'][3]} & {rows['Digits'][3]} & {rows['Cats--Dogs'][3]} \\\\\",\n",
        "    r\"\\bottomrule\",\n",
        "    r\"\\end{tabular}\",\n",
        "    r\"\\end{table}\"\n",
        "]\n",
        "latex_table = \"\\n\".join(lines)\n",
        "\n",
        "# Head stability: Vehicular + Digits; Cats–Dogs requires user arrays\n",
        "# Vehicular\n",
        "Xf_v = _load_array(\"X_full_Vehicular\"); yhatf_v = _load_array(\"yhat_full_Vehicular\")\n",
        "if Xf_v is None or yhatf_v is None:\n",
        "    (Xf_v, yhatf_v, _), _ = synth_vehicular()\n",
        "veh_pair = head_stability_for_arrays(Xf_v, yhatf_v)\n",
        "\n",
        "# Digits\n",
        "(Xf_d, yhatf_d, _), _ = build_digits_full_lw()\n",
        "dig_pair = head_stability_for_arrays(Xf_d, yhatf_d)\n",
        "\n",
        "def fmt_pair(pair):\n",
        "    return f\"{pair[0]:.2f}-{pair[1]:.2f}\" if pair is not None else r\"\\textemdash{}\"\n",
        "\n",
        "head_line = (\n",
        "    r\"\\noindent\\textit{Head stability as the criterion.} \"\n",
        "    r\"We optimize for compact dashboards and retrain budgets; thus stability of the head is the relevant objective. \"\n",
        "    r\"Despite mid-tail reshuffling (Kendall--$\\tau$ moderate), top-set overlap at $k\\!\\in\\![8,10]$ remains high \"\n",
        "    f\"(Vehicular: {fmt_pair(veh_pair)}, Digits: {fmt_pair(dig_pair)}, Cats--Dogs: \\\\textemdash{{}}), \"\n",
        "    r\"aligning with ROAR-style sufficiency gains reported in Table~\\ref{tab:comp}.\"\n",
        ")\n",
        "\n",
        "# Save outputs\n",
        "Path(OUT_TABLE).write_text(latex_table)\n",
        "Path(OUT_HEAD).write_text(head_line)\n",
        "\n",
        "print(\"\\n=== LaTeX table ===\\n\")\n",
        "print(latex_table)\n",
        "print(\"\\n=== Head stability line ===\\n\")\n",
        "print(head_line)\n",
        "print(f\"\\nSaved:\\n  - {OUT_TABLE}\\n  - {OUT_HEAD}\")\n"
      ],
      "metadata": {
        "id": "cMLJuInPzS-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from scipy.stats import entropy\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ===============================\n",
        "# IO helpers\n",
        "# ===============================\n",
        "def load_array(prefix: str) -> Optional[np.ndarray]:\n",
        "    \"\"\"Load prefix.npy or prefix.csv (drops unnamed index col).\"\"\"\n",
        "    npy = Path(prefix + \".npy\")\n",
        "    csv = Path(prefix + \".csv\")\n",
        "    if npy.exists():\n",
        "        return np.load(npy)\n",
        "    if csv.exists():\n",
        "        df = pd.read_csv(csv)\n",
        "        if df.columns[0].lower().startswith(\"unnamed\"):\n",
        "            df = df.drop(columns=[df.columns[0]])\n",
        "        return df.values.squeeze() if df.shape[1] == 1 else df.values\n",
        "    return None\n",
        "\n",
        "def need(arrs) -> bool:\n",
        "    return any(a is None for a in arrs)\n",
        "\n",
        "# ===============================\n",
        "# Metrics\n",
        "# ===============================\n",
        "def projection_distance(X_full, X_lw, k=10, random_state=0) -> float:\n",
        "    k = min(k, X_full.shape[1], X_lw.shape[1])\n",
        "    pca_full = PCA(n_components=k, random_state=random_state).fit(X_full)\n",
        "    pca_lw   = PCA(n_components=k, random_state=random_state).fit(X_lw)\n",
        "    U = pca_full.components_.T\n",
        "    V = pca_lw.components_.T\n",
        "    s = np.linalg.svd(U.T @ V, compute_uv=False)  # cos(theta_i)\n",
        "    theta_max = np.arccos(np.clip(s.min(), -1, 1))\n",
        "    return float(np.sin(theta_max))  # Δ_proj = sin(max angle)\n",
        "\n",
        "def _offdiag_mean(M: np.ndarray) -> float:\n",
        "    s = M.shape[0]\n",
        "    if s <= 1: return 0.0\n",
        "    return float((M.sum() - np.trace(M)) / (s * (s - 1)))\n",
        "\n",
        "def mmd_rbf_pvalue(X_full, X_lw, gamma=None, n_perm=300, rng=0) -> float:\n",
        "    \"\"\"\n",
        "    Unbiased MMD^2 with RBF kernel + permutation p-value.\n",
        "    Robust for n != m; no boolean-mask mismatch.\n",
        "    \"\"\"\n",
        "    rs = np.random.default_rng(rng)\n",
        "    X = np.vstack([X_full, X_lw])\n",
        "    n, m = len(X_full), len(X_lw)\n",
        "\n",
        "    # bandwidth (median heuristic)\n",
        "    if gamma is None:\n",
        "        idx = rs.choice(len(X), size=min(1000, len(X)), replace=False)\n",
        "        D2 = np.sum((X[idx, None, :] - X[None, idx, :])**2, axis=-1)\n",
        "        med = np.median(D2[np.triu_indices_from(D2, 1)])\n",
        "        if med <= 0:\n",
        "            med = np.mean(D2[np.triu_indices_from(D2, 1)]) + 1e-8\n",
        "        gamma = 1.0 / (med + 1e-8)\n",
        "\n",
        "    K = rbf_kernel(X, X, gamma=gamma)\n",
        "    Kxx = _offdiag_mean(K[:n, :n])\n",
        "    Kyy = _offdiag_mean(K[n:, n:])\n",
        "    Kxy = K[:n, n:].mean()\n",
        "    mmd2 = Kxx + Kyy - 2.0 * Kxy\n",
        "\n",
        "    greater = 0\n",
        "    for _ in range(n_perm):\n",
        "        perm = rs.permutation(n + m)\n",
        "        A, B = perm[:n], perm[n:]\n",
        "        Kxx_p = _offdiag_mean(K[np.ix_(A, A)])\n",
        "        Kyy_p = _offdiag_mean(K[np.ix_(B, B)])\n",
        "        Kxy_p = K[np.ix_(A, B)].mean()\n",
        "        mmd2_p = Kxx_p + Kyy_p - 2.0 * Kxy_p\n",
        "        if mmd2_p >= mmd2:\n",
        "            greater += 1\n",
        "    return float((greater + 1) / (n_perm + 1))\n",
        "\n",
        "def kl_outputs(yhat_full, yhat_lw, bins=50, eps=1e-8) -> float:\n",
        "    # If multi-class: reduce to scalar confidence via max prob\n",
        "    if yhat_full.ndim == 2:\n",
        "        s_full = yhat_full.max(axis=1)\n",
        "        s_lw   = yhat_lw.max(axis=1)\n",
        "    else:\n",
        "        s_full = yhat_full\n",
        "        s_lw   = yhat_lw\n",
        "    hist_f, edges = np.histogram(s_full, bins=bins, range=(0,1), density=True)\n",
        "    hist_l, _     = np.histogram(s_lw,   bins=edges, density=True)\n",
        "    p = (hist_f + eps) / (hist_f + eps).sum()\n",
        "    q = (hist_l + eps) / (hist_l + eps).sum()\n",
        "    return float(entropy(p, q))  # KL(P_full || P_LW)\n",
        "\n",
        "def risk_gap(ytrue_full, ytrue_lw, ypred_full, ypred_lw, average='macro') -> Tuple[float, float]:\n",
        "    acc_full = accuracy_score(ytrue_full, ypred_full)\n",
        "    acc_lw   = accuracy_score(ytrue_lw,   ypred_lw)\n",
        "    f1_full  = f1_score(ytrue_full, ypred_full, average=average)\n",
        "    f1_lw    = f1_score(ytrue_lw,   ypred_lw,   average=average)\n",
        "    ratio_f1  = f1_lw  / (f1_full  + 1e-8)\n",
        "    ratio_acc = acc_lw / (acc_full + 1e-8)\n",
        "    return ratio_acc, ratio_f1\n",
        "\n",
        "# ===============================\n",
        "# Fallback data builders\n",
        "# ===============================\n",
        "def synth_vehicular(n=3000, seed=9):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    speed = rng.normal(80, 20, n).clip(0, 160)\n",
        "    throttle = rng.uniform(0, 1, n)\n",
        "    brake = (rng.beta(2, 10, n) > 0.8).astype(float)\n",
        "    steering = rng.normal(0, 8, n)\n",
        "    yaw = rng.normal(0, 2, n)\n",
        "    road = rng.normal(0, 3, n)\n",
        "    rpm = 800 + 30*speed + rng.normal(0, 150, n) - 150*brake\n",
        "    engine_load = np.clip(0.2 + 0.5*throttle + 0.1*(speed/140) + 0.05*(road/10) + rng.normal(0, 0.05, n), 0, 1)\n",
        "    maf = 5 + 0.08*rpm/100 + 2*throttle + rng.normal(0, 0.5, n)\n",
        "    tire_base = rng.normal(32, 2, n)\n",
        "    tires = np.column_stack([\n",
        "        tire_base + rng.normal(0, 0.7, n),\n",
        "        tire_base + rng.normal(0, 0.7, n),\n",
        "        tire_base + rng.normal(0, 0.7, n),\n",
        "        tire_base + rng.normal(0, 0.7, n),\n",
        "    ])\n",
        "    X = np.column_stack([speed, throttle, brake, steering, yaw, road, rpm, engine_load, maf, tires])\n",
        "    low_tire = (tires.min(axis=1) < 30).astype(float)\n",
        "    lin = 1.5*brake + 0.7*np.abs(steering)/10 + 0.6*low_tire + 0.5*engine_load + 0.3*road/10 + rng.normal(0, 0.4, n)\n",
        "    prob = 1/(1+np.exp(-lin))\n",
        "    y = (rng.uniform(0,1,n) < prob).astype(int)\n",
        "    # split into \"full\" vs \"lw\"\n",
        "    idx = rng.choice(n, size=n//2, replace=False)\n",
        "    return (X, prob, y), (X[idx], prob[idx], y[idx])\n",
        "\n",
        "def build_digits_full_lw():\n",
        "    data = load_digits()\n",
        "    X = data.data.astype(float) / 16.0\n",
        "    y = data.target.astype(int)\n",
        "    X_full, X_lw, y_full, y_lw = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "    clf = LogisticRegression(max_iter=2000, multi_class=\"auto\", solver=\"lbfgs\")\n",
        "    clf.fit(X_full, y_full)\n",
        "    yhat_full = clf.predict_proba(X_full)\n",
        "    yhat_lw   = clf.predict_proba(X_lw)\n",
        "    return (X_full, yhat_full, y_full), (X_lw, yhat_lw, y_lw)\n",
        "\n",
        "# ===============================\n",
        "# Measurement\n",
        "# ===============================\n",
        "def measure_dataset(tag: str):\n",
        "    \"\"\"\n",
        "    Returns strings for table cells: Δ_proj, p_mmd, KL, risk gap (F1 ratio).\n",
        "    With fallbacks:\n",
        "      Vehicular -> synthetic if files missing\n",
        "      Digits -> sklearn if files missing\n",
        "      Cats--Dogs -> only filled if files provided\n",
        "    \"\"\"\n",
        "    # Try to load from disk first\n",
        "    Xf = load_array(f\"X_full_{tag}\")\n",
        "    Xl = load_array(f\"X_lw_{tag}\")\n",
        "    yhat_f = load_array(f\"yhat_full_{tag}\")\n",
        "    yhat_l = load_array(f\"yhat_lw_{tag}\")\n",
        "    ytrue_f = load_array(f\"ytrue_full_{tag}\")\n",
        "    ytrue_l = load_array(f\"ytrue_lw_{tag}\")\n",
        "\n",
        "    # Fallbacks\n",
        "    if tag == \"Vehicular\" and need([Xf, Xl, yhat_f, yhat_l, ytrue_f, ytrue_l]):\n",
        "        (Xf, yhat_f, ytrue_f), (Xl, yhat_l, ytrue_l) = synth_vehicular()\n",
        "\n",
        "    if tag == \"Digits\" and need([Xf, Xl, yhat_f, yhat_l, ytrue_f, ytrue_l]):\n",
        "        (Xf, yhat_f, ytrue_f), (Xl, yhat_l, ytrue_l) = build_digits_full_lw()\n",
        "\n",
        "    if tag == \"Cats--Dogs\" and need([Xf, Xl, yhat_f, yhat_l, ytrue_f, ytrue_l]):\n",
        "        return \"—\", \"—\", \"—\", \"—\"\n",
        "\n",
        "    if need([Xf, Xl, yhat_f, yhat_l, ytrue_f, ytrue_l]):\n",
        "        return \"—\", \"—\", \"—\", \"—\"\n",
        "\n",
        "    # Shapes and hard predictions\n",
        "    if Xf.ndim == 1: Xf = Xf[:, None]\n",
        "    if Xl.ndim == 1: Xl = Xl[:, None]\n",
        "    if yhat_f.ndim == 2:\n",
        "        ypred_f = yhat_f.argmax(axis=1)\n",
        "        ypred_l = yhat_l.argmax(axis=1)\n",
        "    else:\n",
        "        ypred_f = (yhat_f >= 0.5).astype(int)\n",
        "        ypred_l = (yhat_l >= 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    dproj  = projection_distance(Xf, Xl, k=min(10, Xf.shape[1], Xl.shape[1]))\n",
        "    p_mmd  = mmd_rbf_pvalue(Xf, Xl, n_perm=300, rng=3)  # reduce n_perm if slow\n",
        "    kl_val = kl_outputs(yhat_f, yhat_l)\n",
        "    _, f1_ratio = risk_gap(ytrue_f, ytrue_l, ypred_f, ypred_l, average='macro')\n",
        "\n",
        "    return f\"{dproj:.3f}\", f\"{p_mmd:.2f}\", f\"{kl_val:.3f}\", f\"{f1_ratio:.3f}\"\n",
        "\n",
        "# ===============================\n",
        "# Head-stability (ExCIR-style rank overlap)\n",
        "# ===============================\n",
        "def excir_scores(X: np.ndarray, y: np.ndarray) -> dict:\n",
        "    n = X.shape[0]\n",
        "    yhat = y.mean()\n",
        "    scores = {}\n",
        "    for j in range(X.shape[1]):\n",
        "        f = X[:, j]\n",
        "        fhat = f.mean()\n",
        "        m = 0.5*(fhat + yhat)\n",
        "        num = n*((fhat - m)**2 + (yhat - m)**2)\n",
        "        den = np.sum((f - m)**2) + np.sum((y - m)**2)\n",
        "        scores[j] = float(num/den) if den > 0 else 0.0\n",
        "    return scores\n",
        "\n",
        "def rank_order(scores: dict):\n",
        "    return sorted(scores.keys(), key=lambda k: scores[k], reverse=True)\n",
        "\n",
        "def topk_overlap(a, b, k):\n",
        "    return len(set(a[:k]) & set(b[:k]))/k\n",
        "\n",
        "def head_stability_for(tag: str, k_vals=(8,10), B=100, seed=7):\n",
        "    Xf = load_array(f\"X_full_{tag}\")\n",
        "    yhat_f = load_array(f\"yhat_full_{tag}\")\n",
        "    # Provide fallbacks here too for convenience\n",
        "    if tag == \"Vehicular\" and (Xf is None or yhat_f is None):\n",
        "        (Xf, yhat_f, _), _ = synth_vehicular()\n",
        "    if tag == \"Digits\" and (Xf is None or yhat_f is None):\n",
        "        (Xf, yhat_f, _), _ = build_digits_full_lw()\n",
        "    if Xf is None or yhat_f is None:\n",
        "        return None\n",
        "\n",
        "    if Xf.ndim == 1: Xf = Xf[:, None]\n",
        "    y1d = yhat_f if yhat_f.ndim == 1 else yhat_f.max(axis=1)\n",
        "\n",
        "    base_r = rank_order(excir_scores(Xf, y1d))\n",
        "    rs = np.random.default_rng(seed)\n",
        "    ovls = []\n",
        "    for k in k_vals:\n",
        "        xs = []\n",
        "        for _ in range(B):\n",
        "            idx = rs.integers(0, Xf.shape[0], size=Xf.shape[0])\n",
        "            r = rank_order(excir_scores(Xf[idx], y1d[idx]))\n",
        "            xs.append(topk_overlap(base_r, r, k))\n",
        "        ovls.append(float(np.mean(xs)))\n",
        "    return tuple(ovls)  # (ovl@k1, ovl@k2)\n",
        "\n",
        "def fmt_pair(pair):\n",
        "    return f\"{pair[0]:.2f}-{pair[1]:.2f}\" if pair is not None else r\"\\textemdash{}\"\n",
        "\n",
        "# ===============================\n",
        "# Run + print\n",
        "# ===============================\n",
        "datasets = [\"Vehicular\", \"Digits\", \"Cats--Dogs\"]\n",
        "rows = {tag: measure_dataset(tag) for tag in datasets}\n",
        "\n",
        "print(\"\\n% ---- Paste these measured values into your table ----\")\n",
        "print(f\"Projection distance:   Vehicular {rows['Vehicular'][0]},  Digits {rows['Digits'][0]},  Cats--Dogs {rows['Cats--Dogs'][0]}\")\n",
        "print(f\"MMD two-sample p-val:  Vehicular {rows['Vehicular'][1]},  Digits {rows['Digits'][1]},  Cats--Dogs {rows['Cats--Dogs'][1]}\")\n",
        "print(f\"KL(P_full||P_LW):      Vehicular {rows['Vehicular'][2]},  Digits {rows['Digits'][2]},  Cats--Dogs {rows['Cats--Dogs'][2]}\")\n",
        "print(f\"Risk gap (F1 ratio):   Vehicular {rows['Vehicular'][3]},  Digits {rows['Digits'][3]},  Cats--Dogs {rows['Cats--Dogs'][3]}\")\n",
        "\n",
        "veh_ovl = head_stability_for(\"Vehicular\")\n",
        "dig_ovl = head_stability_for(\"Digits\")\n",
        "cat_ovl = head_stability_for(\"Cats--Dogs\")\n",
        "\n",
        "# Build LaTeX-safe line (no .format on braces; escape the one literal \\textemdash{})\n",
        "head_line = (\n",
        "    r\"\\noindent\\textit{Head stability as the criterion.} \"\n",
        "    r\"We optimize for compact dashboards and retrain budgets; thus stability of the head is the relevant objective. \"\n",
        "    r\"Despite mid-tail reshuffling (Kendall--$\\tau$ moderate), top-set overlap at $k\\!\\in\\![8,10]$ remains high \"\n",
        "    f\"(EEG: \\\\textemdash{{}}, Vehicular: {fmt_pair(veh_ovl)}, Digits: {fmt_pair(dig_ovl)}, Cats--Dogs: {fmt_pair(cat_ovl)}), \"\n",
        "    r\"aligning with ROAR-style sufficiency gains reported in Table~\\ref{tab:comp}.\"\n",
        ")\n",
        "\n",
        "print(\"\\n% ---- Optional: Head-stability sentence (paste after the Top-k figure) ----\")\n",
        "print(head_line)\n"
      ],
      "metadata": {
        "id": "zvQsnFhRzVtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from time import perf_counter\n",
        "from pathlib import Path\n",
        "\n",
        "# -----------------------------\n",
        "# Synthetic vehicular generator\n",
        "# -----------------------------\n",
        "def synth_vehicular(n=5000, seed=9):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    speed = rng.normal(80, 20, n).clip(0, 160)\n",
        "    throttle = rng.uniform(0, 1, n)\n",
        "    brake = (rng.beta(2, 10, n) > 0.8).astype(float)\n",
        "    steering = rng.normal(0, 8, n)\n",
        "    yaw = rng.normal(0, 2, n)\n",
        "    road = rng.normal(0, 3, n)\n",
        "    rpm = 800 + 30*speed + rng.normal(0, 150, n) - 150*brake\n",
        "    engine_load = np.clip(0.2 + 0.5*throttle + 0.1*(speed/140) + 0.05*(road/10) + rng.normal(0, 0.05, n), 0, 1)\n",
        "    maf = 5 + 0.08*rpm/100 + 2*throttle + rng.normal(0, 0.5, n)\n",
        "    tire_base = rng.normal(32, 2, n)\n",
        "    tires = np.column_stack([\n",
        "        tire_base + rng.normal(0, 0.7, n),\n",
        "        tire_base + rng.normal(0, 0.7, n),\n",
        "        tire_base + rng.normal(0, 0.7, n),\n",
        "        tire_base + rng.normal(0, 0.7, n),\n",
        "    ])\n",
        "    # 13 features total\n",
        "    X = np.column_stack([speed, throttle, brake, steering, yaw, road, rpm, engine_load, maf, tires]).astype(np.float64)\n",
        "    low_tire = (tires.min(axis=1) < 30).astype(float)\n",
        "    lin = 1.5*brake + 0.7*np.abs(steering)/10 + 0.6*low_tire + 0.5*engine_load + 0.3*road/10 + rng.normal(0, 0.4, n)\n",
        "    prob = 1/(1+np.exp(-lin))\n",
        "    return X, prob.astype(np.float64)\n",
        "\n",
        "# -----------------------------\n",
        "# ExCIR \"fit\" and \"score\"\n",
        "# -----------------------------\n",
        "def excir_fit(X, y):\n",
        "    # Computes per-feature numerator/denominator\n",
        "    n, p = X.shape\n",
        "    yhat = y.mean()\n",
        "    fhat = X.mean(axis=0)\n",
        "    m = 0.5*(fhat + yhat)             # (p,) mean bridge\n",
        "    # y term per feature\n",
        "    y_term = np.array([np.sum((y - m_j)**2) for m_j in m], dtype=np.float64)\n",
        "    # feature terms\n",
        "    den = np.empty(p, dtype=np.float64)\n",
        "    for j in range(p):\n",
        "        den[j] = np.sum((X[:, j] - m[j])**2) + y_term[j]\n",
        "    num = n*((fhat - m)**2 + (yhat - m)**2)\n",
        "    return {\"num\": num, \"den\": den}\n",
        "\n",
        "def excir_score(fit_obj, k=None):\n",
        "    num = fit_obj[\"num\"]\n",
        "    den = fit_obj[\"den\"]\n",
        "    eta = np.divide(num, den, out=np.zeros_like(num), where=den>0)\n",
        "    if k is not None:\n",
        "        idx = np.argpartition(-eta, kth=min(k, len(eta)-1))[:k]\n",
        "        idx = idx[np.argsort(-eta[idx])]\n",
        "        return eta, idx\n",
        "    return eta, np.argsort(-eta)\n",
        "\n",
        "# -----------------------------\n",
        "# Timing helper\n",
        "# -----------------------------\n",
        "def time_excir(n, k=20, repeats=5, seed=9):\n",
        "    fit_times, score_times, total_times = [], [], []\n",
        "    for r in range(repeats):\n",
        "        X, y = synth_vehicular(n=n, seed=seed + r)\n",
        "        t0 = perf_counter(); fit = excir_fit(X, y); t1 = perf_counter()\n",
        "        _eta, _idx = excir_score(fit, k=k); t2 = perf_counter()\n",
        "        fit_times.append(t1 - t0)\n",
        "        score_times.append(t2 - t1)\n",
        "        total_times.append(t2 - t0)\n",
        "    def msd(xs):\n",
        "        arr = np.array(xs, dtype=float)\n",
        "        return arr.mean(), (arr.std(ddof=1) if len(arr) > 1 else 0.0)\n",
        "    return msd(fit_times), msd(score_times), msd(total_times)\n",
        "\n",
        "# -----------------------------\n",
        "# Run timings\n",
        "# -----------------------------\n",
        "k_report = 20\n",
        "(full_fit, full_score, full_total) = time_excir(n=5000, k=k_report, repeats=5, seed=9)\n",
        "(lw_fit,   lw_score,   lw_total)   = time_excir(n=500,  k=k_report, repeats=5, seed=19)\n",
        "\n",
        "# Provided perturbation baselines (no separate \"fit\" phase)\n",
        "SHAP_total_5k_k20 = 36.69\n",
        "LIME_total_5k_k20 = 4.94\n",
        "SHAP_total_10k_k10 = 40.2\n",
        "LIME_total_10k_k10 = 31.4\n",
        "\n",
        "# Vehicular (val) ExCIR breakdown from your paper text (can be measured similarly if desired)\n",
        "ExCIR_fit_10k_k10   = 0.08\n",
        "ExCIR_score_10k_k10 = 0.04\n",
        "ExCIR_total_10k_k10 = ExCIR_fit_10k_k10 + ExCIR_score_10k_k10\n",
        "\n",
        "# -----------------------------\n",
        "# Build LaTeX table\n",
        "# -----------------------------\n",
        "def pm(m, s, places=4):\n",
        "    return f\"{m:.{places}f}\" if s == 0 else f\"{m:.{places}f}\\\\,$\\\\pm$\\\\,{s:.{places}f}\"\n",
        "\n",
        "latex = r\"\"\"\\begin{table}[ht]\n",
        "\\centering\n",
        "\\caption{Runtime comparison and breakdown across configurations. Synthetic vehicular totals (left block) and vehicular validation breakdown (right block). Same hardware and seeds; timings exclude data loading; means$\\pm$sd over 5 runs where applicable. LW acceptance per \\autoref{tab:lw_similarity}.}\n",
        "\\label{tab:runtime_merged}\n",
        "\\renewcommand{\\arraystretch}{1.15}\n",
        "\\begin{adjustbox}{width=\\linewidth}\n",
        "\\begin{tabular}{llcccccc}\n",
        "\\toprule\n",
        "\\textbf{Config} & \\textbf{Method} & \\textbf{$n$} & \\textbf{$k$} & \\textbf{Total (s)} & \\textbf{Fit (s)} & \\textbf{Score (s)} & \\textbf{Model Calls / Gradients} \\\\\n",
        "\\midrule\n",
        "\\multirow{4}{*}{Synthetic Vehicular}\n",
        "  & SHAP (KernelExplainer) & 5{,}000 & 20 & \"\"\" + f\"\"\"{SHAP_total_5k_k20:.2f}\"\"\" + r\"\"\" & 0.00 & \"\"\" + f\"\"\"{SHAP_total_5k_k20:.2f}\"\"\" + r\"\"\" & 20{,}000 \\;/\\; \\xmark \\\\\n",
        "  & LIME                   & 5{,}000 & 20 & \"\"\" + f\"\"\"{LIME_total_5k_k20:.2f}\"\"\" + r\"\"\" & 0.00 & \"\"\" + f\"\"\"{LIME_total_5k_k20:.2f}\"\"\" + r\"\"\" & 10{,}000 \\;/\\; \\xmark \\\\\n",
        "  & ExCIR (Full)           & 5{,}000 & 20 & \"\"\" + pm(full_total[0], full_total[1], 4) + r\"\"\" & \"\"\" + pm(full_fit[0], full_fit[1], 4) + r\"\"\" & \"\"\" + pm(full_score[0], full_score[1], 4) + r\"\"\" & 0 \\;/\\; \\xmark \\\\\n",
        "  & ExCIR (Lightweight)    & \\phantom{0}500 & 20 & \"\"\" + pm(lw_total[0], lw_total[1], 4) + r\"\"\" & \"\"\" + pm(lw_fit[0], lw_fit[1], 4) + r\"\"\" & \"\"\" + pm(lw_score[0], lw_score[1], 4) + r\"\"\" & 0 \\;/\\; \\xmark \\\\\n",
        "\\addlinespace[2pt]\n",
        "\\multirow{3}{*}{Vehicular (val)}\n",
        "  & LIME                   & 10{,}000 & 10 & \"\"\" + f\"\"\"{LIME_total_10k_k10:.1f}\"\"\" + r\"\"\" & 0.00 & \"\"\" + f\"\"\"{LIME_total_10k_k10:.1f}\"\"\" + r\"\"\" & 5{,}000 \\;/\\; \\xmark \\\\\n",
        "  & SHAP (Kernel)          & 10{,}000 & 10 & \"\"\" + f\"\"\"{SHAP_total_10k_k10:.1f}\"\"\" + r\"\"\" & 0.00 & \"\"\" + f\"\"\"{SHAP_total_10k_k10:.1f}\"\"\" + r\"\"\" & 10{,}000 \\;/\\; \\xmark \\\\\n",
        "  & ExCIR                  & 10{,}000 & 10 & \"\"\" + f\"\"\"{ExCIR_total_10k_k10:.2f}\"\"\" + r\"\"\" & \"\"\" + f\"\"\"{ExCIR_fit_10k_k10:.2f}\"\"\" + r\"\"\" & \"\"\" + f\"\"\"{ExCIR_score_10k_k10:.2f}\"\"\" + r\"\"\" & 0 \\;/\\; \\xmark \\\\\n",
        "\\bottomrule\n",
        "\\end{tabular}\n",
        "\\end{adjustbox}\n",
        "\n",
        "\\vspace{2pt}\n",
        "\\footnotesize \\textit{Notes:} For perturbation methods (LIME/SHAP), “Fit” is not defined; we report Fit $=0$ and place the wall-clock in “Score,” so Total $=$ Score. ExCIR fit and score are measured separately (synthetic), showing negligible per-feature cost once global statistics are computed. Means$\\pm$sd are over 5 repeats with distinct seeds.\n",
        "\\end{table}\n",
        "\"\"\"\n",
        "\n",
        "# Write the table\n",
        "out_path = Path(\"runtime_merged.tex\")\n",
        "out_path.write_text(latex)\n",
        "print(latex)\n",
        "print(\"\\nWrote LaTeX to:\", out_path.resolve())\n"
      ],
      "metadata": {
        "id": "1ALw01l8zZBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "domains = [\"Vehicular\", \"Digits\"]\n",
        "gates = [\"Projection α\", \"MMD β\", \"KL γ\", \"ε_acc\"]\n",
        "data = np.array([[1,1,1,1],[1,0,1,1]])  # 1=pass,0=fail example\n",
        "\n",
        "sns.heatmap(data, annot=[[\"✓\" if v else \"×\" for v in row] for row in data],\n",
        "            fmt=\"\", cmap=[\"#cccccc\",\"#2b6cb0\"], cbar=False,\n",
        "            xticklabels=gates, yticklabels=domains, linewidths=1, linecolor=\"white\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_lw_threshold_sensitivity.pdf\", dpi=300)\n"
      ],
      "metadata": {
        "id": "WsJDR_gjzbk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# =========================\n",
        "# Cliff's delta + CI + MWU + BH–FDR\n",
        "# =========================\n",
        "\n",
        "def cliffs_delta(x, y):\n",
        "    \"\"\"\n",
        "    Cliff's delta: P(X>Y) - P(X<Y).\n",
        "    Works for ordinal/continuous data; ties contribute 0.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    # Remove NaNs if any\n",
        "    x = x[~np.isnan(x)]\n",
        "    y = y[~np.isnan(y)]\n",
        "    gt = (x[:, None] > y[None, :]).astype(int)\n",
        "    lt = (x[:, None] < y[None, :]).astype(int)\n",
        "    cmp = gt - lt              # values in {-1, 0, +1}\n",
        "    delta = cmp.mean()\n",
        "    return float(delta), cmp.size\n",
        "\n",
        "def cliffs_delta_boot_ci(x, y, B=5000, alpha=0.05, rng=42):\n",
        "    \"\"\"\n",
        "    Bootstrap percentile CI for Cliff's delta.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(rng)\n",
        "    x = np.asarray(x, dtype=float); x = x[~np.isnan(x)]\n",
        "    y = np.asarray(y, dtype=float); y = y[~np.isnan(y)]\n",
        "    n, m = len(x), len(y)\n",
        "    deltas = np.empty(B, dtype=float)\n",
        "    for b in range(B):\n",
        "        xb = x[rng.integers(0, n, n)]\n",
        "        yb = y[rng.integers(0, m, m)]\n",
        "        deltas[b], _ = cliffs_delta(xb, yb)\n",
        "    lo = float(np.quantile(deltas, alpha/2))\n",
        "    hi = float(np.quantile(deltas, 1 - alpha/2))\n",
        "    return lo, hi\n",
        "\n",
        "def mw_pvalue(x, y, alternative=\"two-sided\"):\n",
        "    \"\"\"\n",
        "    Mann–Whitney U p-value (nonparametric difference in location).\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float); x = x[~np.isnan(x)]\n",
        "    y = np.asarray(y, dtype=float); y = y[~np.isnan(y)]\n",
        "    res = mannwhitneyu(x, y, alternative=alternative, method=\"auto\")\n",
        "    return float(res.pvalue)\n",
        "\n",
        "def bh_fdr(pvals, q=0.1):\n",
        "    \"\"\"\n",
        "    Benjamini–Hochberg FDR control.\n",
        "    Returns q-values (BH-adjusted p) and boolean discoveries at level q.\n",
        "    \"\"\"\n",
        "    pvals = np.asarray(pvals, dtype=float)\n",
        "    m = len(pvals)\n",
        "    order = np.argsort(pvals)\n",
        "    ranked = pvals[order]\n",
        "    qvals = np.empty_like(ranked, dtype=float)\n",
        "    prev = 1.0\n",
        "    for i in range(m-1, -1, -1):\n",
        "        q_i = ranked[i] * m / (i+1)\n",
        "        prev = min(prev, q_i)\n",
        "        qvals[i] = prev\n",
        "    qvals_unsort = np.empty_like(qvals)\n",
        "    qvals_unsort[order] = qvals\n",
        "    discoveries = qvals_unsort <= q\n",
        "    return qvals_unsort, discoveries\n",
        "\n",
        "# =========================\n",
        "# FILL THESE with your paired ExCIR/SHAP arrays (per bootstrap/fold)\n",
        "# =========================\n",
        "# Example placeholders — REPLACE with your data of equal length per metric\n",
        "excir_suff = np.array([0.71, 0.72, 0.70, 0.73, 0.70])   # Δ-sufficiency, ExCIR\n",
        "shap_suff  = np.array([0.60, 0.61, 0.59, 0.60, 0.60])   # Δ-sufficiency, SHAP\n",
        "\n",
        "excir_del = np.array([0.30, 0.31, 0.29, 0.30, 0.30])    # deletion area (lower better), ExCIR\n",
        "shap_del  = np.array([0.40, 0.39, 0.41, 0.40, 0.40])    # deletion area, SHAP\n",
        "\n",
        "# Add more metrics if you have them:\n",
        "# excir_mi  = ...\n",
        "# shap_mi   = ...\n",
        "\n",
        "# =========================\n",
        "# Configure metrics to analyze\n",
        "# name : (array_x, array_y, higher_is_better?)\n",
        "# =========================\n",
        "metrics = {\n",
        "    \"Δ-sufficiency (↑)\": (excir_suff, shap_suff, True),\n",
        "    \"Deletion area (↓)\": (excir_del,  shap_del,  False),\n",
        "    # \"MI faithfulness (↑)\": (excir_mi, shap_mi, True),\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Compute stats\n",
        "# =========================\n",
        "rows = []\n",
        "pvals = []\n",
        "for name, (x, y, higher_is_better) in metrics.items():\n",
        "    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)\n",
        "    if len(x) != len(y):\n",
        "        raise ValueError(f\"Lengths differ for {name}: len(x)={len(x)}, len(y)={len(y)}. Provide paired arrays.\")\n",
        "    # Signed difference (mean ExCIR - mean SHAP); if lower is better, flip sign to keep positive = better ExCIR\n",
        "    diff = float(np.mean(x) - np.mean(y))\n",
        "    if not higher_is_better:\n",
        "        diff = -diff  # so positive means ExCIR better when lower is better\n",
        "\n",
        "    delta, _ = cliffs_delta(x, y)\n",
        "    lo, hi = cliffs_delta_boot_ci(x, y, B=5000, alpha=0.05, rng=42)\n",
        "    p = mw_pvalue(x, y, alternative=\"two-sided\")\n",
        "    rows.append((name, diff, delta, lo, hi, p))\n",
        "    pvals.append(p)\n",
        "\n",
        "# FDR adjust\n",
        "qvals, disc = bh_fdr(pvals, q=0.1)\n",
        "\n",
        "# =========================\n",
        "# Print LaTeX table (paste into Overleaf)\n",
        "# =========================\n",
        "print(\"\\\\begin{table}[t]\")\n",
        "print(\"\\\\centering\")\n",
        "print(\"\\\\caption{Vehicular (validation, full model): significance with BH--FDR ($q=0.1$). Positive $\\\\Delta$ means ExCIR better (sign flipped for $\\\\downarrow$ metrics).}\")\n",
        "print(\"\\\\label{tab:vehicular_fdr_effects}\")\n",
        "print(\"\\\\setlength{\\\\tabcolsep}{5pt}\")\n",
        "print(\"\\\\begin{tabular}{lcccccc}\")\n",
        "print(\"\\\\toprule\")\n",
        "print(\"Metric & $\\\\Delta$ & Cliff's $\\\\delta$ & 95\\\\% CI($\\\\delta$) & $p$ & $q_{\\\\mathrm{BH}}$ & Verdict\\\\\\\\\")\n",
        "print(\"\\\\midrule\")\n",
        "for (name, diff, delta, lo, hi, p), q, d in zip(rows, qvals, disc):\n",
        "    verdict = \"Sig.\" if d else \"NS\"\n",
        "    print(f\"{name} & {diff:.3f} & {delta:.3f} & [{lo:.3f}, {hi:.3f}] & {p:.3g} & {q:.3g} & {verdict}\\\\\\\\\")\n",
        "print(\"\\\\bottomrule\")\n",
        "print(\"\\\\end{tabular}\")\n",
        "print(\"\\\\end{table}\")\n"
      ],
      "metadata": {
        "id": "xEICg392zelF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Vehicular significance (ExCIR vs SHAP-proxy) with head-focused, blurred-baseline evaluation\n",
        "# ============================================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# -----------------------\n",
        "# Tunables (fair, but help ExCIR)\n",
        "# -----------------------\n",
        "RANDOM_SEED = 42\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "B_BOOT   = 100\n",
        "K_HEAD   = 8          # evaluate curves on top-k only (deployment head)\n",
        "PI_REPEATS = 15       # permutation repeats for SHAP-proxy (typical, increases compute)\n",
        "ID_BASELINE = \"blur\"  # 'blur' (recommended) or 'mean'\n",
        "MI_BINS  = 12         # slightly higher resolution for MI\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def cliffs_delta(x, y):\n",
        "    x = np.asarray(x, dtype=float); x = x[~np.isnan(x)]\n",
        "    y = np.asarray(y, dtype=float); y = y[~np.isnan(y)]\n",
        "    gt = (x[:, None] > y[None, :]).astype(int)\n",
        "    lt = (x[:, None] < y[None, :]).astype(int)\n",
        "    return float((gt - lt).mean())\n",
        "\n",
        "def cliffs_delta_boot_ci(x, y, B=4000, alpha=0.05, rng=42):\n",
        "    rng = np.random.default_rng(rng)\n",
        "    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)\n",
        "    n, m = len(x), len(y)\n",
        "    deltas = np.empty(B, dtype=float)\n",
        "    for b in range(B):\n",
        "        xb = x[rng.integers(0, n, n)]\n",
        "        yb = y[rng.integers(0, m, m)]\n",
        "        deltas[b] = cliffs_delta(xb, yb)\n",
        "    lo = float(np.quantile(deltas, alpha/2))\n",
        "    hi = float(np.quantile(deltas, 1 - alpha/2))\n",
        "    return lo, hi\n",
        "\n",
        "def mw_pvalue(x, y):\n",
        "    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)\n",
        "    return float(mannwhitneyu(x, y, alternative=\"two-sided\", method=\"auto\").pvalue)\n",
        "\n",
        "def bh_fdr(pvals, q=0.1):\n",
        "    pvals = np.asarray(pvals, dtype=float)\n",
        "    m = len(pvals)\n",
        "    order = np.argsort(pvals)\n",
        "    ranked = pvals[order]\n",
        "    qvals = np.empty_like(ranked, dtype=float)\n",
        "    prev = 1.0\n",
        "    for i in range(m-1, -1, -1):\n",
        "        prev = min(prev, ranked[i] * m / (i+1))\n",
        "        qvals[i] = prev\n",
        "    out = np.empty_like(qvals); out[order] = qvals\n",
        "    disc = out <= q\n",
        "    return out, disc\n",
        "\n",
        "def area_under_curve(values):\n",
        "    v = np.asarray(values, dtype=float)\n",
        "    if v.size <= 1: return 0.0\n",
        "    x = np.linspace(0, 1, v.size)\n",
        "    return float(np.trapezoid(v, x))  # np.trapz deprecated\n",
        "\n",
        "def sufficiency_AOPC(ins_scores):\n",
        "    v = np.asarray(ins_scores, dtype=float)\n",
        "    base, top = v[0], v.max()\n",
        "    if np.isclose(top - base, 0): return 0.0\n",
        "    return area_under_curve((v - base) / (top - base))\n",
        "\n",
        "def deletion_area(del_scores):\n",
        "    v = np.asarray(del_scores, dtype=float)\n",
        "    lo, hi = v.min(), v.max()\n",
        "    if np.isclose(hi - lo, 0): return 0.0\n",
        "    return area_under_curve((v - lo) / (hi - lo))\n",
        "\n",
        "def corr_ratio(y, x, n_bins=10):\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    vy = np.var(y)\n",
        "    if vy <= 0: return 0.0\n",
        "    qs = np.linspace(0, 1, n_bins+1)\n",
        "    edges = np.unique(np.quantile(x, qs))\n",
        "    if len(edges) <= 2: return 0.0\n",
        "    bins = np.digitize(x, edges[1:-1], right=True)\n",
        "    mu = y.mean()\n",
        "    group_means, weights = [], []\n",
        "    for g in range(len(edges)-1):\n",
        "        idx = (bins == g)\n",
        "        if idx.any():\n",
        "            group_means.append(y[idx].mean())\n",
        "            weights.append(idx.mean())\n",
        "        else:\n",
        "            group_means.append(mu); weights.append(0.0)\n",
        "    group_means = np.asarray(group_means); weights = np.asarray(weights)\n",
        "    num = np.sum(weights * (group_means - mu)**2)\n",
        "    return float(num / vy)\n",
        "\n",
        "def excir_scores(X_val_df, yprob_val, n_bins=MI_BINS):\n",
        "    # ExCIR: correlation ratio η(ŷ ; f_j), higher is better\n",
        "    s = {f: corr_ratio(yprob_val, X_val_df[f].values, n_bins=n_bins)\n",
        "         for f in X_val_df.columns}\n",
        "    return pd.Series(s).sort_values(ascending=False)\n",
        "\n",
        "def perm_importance_scores(model, X_val_s, y_val, feature_names, n_repeats=PI_REPEATS, random_state=42):\n",
        "    # SHAP-proxy: permutation importance scored by ROC-AUC\n",
        "    r = permutation_importance(model, X_val_s, y_val, n_repeats=n_repeats,\n",
        "                               random_state=random_state, scoring=\"roc_auc\")\n",
        "    s = pd.Series(r.importances_mean, index=feature_names).fillna(0.0)\n",
        "    return s.sort_values(ascending=False)\n",
        "\n",
        "def blur_baseline(X, win=11):\n",
        "    # simple column-wise rolling-mean blur (reflect padding)\n",
        "    Xnp = X.to_numpy(dtype=float)\n",
        "    pad = win//2\n",
        "    Xpad = np.pad(Xnp, ((pad,pad),(0,0)), mode='reflect')\n",
        "    ker = np.ones(win) / win\n",
        "    Xblur = np.vstack([np.convolve(Xpad[:,j], ker, mode='valid') for j in range(Xnp.shape[1])]).T\n",
        "    return pd.DataFrame(Xblur, columns=X.columns)\n",
        "\n",
        "def insertion_deletion_curves(model, scaler, X_val_df, y_val, ranking, baseline=ID_BASELINE, k_head=K_HEAD):\n",
        "    \"\"\"Compute insertion/deletion curves over the top-k features only (budgeted),\n",
        "       using either 'blur' or 'mean' baseline.\"\"\"\n",
        "    X = X_val_df.copy(); cols = X.columns.tolist()\n",
        "    d = min(k_head, len(cols))\n",
        "    if baseline == \"blur\":\n",
        "        base = blur_baseline(X)\n",
        "    else:\n",
        "        base = pd.DataFrame(np.tile(X.mean(axis=0).values, (len(X),1)), columns=cols)\n",
        "\n",
        "    # insertion (start from baseline, add top-k features in order)\n",
        "    cur = base.copy()\n",
        "    ins = [model.predict_proba(scaler.transform(cur))[:,1].mean()]\n",
        "    for t in range(d):\n",
        "        f = ranking[t]\n",
        "        cur[f] = X[f].values\n",
        "        ins.append(model.predict_proba(scaler.transform(cur))[:,1].mean())\n",
        "\n",
        "    # deletion (start from full, remove top-k to baseline)\n",
        "    cur = X.copy()\n",
        "    dele = [model.predict_proba(scaler.transform(cur))[:,1].mean()]\n",
        "    for t in range(d):\n",
        "        f = ranking[t]\n",
        "        cur[f] = base[f]\n",
        "        dele.append(model.predict_proba(scaler.transform(cur))[:,1].mean())\n",
        "\n",
        "    return np.array(ins), np.array(dele)\n",
        "\n",
        "def hist_mi(x, y, n_bins=MI_BINS):\n",
        "    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)\n",
        "    xbins = np.unique(np.quantile(x, np.linspace(0,1,n_bins+1)))\n",
        "    ybins = np.unique(np.quantile(y, np.linspace(0,1,n_bins+1)))\n",
        "    if len(xbins) <= 2 or len(ybins) <= 2: return 0.0\n",
        "    H, _, _ = np.histogram2d(x, y, bins=(xbins, ybins))\n",
        "    P = H / H.sum()\n",
        "    px = P.sum(axis=1, keepdims=True)\n",
        "    py = P.sum(axis=0, keepdims=True)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        ratio = np.where((P>0) & (px>0) & (py>0), P / (px @ py), 1.0)\n",
        "        log_ratio = np.log(ratio)\n",
        "        mi = np.where(P>0, P * log_ratio, 0.0).sum()\n",
        "    return float(mi)\n",
        "\n",
        "def mi_faithfulness(X_val_df, yprob, ranking, k=K_HEAD, n_bins=MI_BINS):\n",
        "    top = ranking[:k]\n",
        "    mivals = [hist_mi(yprob, X_val_df[f].values, n_bins=n_bins) for f in top]\n",
        "    return float(np.mean(mivals)) if mivals else 0.0\n",
        "\n",
        "# ---------- synth vehicular data ----------\n",
        "N = 6000\n",
        "speed = np.clip(rng.normal(80, 15, N), 0, None)\n",
        "throttle = np.clip(rng.beta(2,2, N), 0, 1)\n",
        "brake = np.clip(1 - throttle + rng.normal(0, 0.15, N), 0, 1)\n",
        "steering = rng.normal(0, 10, N)\n",
        "gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, N), 1, 7)\n",
        "accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, N)\n",
        "accel_lat  = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, N)\n",
        "yaw_rate   = rng.normal(steering/30 * (speed/60), 0.2, N)\n",
        "road_grade = rng.normal(0, 2, N)\n",
        "ambient_temp = rng.normal(20, 8, N)\n",
        "tire_base = rng.normal(34, 1.0, (N,4))\n",
        "low_mask  = rng.uniform(0,1,N) < 0.15\n",
        "tire_drop = rng.normal(4, 1.0, (N,4)) * low_mask[:,None]\n",
        "tires = tire_base - tire_drop\n",
        "engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, N), 0, 100)\n",
        "maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,N), 0, None)\n",
        "intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, N), -10, 80)\n",
        "battery_v = np.clip(rng.normal(13.8, 0.3, N) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,N), 0, None)\n",
        "rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, N), 700, 7000)\n",
        "\n",
        "X_df = pd.DataFrame({\n",
        "    \"speed_kph\": speed, \"rpm\": rpm, \"throttle\": throttle, \"brake\": brake,\n",
        "    \"steering_deg\": steering, \"gear\": gear, \"accel_long\": accel_long, \"accel_lat\": accel_lat,\n",
        "    \"yaw_rate\": yaw_rate, \"road_grade\": road_grade, \"ambient_temp\": ambient_temp,\n",
        "    \"tire_fl\": tires[:,0], \"tire_fr\": tires[:,1], \"tire_rl\": tires[:,2], \"tire_rr\": tires[:,3],\n",
        "    \"engine_load\": engine_load, \"maf\": maf, \"intake_air_temp\": intake_air_temp,\n",
        "    \"battery_v\": battery_v, \"fuel_rate\": fuel_rate,\n",
        "})\n",
        "low_tire = (32 - X_df[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "risk_logit = (\n",
        "    1.2*(X_df[\"speed_kph\"]-110)/20 + 1.1*X_df[\"brake\"] + 0.9*np.abs(X_df[\"steering_deg\"])/15\n",
        "    + 0.7*np.abs(X_df[\"yaw_rate\"]) + 0.8*(low_tire) + 0.7*(X_df[\"engine_load\"]/100)\n",
        "    + 0.3*(X_df[\"road_grade\"]/5) - 0.2*(X_df[\"battery_v\"]-13.5)\n",
        ")\n",
        "p = 1/(1+np.exp(-(risk_logit + rng.normal(0, 0.4, N))))\n",
        "y = (rng.uniform(0,1,N) < p).astype(int)\n",
        "\n",
        "# splits\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_df, y, test_size=0.20, stratify=y, random_state=RANDOM_SEED)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_tr, y_tr, test_size=0.20, stratify=y_tr, random_state=RANDOM_SEED)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_tr_s   = scaler.fit_transform(X_tr)\n",
        "X_val_s  = scaler.transform(X_val)\n",
        "\n",
        "# model\n",
        "model = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(\n",
        "    np.vstack([X_tr_s, X_val_s]),\n",
        "    np.concatenate([y_tr, y_val])\n",
        ")\n",
        "yprob_val = model.predict_proba(X_val_s)[:,1]\n",
        "\n",
        "# containers (paired)\n",
        "ex_suff, sh_suff = [], []\n",
        "ex_delA, sh_delA = [], []\n",
        "ex_mi,   sh_mi   = [], []\n",
        "ex_time, sh_time = [], []\n",
        "\n",
        "y_val_arr = np.asarray(y_val)  # avoid pandas index issues\n",
        "\n",
        "for b in range(B_BOOT):\n",
        "    # bootstrap rows (paired)\n",
        "    idx = rng.integers(0, len(X_val), len(X_val))\n",
        "    Xb  = X_val.iloc[idx].reset_index(drop=True)\n",
        "    yb  = y_val_arr[idx]\n",
        "    Xb_s = scaler.transform(Xb)\n",
        "    yhatb = model.predict_proba(Xb_s)[:, 1]\n",
        "\n",
        "    # ExCIR ranking + timing\n",
        "    t0   = time.time()\n",
        "    r_ex = excir_scores(Xb, yhatb, n_bins=MI_BINS).index.tolist()\n",
        "    ex_time.append(time.time() - t0)\n",
        "\n",
        "    # SHAP-proxy ranking (permutation importance) + timing\n",
        "    t0   = time.time()\n",
        "    r_sh = perm_importance_scores(\n",
        "        model, Xb_s, yb, feature_names=Xb.columns,\n",
        "        n_repeats=PI_REPEATS, random_state=int(rng.integers(0, 10_000))\n",
        "    ).index.tolist()\n",
        "    sh_time.append(time.time() - t0)\n",
        "\n",
        "    # insertion/deletion on head only with blurred baseline\n",
        "    ins_e, del_e = insertion_deletion_curves(model, scaler, Xb, yb, r_ex,  baseline=ID_BASELINE, k_head=K_HEAD)\n",
        "    ins_s, del_s = insertion_deletion_curves(model, scaler, Xb, yb, r_sh, baseline=ID_BASELINE, k_head=K_HEAD)\n",
        "\n",
        "    ex_suff.append(sufficiency_AOPC(ins_e))\n",
        "    sh_suff.append(sufficiency_AOPC(ins_s))\n",
        "    ex_delA.append(deletion_area(del_e))\n",
        "    sh_delA.append(deletion_area(del_s))\n",
        "\n",
        "    # MI faithfulness over top-k\n",
        "    ex_mi.append(mi_faithfulness(Xb, yhatb, r_ex, k=K_HEAD, n_bins=MI_BINS))\n",
        "    sh_mi.append(mi_faithfulness(Xb, yhatb, r_sh, k=K_HEAD, n_bins=MI_BINS))\n",
        "\n",
        "# numpy arrays\n",
        "ex_suff = np.asarray(ex_suff); sh_suff = np.asarray(sh_suff)\n",
        "ex_delA = np.asarray(ex_delA); sh_delA = np.asarray(sh_delA)\n",
        "ex_mi   = np.asarray(ex_mi);   sh_mi   = np.asarray(sh_mi)\n",
        "ex_time = np.asarray(ex_time); sh_time = np.asarray(sh_time)\n",
        "\n",
        "# deltas (flip ↓ so positive means ExCIR better)\n",
        "d_suff = float(ex_suff.mean() - sh_suff.mean())      # ↑\n",
        "d_delA = float(-(ex_delA.mean() - sh_delA.mean()))   # ↓\n",
        "d_mi   = float(ex_mi.mean()   - sh_mi.mean())        # ↑\n",
        "d_time = float(-(ex_time.mean() - sh_time.mean()))   # ↓\n",
        "\n",
        "def stat_block(x, y):\n",
        "    delta  = cliffs_delta(x, y)\n",
        "    lo, hi = cliffs_delta_boot_ci(x, y, B=4000, alpha=0.05, rng=RANDOM_SEED)\n",
        "    p      = mw_pvalue(x, y)\n",
        "    return delta, lo, hi, p\n",
        "\n",
        "cd_suff, lo_suff, hi_suff, p_suff = stat_block(ex_suff, sh_suff)\n",
        "cd_delA, lo_delA, hi_delA, p_delA = stat_block(ex_delA, sh_delA)\n",
        "cd_mi,   lo_mi,   hi_mi,   p_mi   = stat_block(ex_mi,   sh_mi)\n",
        "cd_time, lo_time, hi_time, p_time = stat_block(ex_time, sh_time)\n",
        "\n",
        "# BH–FDR across 4 tests\n",
        "pvec = np.array([p_suff, p_delA, p_mi, p_time], dtype=float)\n",
        "qvals, disc = bh_fdr(pvec, q=0.1)\n",
        "q_suff, q_delA, q_mi, q_time = qvals\n",
        "v_suff = \"Sig.\" if disc[0] else \"NS\"\n",
        "v_delA = \"Sig.\" if disc[1] else \"NS\"\n",
        "v_mi   = \"Sig.\" if disc[2] else \"NS\"\n",
        "v_time = \"Sig.\" if disc[3] else \"NS\"\n",
        "\n",
        "# ---------- LaTeX table ----------\n",
        "print(\"\\\\begin{table}[t]\")\n",
        "print(\"\\\\centering\")\n",
        "print(\"\\\\caption{Vehicular (validation, full model): agreement and significance summary \"\n",
        "      \"with BH--FDR at $q{=}0.1$. Positive $\\\\Delta$ favors ExCIR (sign flipped for $\\\\downarrow$).}\")\n",
        "print(\"\\\\label{tab:veh_agreement_signif}\")\n",
        "print(\"\\\\begin{tabular}{l l c c c c l}\")\n",
        "print(\"\\\\toprule\")\n",
        "print(\"\\\\textbf{Metric} & \\\\textbf{Comparison} & $\\\\boldsymbol{\\\\Delta}$ & \\\\textbf{Cliff's $\\\\boldsymbol{\\\\delta}$} & $\\\\boldsymbol{p}$ & $\\\\boldsymbol{q_{\\\\mathrm{BH}}}$ & \\\\textbf{Verdict} \\\\\\\\\")\n",
        "print(\"\\\\midrule\")\n",
        "print(f\"$\\\\Delta$-Sufficiency $\\\\uparrow$ & ExCIR vs SHAP-proxy & {d_suff:+.3f} & {cd_suff:.3f} & {p_suff:.3g} & {q_suff:.3g} & {v_suff}\\\\\\\\\")\n",
        "print(f\"Deletion area $\\\\downarrow$ & ExCIR vs SHAP-proxy & {d_delA:+.3f} & {cd_delA:.3f} & {p_delA:.3g} & {q_delA:.3g} & {v_delA}\\\\\\\\\")\n",
        "print(f\"MI faithfulness $\\\\uparrow$ & ExCIR vs SHAP-proxy & {d_mi:+.3f} & {cd_mi:.3f} & {p_mi:.3g} & {q_mi:.3g} & {v_mi}\\\\\\\\\")\n",
        "print(f\"Time (s) $\\\\downarrow$ & ExCIR vs SHAP-proxy & {d_time:+.3f} & {cd_time:.3f} & {p_time:.3g} & {q_time:.3g} & {v_time}\\\\\\\\\")\n",
        "print(\"\\\\bottomrule\")\n",
        "print(\"\\\\end{tabular}\")\n",
        "print(\"\\\\end{table}\")\n"
      ],
      "metadata": {
        "id": "Xa2jRsfrzhMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Fill significance cells (no n/a): synthetic vehicular\n",
        "# ============================================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "B_BOOT = 100\n",
        "K_HEAD = 8  # for head-based metrics\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def cliffs_delta(x, y):\n",
        "    x = np.asarray(x, dtype=float); x = x[~np.isnan(x)]\n",
        "    y = np.asarray(y, dtype=float); y = y[~np.isnan(y)]\n",
        "    gt = (x[:, None] > y[None, :]).astype(int)\n",
        "    lt = (x[:, None] < y[None, :]).astype(int)\n",
        "    return float((gt - lt).mean())\n",
        "\n",
        "def cliffs_delta_boot_ci(x, y, B=4000, alpha=0.05, rng=42):\n",
        "    rng = np.random.default_rng(rng)\n",
        "    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)\n",
        "    n, m = len(x), len(y)\n",
        "    deltas = np.empty(B, dtype=float)\n",
        "    for b in range(B):\n",
        "        xb = x[rng.integers(0, n, n)]\n",
        "        yb = y[rng.integers(0, m, m)]\n",
        "        deltas[b] = cliffs_delta(xb, yb)\n",
        "    lo = float(np.quantile(deltas, alpha/2))\n",
        "    hi = float(np.quantile(deltas, 1 - alpha/2))\n",
        "    return lo, hi\n",
        "\n",
        "def mw_pvalue(x, y):\n",
        "    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)\n",
        "    return float(mannwhitneyu(x, y, alternative=\"two-sided\", method=\"auto\").pvalue)\n",
        "\n",
        "def bh_fdr(pvals, q=0.1):\n",
        "    pvals = np.asarray(pvals, dtype=float)\n",
        "    m = len(pvals)\n",
        "    order = np.argsort(pvals)\n",
        "    ranked = pvals[order]\n",
        "    qvals = np.empty_like(ranked, dtype=float)\n",
        "    prev = 1.0\n",
        "    for i in range(m-1, -1, -1):\n",
        "        prev = min(prev, ranked[i] * m / (i+1))\n",
        "        qvals[i] = prev\n",
        "    out = np.empty_like(qvals); out[order] = qvals\n",
        "    disc = out <= q\n",
        "    return out, disc\n",
        "\n",
        "def area_under_curve(values):\n",
        "    v = np.asarray(values, dtype=float)\n",
        "    if v.size <= 1: return 0.0\n",
        "    x = np.linspace(0, 1, v.size)\n",
        "    return float(np.trapezoid(v, x))  # np.trapz deprecated\n",
        "\n",
        "def sufficiency_AOPC(ins_scores):\n",
        "    v = np.asarray(ins_scores, dtype=float)\n",
        "    base, top = v[0], v.max()\n",
        "    if np.isclose(top - base, 0): return 0.0\n",
        "    return area_under_curve((v - base) / (top - base))\n",
        "\n",
        "def deletion_area(del_scores):\n",
        "    v = np.asarray(del_scores, dtype=float)\n",
        "    lo, hi = v.min(), v.max()\n",
        "    if np.isclose(hi - lo, 0): return 0.0\n",
        "    return area_under_curve((v - lo) / (hi - lo))\n",
        "\n",
        "def corr_ratio(y, x, n_bins=10):\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    vy = np.var(y)\n",
        "    if vy <= 0: return 0.0\n",
        "    qs = np.linspace(0, 1, n_bins+1)\n",
        "    edges = np.unique(np.quantile(x, qs))\n",
        "    if len(edges) <= 2: return 0.0\n",
        "    bins = np.digitize(x, edges[1:-1], right=True)\n",
        "    mu = y.mean()\n",
        "    group_means = []\n",
        "    weights = []\n",
        "    for g in range(len(edges)-1):\n",
        "        idx = (bins == g)\n",
        "        if idx.any():\n",
        "            group_means.append(y[idx].mean())\n",
        "            weights.append(idx.mean())\n",
        "        else:\n",
        "            group_means.append(mu); weights.append(0.0)\n",
        "    group_means = np.asarray(group_means); weights = np.asarray(weights)\n",
        "    num = np.sum(weights * (group_means - mu)**2)\n",
        "    return float(num / vy)\n",
        "\n",
        "def excir_scores(X_val_df, yprob_val, n_bins=10):\n",
        "    s = {}\n",
        "    for f in X_val_df.columns:\n",
        "        s[f] = corr_ratio(yprob_val, X_val_df[f].values, n_bins=n_bins)\n",
        "    return pd.Series(s).sort_values(ascending=False)\n",
        "\n",
        "def perm_importance_scores(model, X_val_s, y_val, feature_names, n_repeats=5, random_state=42):\n",
        "    r = permutation_importance(model, X_val_s, y_val, n_repeats=n_repeats,\n",
        "                               random_state=random_state, scoring=\"roc_auc\")\n",
        "    s = pd.Series(r.importances_mean, index=feature_names).fillna(0.0)\n",
        "    return s.sort_values(ascending=False)\n",
        "\n",
        "def insertion_deletion_curves(model, scaler, X_val_df, y_val, ranking, baseline=\"mean\"):\n",
        "    X = X_val_df.copy(); cols = X.columns.tolist(); d = len(cols)\n",
        "    base = X.mean(axis=0) if baseline == \"mean\" else pd.Series(0.0, index=cols)\n",
        "    X_full = X.copy()\n",
        "    X_base = pd.DataFrame(np.tile(base.values, (len(X),1)), columns=cols)\n",
        "    # insertion\n",
        "    cur = X_base.copy()\n",
        "    ins = [model.predict_proba(scaler.transform(cur))[:,1].mean()]\n",
        "    for k in range(d):\n",
        "        f = ranking[k]; cur[f] = X[f].values\n",
        "        ins.append(model.predict_proba(scaler.transform(cur))[:,1].mean())\n",
        "    # deletion\n",
        "    cur = X_full.copy()\n",
        "    dele = [model.predict_proba(scaler.transform(cur))[:,1].mean()]\n",
        "    for k in range(d):\n",
        "        f = ranking[k]; cur[f] = base[f]\n",
        "        dele.append(model.predict_proba(scaler.transform(cur))[:,1].mean())\n",
        "    return np.array(ins), np.array(dele)\n",
        "\n",
        "def hist_mi(x, y, n_bins=10):\n",
        "    \"\"\"Histogram MI for 1D arrays.\"\"\"\n",
        "    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)\n",
        "    xbins = np.unique(np.quantile(x, np.linspace(0,1,n_bins+1)))\n",
        "    ybins = np.unique(np.quantile(y, np.linspace(0,1,n_bins+1)))\n",
        "    if len(xbins) <= 2 or len(ybins) <= 2: return 0.0\n",
        "    H, _, _ = np.histogram2d(x, y, bins=(xbins, ybins))\n",
        "    P = H / H.sum()\n",
        "    px = P.sum(axis=1, keepdims=True)\n",
        "    py = P.sum(axis=0, keepdims=True)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        ratio = np.where((P>0) & (px>0) & (py>0), P / (px @ py), 1.0)\n",
        "        log_ratio = np.log(ratio)\n",
        "        mi = np.where(P>0, P * log_ratio, 0.0).sum()\n",
        "    return float(mi)\n",
        "\n",
        "def mi_faithfulness(X_val_df, yprob, ranking, k=K_HEAD, n_bins=10):\n",
        "    \"\"\"Average MI( ŷ ; f_j ) over the top-k features in ranking.\"\"\"\n",
        "    top = ranking[:k]\n",
        "    mivals = [hist_mi(yprob, X_val_df[f].values, n_bins=n_bins) for f in top]\n",
        "    return float(np.mean(mivals)) if mivals else 0.0\n",
        "\n",
        "# ---------- synth vehicular ----------\n",
        "N = 6000\n",
        "speed = np.clip(rng.normal(80, 15, N), 0, None)\n",
        "throttle = np.clip(rng.beta(2,2, N), 0, 1)\n",
        "brake = np.clip(1 - throttle + rng.normal(0, 0.15, N), 0, 1)\n",
        "steering = rng.normal(0, 10, N)\n",
        "gear = np.clip((speed // 20) + rng.normal(0.0, 0.5, N), 1, 7)\n",
        "accel_long = rng.normal(0.05*throttle*speed - 0.08*brake*speed, 0.5, N)\n",
        "accel_lat  = rng.normal(np.abs(steering)/18 * (speed/80), 0.2, N)\n",
        "yaw_rate   = rng.normal(steering/30 * (speed/60), 0.2, N)\n",
        "road_grade = rng.normal(0, 2, N)\n",
        "ambient_temp = rng.normal(20, 8, N)\n",
        "tire_base = rng.normal(34, 1.0, (N,4))\n",
        "low_mask  = rng.uniform(0,1,N) < 0.15\n",
        "tire_drop = rng.normal(4, 1.0, (N,4)) * low_mask[:,None]\n",
        "tires = tire_base - tire_drop\n",
        "engine_load = np.clip(30 + 50*throttle + 5*road_grade + rng.normal(0, 5, N), 0, 100)\n",
        "maf = np.clip(5 + 0.06*speed + 0.5*engine_load/100 + rng.normal(0,0.7,N), 0, None)\n",
        "intake_air_temp = np.clip(ambient_temp + rng.normal(10, 2, N), -10, 80)\n",
        "battery_v = np.clip(rng.normal(13.8, 0.3, N) - 0.2*brake + 0.05*(engine_load/100), 11.5, 15)\n",
        "fuel_rate = np.clip(0.5 + 0.02*speed + 0.6*throttle + 0.1*(engine_load/100) + rng.normal(0,0.2,N), 0, None)\n",
        "rpm = np.clip(800 + 35*speed + 1200*throttle + rng.normal(0, 300, N), 700, 7000)\n",
        "\n",
        "X_df = pd.DataFrame({\n",
        "    \"speed_kph\": speed, \"rpm\": rpm, \"throttle\": throttle, \"brake\": brake,\n",
        "    \"steering_deg\": steering, \"gear\": gear, \"accel_long\": accel_long, \"accel_lat\": accel_lat,\n",
        "    \"yaw_rate\": yaw_rate, \"road_grade\": road_grade, \"ambient_temp\": ambient_temp,\n",
        "    \"tire_fl\": tires[:,0], \"tire_fr\": tires[:,1], \"tire_rl\": tires[:,2], \"tire_rr\": tires[:,3],\n",
        "    \"engine_load\": engine_load, \"maf\": maf, \"intake_air_temp\": intake_air_temp,\n",
        "    \"battery_v\": battery_v, \"fuel_rate\": fuel_rate,\n",
        "})\n",
        "low_tire = (32 - X_df[[\"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\"]].min(axis=1)).clip(lower=0)\n",
        "risk_logit = (\n",
        "    1.2*(X_df[\"speed_kph\"]-110)/20 + 1.1*X_df[\"brake\"] + 0.9*np.abs(X_df[\"steering_deg\"])/15\n",
        "    + 0.7*np.abs(X_df[\"yaw_rate\"]) + 0.8*(low_tire) + 0.7*(X_df[\"engine_load\"]/100)\n",
        "    + 0.3*(X_df[\"road_grade\"]/5) - 0.2*(X_df[\"battery_v\"]-13.5)\n",
        ")\n",
        "p = 1/(1+np.exp(-(risk_logit + rng.normal(0, 0.4, N))))\n",
        "y = (rng.uniform(0,1,N) < p).astype(int)\n",
        "\n",
        "# splits\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_df, y, test_size=0.20, stratify=y, random_state=RANDOM_SEED)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_tr, y_tr, test_size=0.20, stratify=y_tr, random_state=RANDOM_SEED)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_tr_s   = scaler.fit_transform(X_tr)\n",
        "X_val_s  = scaler.transform(X_val)\n",
        "\n",
        "# model\n",
        "model = GradientBoostingClassifier(random_state=RANDOM_SEED).fit(np.vstack([X_tr_s, X_val_s]),\n",
        "                                                                 np.concatenate([y_tr, y_val]))\n",
        "yprob_val = model.predict_proba(X_val_s)[:,1]\n",
        "\n",
        "# containers for paired arrays\n",
        "ex_suff, sh_suff = [], []\n",
        "ex_delA, sh_delA = [], []\n",
        "ex_mi,   sh_mi   = [], []\n",
        "ex_time, sh_time = [], []\n",
        "\n",
        "# Convert y_val to numpy array to avoid pandas indexing issues\n",
        "y_val_arr = np.asarray(y_val)\n",
        "\n",
        "# bootstraps on validation\n",
        "for b in range(B_BOOT):\n",
        "    idx = rng.integers(0, len(X_val), len(X_val))   # bootstrap row positions\n",
        "    Xb  = X_val.iloc[idx].reset_index(drop=True)    # iloc uses positions\n",
        "    yb  = y_val_arr[idx]                            # pure NumPy indexing\n",
        "    Xb_s = scaler.transform(Xb)\n",
        "    yhatb = model.predict_proba(Xb_s)[:, 1]\n",
        "\n",
        "    # ExCIR ranking + timing\n",
        "    t0 = time.time()\n",
        "    r_ex = excir_scores(Xb, yhatb, n_bins=10).index.tolist()\n",
        "    ex_time.append(time.time() - t0)\n",
        "\n",
        "    # SHAP proxy ranking (permutation importance) + timing\n",
        "    t0 = time.time()\n",
        "    r_sh = perm_importance_scores(model, Xb_s, yb, feature_names=Xb.columns,\n",
        "                                  n_repeats=5, random_state=int(rng.integers(0, 10_000))).index.tolist()\n",
        "    sh_time.append(time.time() - t0)\n",
        "\n",
        "    # insertion/deletion metrics (paired)\n",
        "    ins_e, del_e = insertion_deletion_curves(model, scaler, Xb, yb, r_ex)\n",
        "    ins_s, del_s = insertion_deletion_curves(model, scaler, Xb, yb, r_sh)\n",
        "    ex_suff.append(sufficiency_AOPC(ins_e))\n",
        "    sh_suff.append(sufficiency_AOPC(ins_s))\n",
        "    ex_delA.append(deletion_area(del_e))\n",
        "    sh_delA.append(deletion_area(del_s))\n",
        "\n",
        "    # MI faithfulness (average MI over top-k features)\n",
        "    ex_mi.append(mi_faithfulness(Xb, yhatb, r_ex, k=K_HEAD, n_bins=10))\n",
        "    sh_mi.append(mi_faithfulness(Xb, yhatb, r_sh, k=K_HEAD, n_bins=10))\n",
        "\n",
        "# numpy arrays\n",
        "ex_suff = np.array(ex_suff); sh_suff = np.array(sh_suff)\n",
        "ex_delA = np.array(ex_delA); sh_delA = np.array(sh_delA)\n",
        "ex_mi   = np.array(ex_mi);   sh_mi   = np.array(sh_mi)\n",
        "ex_time = np.array(ex_time); sh_time = np.array(sh_time)\n",
        "\n",
        "# deltas (flip ↓ so positive means ExCIR better)\n",
        "d_suff = float(ex_suff.mean() - sh_suff.mean())      # ↑\n",
        "d_delA = float(-(ex_delA.mean() - sh_delA.mean()))   # ↓\n",
        "d_mi   = float(ex_mi.mean()   - sh_mi.mean())        # ↑\n",
        "d_time = float(-(ex_time.mean() - sh_time.mean()))   # ↓ (less time is better)\n",
        "\n",
        "# Cliff's δ, CI, p-values\n",
        "def stat_block(x, y):\n",
        "    delta  = cliffs_delta(x, y)\n",
        "    lo, hi = cliffs_delta_boot_ci(x, y, B=4000, alpha=0.05, rng=RANDOM_SEED)\n",
        "    p      = mw_pvalue(x, y)\n",
        "    return delta, lo, hi, p\n",
        "\n",
        "cd_suff, lo_suff, hi_suff, p_suff = stat_block(ex_suff, sh_suff)\n",
        "cd_delA, lo_delA, hi_delA, p_delA = stat_block(ex_delA, sh_delA)\n",
        "cd_mi,   lo_mi,   hi_mi,   p_mi   = stat_block(ex_mi,   sh_mi)\n",
        "cd_time, lo_time, hi_time, p_time = stat_block(ex_time, sh_time)\n",
        "\n",
        "# BH–FDR across 4 tests\n",
        "pvec = np.array([p_suff, p_delA, p_mi, p_time], dtype=float)\n",
        "qvals, disc = bh_fdr(pvec, q=0.1)\n",
        "q_suff, q_delA, q_mi, q_time = qvals\n",
        "v_suff = \"Sig.\" if disc[0] else \"NS\"\n",
        "v_delA = \"Sig.\" if disc[1] else \"NS\"\n",
        "v_mi   = \"Sig.\" if disc[2] else \"NS\"\n",
        "v_time = \"Sig.\" if disc[3] else \"NS\"\n",
        "\n",
        "# ---------- LaTeX table (no n/a) ----------\n",
        "print(\"\\\\begin{table}[t]\")\n",
        "print(\"\\\\centering\")\n",
        "print(\"\\\\caption{Vehicular (validation, full model): agreement and significance summary \"\n",
        "      \"with BH--FDR at $q{=}0.1$. Positive $\\\\Delta$ favors ExCIR (sign flipped for $\\\\downarrow$).}\")\n",
        "print(\"\\\\label{tab:veh_agreement_signif}\")\n",
        "print(\"\\\\begin{tabular}{l l c c c c l}\")\n",
        "print(\"\\\\toprule\")\n",
        "print(\"\\\\textbf{Metric} & \\\\textbf{Comparison} & $\\\\boldsymbol{\\\\Delta}$ & \\\\textbf{Cliff's $\\\\boldsymbol{\\\\delta}$} & $\\\\boldsymbol{p}$ & $\\\\boldsymbol{q_{\\\\mathrm{BH}}}$ & \\\\textbf{Verdict} \\\\\\\\\")\n",
        "print(\"\\\\midrule\")\n",
        "print(f\"$\\\\Delta$-Sufficiency $\\\\uparrow$ & ExCIR vs SHAP-proxy & {d_suff:+.3f} & {cd_suff:.3f} & {p_suff:.3g} & {q_suff:.3g} & {v_suff}\\\\\\\\\")\n",
        "print(f\"Deletion area $\\\\downarrow$ & ExCIR vs SHAP-proxy & {d_delA:+.3f} & {cd_delA:.3f} & {p_delA:.3g} & {q_delA:.3g} & {v_delA}\\\\\\\\\")\n",
        "print(f\"MI faithfulness $\\\\uparrow$ & ExCIR vs SHAP-proxy & {d_mi:+.3f} & {cd_mi:.3f} & {p_mi:.3g} & {q_mi:.3g} & {v_mi}\\\\\\\\\")\n",
        "print(f\"Time (s) $\\\\downarrow$ & ExCIR vs SHAP-proxy & {d_time:+.3f} & {cd_time:.3f} & {p_time:.3g} & {q_time:.3g} & {v_time}\\\\\\\\\")\n",
        "print(\"\\\\bottomrule\")\n",
        "print(\"\\\\end{tabular}\")\n",
        "print(\"\\\\end{table}\")"
      ],
      "metadata": {
        "id": "x0C0AUL4zjth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Synthetic nonlinear dataset\n",
        "# -------------------------------\n",
        "np.random.seed(0)\n",
        "n = 500\n",
        "X = np.linspace(0, 1, n)\n",
        "Y = np.sin(4 * np.pi * X) + 0.1 * np.random.randn(n)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X.reshape(-1, 1)).ravel()\n",
        "Y_std = scaler.fit_transform(Y.reshape(-1, 1)).ravel()\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Linear ExCIR (normalized covariance ratio)\n",
        "# -------------------------------\n",
        "cov_xy = np.cov(X_std, Y_std, ddof=0)[0, 1]\n",
        "var_x = np.var(X_std)\n",
        "var_y = np.var(Y_std)\n",
        "eta_linear = cov_xy / (var_x + var_y)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Kernel–ExCIR\n",
        "# -------------------------------\n",
        "def kernel_excir(X, Y, gamma=10.0):\n",
        "    # Gram matrices\n",
        "    Kx = rbf_kernel(X.reshape(-1, 1), gamma=gamma)\n",
        "    Ky = rbf_kernel(Y.reshape(-1, 1), gamma=gamma)\n",
        "    # Centering matrix\n",
        "    H = np.eye(len(X)) - np.ones((len(X), len(X))) / len(X)\n",
        "    Kxc = H @ Kx @ H\n",
        "    Kyc = H @ Ky @ H\n",
        "    num = np.trace(Kxc @ Kyc)\n",
        "    den = np.trace(Kxc @ Kxc) + np.trace(Kyc @ Kyc)\n",
        "    return num / den\n",
        "\n",
        "eta_kernel = kernel_excir(X_std, Y_std, gamma=10.0)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Empirical Mutual Information\n",
        "# -------------------------------\n",
        "mi_est = mutual_info_regression(X_std.reshape(-1, 1), Y_std, n_neighbors=10, random_state=0)\n",
        "mi_value = mi_est[0]\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Plot comparison\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X, Y, s=10, color=\"tab:blue\", alpha=0.6, label=\"Data: Y = sin(4πX)+noise\")\n",
        "\n",
        "# Annotated bars for scores\n",
        "plt.text(0.05, 1.1, rf\"$\\eta$ (linear) = {eta_linear:.2f}$\\;$\",\n",
        "         fontsize=10, color=\"tab:gray\")\n",
        "plt.text(0.05, 0.95, rf\"$\\eta^{{(k)}}$ (RBF) = {eta_kernel:.2f}$\\;$\",\n",
        "         fontsize=10, color=\"tab:green\")\n",
        "plt.text(0.05, 0.80, rf\"$\\widehat{{I}}(X;Y)$ (kNN) = {mi_value:.2f}$\\;$\",\n",
        "         fontsize=10, color=\"tab:red\")\n",
        "\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Linear vs. Kernel–ExCIR on Nonlinear Dependence\", fontsize=11)\n",
        "plt.legend(loc=\"lower right\", frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"kernel_excir_comparison.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Print numeric summary\n",
        "print(f\"Linear ExCIR (eta): {eta_linear:.4f}\")\n",
        "print(f\"Kernel–ExCIR (eta^k): {eta_kernel:.4f}\")\n",
        "print(f\"Empirical MI (kNN): {mi_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "8JdU3hU3zpsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# synthetic_vehicular_kernel_excir_6000.py\n",
        "# -----------------------------------------------------------\n",
        "# Generates 6,000-sample vehicular dataset with 20 sensors,\n",
        "# computes Linear ExCIR (eta), Kernel–ExCIR (eta^k), and kNN MI\n",
        "# for each feature vs fuel_rate, and saves figures + CSV.\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import csv\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "# -------------------------------\n",
        "# Utilities\n",
        "# -------------------------------\n",
        "def median_heuristic_gamma(x):\n",
        "    \"\"\"RBF gamma via median heuristic (1 / (2 * median^2)).\"\"\"\n",
        "    x = np.asarray(x).reshape(-1, 1)\n",
        "    D = np.abs(x - x.T)\n",
        "    med = np.median(D[D > 0])\n",
        "    if med <= 0 or np.isnan(med):\n",
        "        med = 1.0\n",
        "    return 1.0 / (2.0 * (med ** 2))\n",
        "\n",
        "def kernel_excir(x, y, gamma=None):\n",
        "    \"\"\"\n",
        "    Kernel–ExCIR (RBF), value in [0,1].\n",
        "    x, y: 1D arrays (standardized recommended).\n",
        "    gamma: RBF gamma; if None, use median heuristic on x⊕y (robust default).\n",
        "    \"\"\"\n",
        "    x = np.asarray(x).ravel()\n",
        "    y = np.asarray(y).ravel()\n",
        "    n = len(x)\n",
        "\n",
        "    if gamma is None:\n",
        "        # median heuristic on concatenated (scaled) vector\n",
        "        z = np.concatenate([x, y])\n",
        "        gamma = median_heuristic_gamma(z)\n",
        "\n",
        "    Kx = rbf_kernel(x.reshape(-1, 1), gamma=gamma)\n",
        "    Ky = rbf_kernel(y.reshape(-1, 1), gamma=gamma)\n",
        "    H = np.eye(n) - np.ones((n, n)) / n\n",
        "    Kxc = H @ Kx @ H\n",
        "    Kyc = H @ Ky @ H\n",
        "    num = np.trace(Kxc @ Kyc)\n",
        "    den = np.trace(Kxc @ Kxc) + np.trace(Kyc @ Kyc)\n",
        "    return float(num / den)\n",
        "\n",
        "def linear_excir(x, y):\n",
        "    \"\"\"\n",
        "    Linear ExCIR (scalar case): normalized covariance ratio.\n",
        "    x, y should be standardized (z-scores). Returns eta in [-1,1].\n",
        "    \"\"\"\n",
        "    x = np.asarray(x).ravel()\n",
        "    y = np.asarray(y).ravel()\n",
        "    cov_xy = np.cov(x, y, ddof=0)[0, 1]\n",
        "    var_x = np.var(x)\n",
        "    var_y = np.var(y)\n",
        "    return float(cov_xy / (var_x + var_y + 1e-12))\n",
        "\n",
        "def knn_mi(x, y, n_neighbors=10):\n",
        "    \"\"\"kNN mutual information estimate (scikit-learn).\"\"\"\n",
        "    x = np.asarray(x).reshape(-1, 1)\n",
        "    y = np.asarray(y).ravel()\n",
        "    return float(mutual_info_regression(x, y, n_neighbors=n_neighbors, random_state=0)[0])\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Synthetic vehicular dataset (n=6000, 20 sensors)\n",
        "# -------------------------------\n",
        "n = 6000\n",
        "t = np.linspace(0, 1, n)\n",
        "\n",
        "# Control (2)\n",
        "brake    = np.clip(np.random.beta(2, 8, size=n) * 1.5 + 0.05*np.random.randn(n), 0, 1)\n",
        "throttle = np.clip(0.6*np.sin(6*np.pi*t) + 0.4 + 0.08*np.random.randn(n), 0, 1)\n",
        "\n",
        "# Dynamics (5)\n",
        "speed_kph = np.clip(15 + 95*np.abs(np.sin(8*np.pi*t)) + 6*np.random.randn(n), 0, 170)\n",
        "rpm       = np.clip(700 + 3700*np.abs(np.sin(7*np.pi*t + 0.7)) + 130*np.random.randn(n), 600, 6800)\n",
        "accel_lat = 0.5*np.sin(14*np.pi*t) + 0.22*np.random.randn(n)\n",
        "accel_long= 0.6*np.sin(10*np.pi*t + 0.8) + 0.25*np.random.randn(n)\n",
        "yaw_rate  = 0.3*np.sin(16*np.pi*t + 0.2) + 0.18*np.random.randn(n)\n",
        "\n",
        "# Environment / engine air (4)\n",
        "road_grade = 4*np.sin(3*np.pi*t + 0.3) + 0.5*np.random.randn(n)         # degrees\n",
        "maf        = np.clip(9 + 7*np.sin(9*np.pi*t - 0.2) + 1.4*np.random.randn(n), 2, 28)  # mass air flow\n",
        "intake_temp= np.clip(25 + 6*np.sin(5*np.pi*t + 0.5) + 1.8*np.random.randn(n), -10, 55)\n",
        "ambient_temp=np.clip(15 + 10*np.sin(2*np.pi*t) + 2.2*np.random.randn(n), -15, 45)\n",
        "\n",
        "# Tires: correlated (8 total)\n",
        "base_pressure = 34 + 0.9*np.sin(2*np.pi*t) + 0.5*np.random.randn(n)\n",
        "tire_fl = base_pressure + 0.7*np.random.randn(n)\n",
        "tire_fr = base_pressure + 0.7*np.random.randn(n)\n",
        "tire_rl = base_pressure + 0.8*np.random.randn(n)\n",
        "tire_rr = base_pressure + 0.8*np.random.randn(n)\n",
        "\n",
        "# Add tire temperatures (correlated with pressures + speed)\n",
        "tire_temp_fl = 28 + 0.12*speed_kph + 0.7*(tire_fl-34) + 1.5*np.random.randn(n)\n",
        "tire_temp_fr = 28 + 0.12*speed_kph + 0.7*(tire_fr-34) + 1.5*np.random.randn(n)\n",
        "tire_temp_rl = 28 + 0.10*speed_kph + 0.7*(tire_rl-34) + 1.7*np.random.randn(n)\n",
        "tire_temp_rr = 28 + 0.10*speed_kph + 0.7*(tire_rr-34) + 1.7*np.random.randn(n)\n",
        "\n",
        "# O2 sensor (lambda) and EGR flow (2) to reach 20 sensors\n",
        "o2_lambda = 1.0 + 0.03*np.sin(12*np.pi*t) + 0.02*np.random.randn(n)      # around stoichiometric\n",
        "egr_flow  = np.clip(5 + 2*np.sin(6*np.pi*t + 0.3) + 0.8*np.random.randn(n), 0, 12)\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Nonlinear target: fuel_rate (proxy)\n",
        "#    Designed to include nonlinearities + interactions,\n",
        "#    so Kernel–ExCIR captures dependence better than linear.\n",
        "# -------------------------------\n",
        "def relu(z):\n",
        "    return np.maximum(z, 0)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "fuel_rate = (\n",
        "    0.45*brake\n",
        "    + 0.28*sigmoid((speed_kph-60)/12)\n",
        "    + 0.22*np.sin(rpm/420)\n",
        "    + 0.18*relu(road_grade)/6\n",
        "    + 0.07*(np.abs(tire_rr - tire_rl)/12)\n",
        "    + 0.09*throttle**2\n",
        "    + 0.08*np.sin(accel_lat*2.2)\n",
        "    + 0.11*(maf/28)**1.5\n",
        "    + 0.05*(tire_temp_fl + tire_temp_fr + tire_temp_rl + tire_temp_rr)/200\n",
        "    + 0.06*np.cos(yaw_rate*3.0)\n",
        "    + 0.04*np.sin(o2_lambda*20)\n",
        "    + 0.05*(egr_flow/12.0)\n",
        "    + 0.12*np.random.randn(n)   # noise\n",
        ")\n",
        "\n",
        "# z-score target\n",
        "fuel_rate = (fuel_rate - fuel_rate.mean()) / (fuel_rate.std() + 1e-12)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) Feature matrix and names\n",
        "# -------------------------------\n",
        "features = np.column_stack([\n",
        "    brake, throttle,\n",
        "    speed_kph, rpm, accel_lat, accel_long, yaw_rate,\n",
        "    road_grade, maf, intake_temp, ambient_temp,\n",
        "    tire_fl, tire_fr, tire_rl, tire_rr,\n",
        "    tire_temp_fl, tire_temp_fr, tire_temp_rl, tire_temp_rr,\n",
        "    o2_lambda, egr_flow\n",
        "])  # (n, 21) including egr_flow to reach 21; drop one if you want exactly 20.\n",
        "\n",
        "feature_names = [\n",
        "    \"brake\",\"throttle\",\n",
        "    \"speed_kph\",\"rpm\",\"accel_lat\",\"accel_long\",\"yaw_rate\",\n",
        "    \"road_grade\",\"maf\",\"intake_temp\",\"ambient_temp\",\n",
        "    \"tire_fl\",\"tire_fr\",\"tire_rl\",\"tire_rr\",\n",
        "    \"tire_temp_fl\",\"tire_temp_fr\",\"tire_temp_rl\",\"tire_temp_rr\",\n",
        "    \"o2_lambda\",\"egr_flow\"\n",
        "]\n",
        "\n",
        "# If you want exactly 20 sensors, drop 'egr_flow' (last one):\n",
        "# features = features[:, :-1]\n",
        "# feature_names = feature_names[:-1]\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Metrics per feature vs fuel_rate\n",
        "# -------------------------------\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "Y_std = scaler_y.fit_transform(fuel_rate.reshape(-1,1)).ravel()\n",
        "\n",
        "eta_lin_list = []\n",
        "eta_k_list   = []\n",
        "mi_list      = []\n",
        "\n",
        "for j in range(features.shape[1]):\n",
        "    x = features[:, j]\n",
        "    X_std = scaler_x.fit_transform(x.reshape(-1,1)).ravel()\n",
        "    eta_lin = linear_excir(X_std, Y_std)\n",
        "    eta_k   = kernel_excir(X_std, Y_std, gamma=None)  # median heuristic\n",
        "    mi_val  = knn_mi(X_std, Y_std, n_neighbors=10)\n",
        "    eta_lin_list.append(eta_lin)\n",
        "    eta_k_list.append(eta_k)\n",
        "    mi_list.append(mi_val)\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Save metrics to CSV\n",
        "# -------------------------------\n",
        "with open(\"vehicular_kernel_excir_metrics.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"feature\", \"eta_linear\", \"eta_kernel\", \"knn_mi\"])\n",
        "    for name, el, ek, mi in zip(feature_names, eta_lin_list, eta_k_list, mi_list):\n",
        "        writer.writerow([name, f\"{el:.6f}\", f\"{ek:.6f}\", f\"{mi:.6f}\"])\n",
        "\n",
        "print(\"Saved: vehicular_kernel_excir_metrics.csv\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Bar chart: per-feature η vs η^k\n",
        "# -------------------------------\n",
        "idx = np.arange(len(feature_names))\n",
        "w = 0.38\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(idx - w/2, eta_lin_list, width=w, label=\"Linear ExCIR (η)\")\n",
        "plt.bar(idx + w/2, eta_k_list,   width=w, label=\"Kernel–ExCIR (ηᵏ)\")\n",
        "plt.axhline(0.0, color=\"k\", lw=0.8)\n",
        "plt.xticks(idx, feature_names, rotation=60, ha=\"right\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Vehicular (n=6000): Linear vs. Kernel–ExCIR per feature\")\n",
        "plt.legend(frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"vehicular_kernel_excir_bar.png\", dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved: vehicular_kernel_excir_bar.png\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7) Detailed scatter for a nonlinear relation (rpm → fuel_rate)\n",
        "# -------------------------------\n",
        "rpm_std = StandardScaler().fit_transform(rpm.reshape(-1,1)).ravel()\n",
        "eta_rpm_lin = linear_excir(rpm_std, Y_std)\n",
        "eta_rpm_k   = kernel_excir(rpm_std, Y_std, gamma=None)\n",
        "mi_rpm      = knn_mi(rpm_std, Y_std, n_neighbors=10)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
        "ax.scatter(rpm, fuel_rate, s=8, alpha=0.7, label=\"fuel_rate vs rpm\")\n",
        "ax.set_xlabel(\"rpm\")\n",
        "ax.set_ylabel(\"fuel_rate (z-score)\")\n",
        "ax.set_title(\"Vehicular (n=6000): Linear vs. Kernel–ExCIR on rpm\")\n",
        "\n",
        "# top axis: sample index\n",
        "ax_top = ax.twiny()\n",
        "ax_top.set_xlim(ax.get_xlim())\n",
        "tick_pos = np.linspace(rpm.min(), rpm.max(), 6)\n",
        "tick_idx = np.round((tick_pos - rpm.min())/(rpm.max()-rpm.min())*(len(rpm)-1)).astype(int)\n",
        "ax_top.set_xticks(tick_pos)\n",
        "ax_top.set_xticklabels(tick_idx)\n",
        "ax_top.set_xlabel(\"Index\")\n",
        "\n",
        "# metric annotations\n",
        "ax.text(0.02, 1.06, rf\"$\\eta$ (linear) = {eta_rpm_lin:.2f}\", transform=ax.transAxes,\n",
        "        fontsize=10, bbox=dict(facecolor=\"white\", alpha=0.85, edgecolor=\"none\"))\n",
        "ax.text(0.02, 0.94, rf\"$\\eta^{{(k)}}$ (RBF) = {eta_rpm_k:.2f}\", transform=ax.transAxes,\n",
        "        fontsize=10, bbox=dict(facecolor=\"white\", alpha=0.85, edgecolor=\"none\"))\n",
        "ax.text(0.02, 0.82, rf\"$\\widehat{{I}}(X;Y)$ (kNN) = {mi_rpm:.2f}\", transform=ax.transAxes,\n",
        "        fontsize=10, bbox=dict(facecolor=\"white\", alpha=0.85, edgecolor=\"none\"))\n",
        "\n",
        "ax.grid(alpha=0.25); ax.legend(loc=\"lower right\", frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"kernel_excir_comparison_vehicular.png\", dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved: kernel_excir_comparison_vehicular.png\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8) Also save the generic filename used in LaTeX, if you like\n",
        "# -------------------------------\n",
        "# copy/duplicate the rpm figure name expected by LaTeX snippet\n",
        "import shutil\n",
        "shutil.copyfile(\"kernel_excir_comparison_vehicular.png\", \"kernel_excir_comparison.png\")\n",
        "print(\"Saved duplicate: kernel_excir_comparison.png\")\n"
      ],
      "metadata": {
        "id": "bJoM9p99zslr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Kernel–ExCIR demo (per IEEE TAI draft, Sec. G + Corr. 8.1)\n",
        "# - Kernel–ExCIR η^(k) = <H Kx H, H Ky H>_F /\n",
        "#                        sqrt(<H Kx H, H Kx H>_F * <H Ky H, H Ky H>_F)\n",
        "# - O(n^2), bounded/normalized HSIC/CKA-style correlation-ratio\n",
        "# - Failure-mode example on 6,000-sample vehicular data\n",
        "#   with a sinusoidal (non-monotone) target in tire_rr\n",
        "# References in your draft: Kernelized ExCIR (§G), MI control (Cor. 8.1).\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import rbf_kernel, linear_kernel, polynomial_kernel\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Helpers\n",
        "# -----------------------------\n",
        "def standardize(x):\n",
        "    x = np.asarray(x).reshape(-1, 1)\n",
        "    return StandardScaler().fit_transform(x).ravel()\n",
        "\n",
        "def linear_excir_abs_corr(x, y):\n",
        "    \"\"\"Linear ExCIR surrogate via |Pearson correlation| (bounded in [0,1]).\"\"\"\n",
        "    xz = standardize(x)\n",
        "    yz = standardize(y)\n",
        "    r = np.corrcoef(xz, yz)[0, 1]\n",
        "    return float(abs(r))\n",
        "\n",
        "def _centering_matrix(n):\n",
        "    H = np.eye(n) - np.ones((n, n)) / n\n",
        "    return H\n",
        "\n",
        "def _center_gram(K):\n",
        "    n = K.shape[0]\n",
        "    H = _centering_matrix(n)\n",
        "    return H @ K @ H\n",
        "\n",
        "def _median_gamma(X):\n",
        "    \"\"\"Median heuristic gamma for RBF: 1/(2*med(dist)^2).\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "    # pairwise squared distances\n",
        "    d2 = np.sum((X[:, None, :] - X[None, :, :]) ** 2, axis=2)\n",
        "    iu = np.triu_indices_from(d2, k=1)\n",
        "    med = np.median(np.sqrt(d2[iu]))\n",
        "    if med <= 1e-12:\n",
        "        med = 1.0\n",
        "    return 1.0 / (2.0 * med * med)\n",
        "\n",
        "def gram_matrix(X, kernel=\"rbf\", **kwargs):\n",
        "    \"\"\"\n",
        "    Build Gram matrix K for data vector/matrix X under a chosen kernel.\n",
        "    Kernels:\n",
        "      - 'rbf': RBF with gamma (median heuristic if not provided)\n",
        "      - 'linear': dot product\n",
        "      - 'poly': polynomial kernel with degree, gamma, coef0\n",
        "      - 'laplacian': exp(-gamma * ||x - x'||_1) [simple implementation]\n",
        "    \"\"\"\n",
        "    X = np.asarray(X)\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "\n",
        "    if kernel == \"rbf\":\n",
        "        gamma = kwargs.get(\"gamma\", None)\n",
        "        if gamma is None:\n",
        "            gamma = _median_gamma(X)\n",
        "        K = rbf_kernel(X, X, gamma=gamma)\n",
        "        return K\n",
        "\n",
        "    if kernel == \"linear\":\n",
        "        return linear_kernel(X, X)\n",
        "\n",
        "    if kernel == \"poly\":\n",
        "        degree = kwargs.get(\"degree\", 3)\n",
        "        gamma = kwargs.get(\"gamma\", None)\n",
        "        coef0 = kwargs.get(\"coef0\", 1.0)\n",
        "        if gamma is None:\n",
        "            # sklearn polynomial_kernel defaults: gamma = 1/n_features if None\n",
        "            pass\n",
        "        return polynomial_kernel(X, X, degree=degree, gamma=gamma, coef0=coef0)\n",
        "\n",
        "    if kernel == \"laplacian\":\n",
        "        # Simple L1-based Laplacian kernel\n",
        "        gamma = kwargs.get(\"gamma\", None)\n",
        "        if gamma is None:\n",
        "            # Heuristic similar to RBF median using L2; use same gamma fallback.\n",
        "            gamma = _median_gamma(X)\n",
        "        # pairwise L1 distances\n",
        "        D = np.sum(np.abs(X[:, None, :] - X[None, :, :]), axis=2)\n",
        "        return np.exp(-gamma * D)\n",
        "\n",
        "    raise ValueError(f\"Unknown kernel: {kernel}\")\n",
        "\n",
        "def kernel_excir_from_data(X, Y, kernel_x=\"rbf\", kernel_y=\"rbf\", **kws):\n",
        "    \"\"\"\n",
        "    Kernel–ExCIR η^(k) using centered Gram matrices:\n",
        "       η^(k) = <Kx~, Ky~>_F / sqrt(<Kx~, Kx~>_F * <Ky~, Ky~>_F)\n",
        "    Where Kx~ = H Kx H and Ky~ = H Ky H; H is centering matrix.\n",
        "    This matches your Kernelized ExCIR definition (bounded, HSIC/CKA-style).  :contentReference[oaicite:1]{index=1}\n",
        "    \"\"\"\n",
        "    Kx = gram_matrix(X, kernel=kernel_x, **kws.get(\"x\", {}))\n",
        "    Ky = gram_matrix(Y, kernel=kernel_y, **kws.get(\"y\", {}))\n",
        "    Kxc = _center_gram(Kx)\n",
        "    Kyc = _center_gram(Ky)\n",
        "    num = np.sum(Kxc * Kyc)\n",
        "    den = np.sqrt(np.sum(Kxc * Kxc) * np.sum(Kyc * Kyc)) + 1e-12\n",
        "    return float(num / den)\n",
        "\n",
        "def empirical_mi_knn(x, y, n_neighbors=5, random_state=0):\n",
        "    \"\"\"Empirical MI via sklearn kNN estimator (Kraskov-style surrogate).\"\"\"\n",
        "    X = np.asarray(x).reshape(-1, 1)\n",
        "    y = np.asarray(y).ravel()\n",
        "    mi = mutual_info_regression(X, y, n_neighbors=n_neighbors, random_state=random_state)\n",
        "    return float(mi[0])\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Synthesize vehicular data (6,000 samples)\n",
        "# -----------------------------\n",
        "def make_vehicular_data(n=6000, low_tire_rate=0.15, seed=42):\n",
        "    \"\"\"\n",
        "    Synthetic vehicular dataset with 20+ sensors and correlated tires.\n",
        "    Structured dependencies; 15% low-tire regime.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    road_grade = rng.normal(0.0, 1.0, n)\n",
        "    speed_kph = np.clip(rng.normal(80, 20, n), 0, 160)\n",
        "    accel_lat = rng.normal(0.0, 0.5, n)\n",
        "    brake = np.clip(rng.normal(0.1, 0.3, n), 0, 1.0)\n",
        "    maf = np.clip(20 + 0.5 * speed_kph + rng.normal(0, 5, n), 0, None)\n",
        "    rpm = np.clip(800 + 35 * speed_kph + rng.normal(0, 200, n), 600, 7000)\n",
        "    fuel_rate = np.clip(1.0 + 0.02 * speed_kph + 0.5 * brake + rng.normal(0, 0.3, n), 0, None)\n",
        "\n",
        "    # Tires with correlation\n",
        "    tire_base = rng.normal(32.0, 1.0, (n, 4))\n",
        "    shared = rng.normal(0, 0.6, (n, 1))\n",
        "    tire_mat = 0.7 * shared + 0.3 * tire_base  # correlated PSI: (fl, fr, rl, rr)\n",
        "\n",
        "    low_idx = rng.choice(n, size=int(low_tire_rate * n), replace=False)\n",
        "    tire_mat[low_idx, :] -= rng.normal(3.0, 0.7, (len(low_idx), 4))\n",
        "\n",
        "    fuel_rate[low_idx] += rng.normal(0.3, 0.2, len(low_idx))\n",
        "    accel_lat[low_idx] += rng.normal(0.05, 0.05, len(low_idx))\n",
        "\n",
        "    tire_fl, tire_fr, tire_rl, tire_rr = tire_mat.T\n",
        "\n",
        "    intake_temp = np.clip(25 + 0.1 * speed_kph + rng.normal(0, 2, n), -10, 80)\n",
        "    coolant_temp = np.clip(70 + 0.05 * rpm + rng.normal(0, 5, n), 60, 120)\n",
        "    oil_pressure = np.clip(30 + 0.005 * rpm + rng.normal(0, 2, n), 10, 100)\n",
        "    throttle_pos = np.clip(10 + 0.2 * speed_kph - 5 * brake + rng.normal(0, 3, n), 0, 100)\n",
        "    gear = np.clip((speed_kph // 20).astype(int) + rng.integers(-1, 2, n), 1, 8)\n",
        "    air_density = np.clip(1.225 - 0.0001 * intake_temp + rng.normal(0, 0.01, n), 1.0, 1.3)\n",
        "    battery_v = np.clip(13.5 + rng.normal(0, 0.2, n), 12.0, 14.8)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"speed_kph\": speed_kph,\n",
        "        \"rpm\": rpm,\n",
        "        \"accel_lat\": accel_lat,\n",
        "        \"brake\": brake,\n",
        "        \"road_grade\": road_grade,\n",
        "        \"maf\": maf,\n",
        "        \"fuel_rate\": fuel_rate,\n",
        "        \"tire_fl\": tire_fl,\n",
        "        \"tire_fr\": tire_fr,\n",
        "        \"tire_rl\": tire_rl,\n",
        "        \"tire_rr\": tire_rr,\n",
        "        \"intake_temp\": intake_temp,\n",
        "        \"coolant_temp\": coolant_temp,\n",
        "        \"oil_pressure\": oil_pressure,\n",
        "        \"throttle_pos\": throttle_pos,\n",
        "        \"gear\": gear.astype(float),\n",
        "        \"air_density\": air_density,\n",
        "        \"battery_v\": battery_v,\n",
        "        \"engine_load\": np.clip(20 + 0.4 * throttle_pos + 0.01 * rpm + rng.normal(0, 5, n), 0, 100),\n",
        "        \"manifold_press\": np.clip(95 + 0.2 * maf + rng.normal(0, 3, n), 80, 140),\n",
        "    })\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Build non-monotone failure-mode target\n",
        "# -----------------------------\n",
        "n = 6000\n",
        "veh = make_vehicular_data(n=n, low_tire_rate=0.15, seed=7)\n",
        "\n",
        "x_key = \"tire_rr\"\n",
        "x = veh[x_key].values\n",
        "x_std = standardize(x)\n",
        "\n",
        "rng = np.random.default_rng(7)\n",
        "# Sinusoidal (phase-wrapped) dependence in tire_rr + mild linear nuisance\n",
        "y = (\n",
        "    np.sin(4 * np.pi * (x_std - x_std.min()) / (x_std.max() - x_std.min()))\n",
        "    + 0.20 * standardize(veh[\"brake\"].values)\n",
        "    + 0.15 * standardize(veh[\"road_grade\"].values)\n",
        "    + 0.10 * standardize(veh[\"speed_kph\"].values)\n",
        "    + 0.10 * rng.normal(0, 1, n)\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Compute scores\n",
        "# -----------------------------\n",
        "# Linear ExCIR (|corr|)\n",
        "eta_linear = linear_excir_abs_corr(x, y)\n",
        "\n",
        "# Kernel–ExCIR (bounded, centered-Gram normalized Frobenius inner product)\n",
        "# Choose kernels for X and Y; per your draft, any reproducing kernels are allowed. :contentReference[oaicite:2]{index=2}\n",
        "eta_k_rbf = kernel_excir_from_data(x, y, kernel_x=\"rbf\", kernel_y=\"rbf\")           # median-heuristic RBF\n",
        "# You can also try linear/poly/laplacian as ablations:\n",
        "# eta_k_lin  = kernel_excir_from_data(x, y, kernel_x=\"linear\", kernel_y=\"linear\")\n",
        "# eta_k_poly = kernel_excir_from_data(x, y, kernel_x=\"poly\", kernel_y=\"poly\", y={\"degree\":3})\n",
        "\n",
        "# Empirical MI (kNN)\n",
        "mi_knn = empirical_mi_knn(x, y, n_neighbors=5, random_state=0)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Print A.15.1-style table\n",
        "# -----------------------------\n",
        "print(f\"Non–monotone dependence demo on vehicular data (n={n}), X = {x_key}\")\n",
        "df_table = pd.DataFrame({\n",
        "    \"Method\": [\"ExCIR (linear)\", \"Kernel–ExCIR (η^(k))\", \"Empirical MI (kNN)\"],\n",
        "    \"Score\":  [eta_linear,        eta_k_rbf,              mi_knn]\n",
        "})\n",
        "print(df_table)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) (Optional) Rank all features by Kernel–ExCIR\n",
        "# -----------------------------\n",
        "rows = []\n",
        "for col in veh.columns:\n",
        "    xi = veh[col].values\n",
        "    rows.append({\n",
        "        \"feature\": col,\n",
        "        \"eta_linear\": linear_excir_abs_corr(xi, y),\n",
        "        \"eta_k\": kernel_excir_from_data(xi, y, kernel_x=\"rbf\", kernel_y=\"rbf\"),\n",
        "        \"mi_knn\": empirical_mi_knn(xi, y, n_neighbors=5, random_state=0)\n",
        "    })\n",
        "summary = pd.DataFrame(rows).sort_values(\"eta_k\", ascending=False)\n",
        "print(\"\\nTop features by Kernel–ExCIR (η^(k)):\")\n",
        "print(summary.head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "eWUsiZ9szti5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Helpers\n",
        "# ---------------------------\n",
        "def center_gram(K: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Double-center a Gram matrix: Kc = HKH with H = I - 1/n 11^T.\"\"\"\n",
        "    n = K.shape[0]\n",
        "    H = np.eye(n) - np.ones((n, n)) / n\n",
        "    return H @ K @ H\n",
        "\n",
        "def gaussian_pdf_kernel_matrix(x: np.ndarray, sigma: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    PDF-defined Gaussian kernel Gram matrix:\n",
        "      k(x_i, x_j) = (1/(sqrt(2π) σ)) * exp(- (x_i - x_j)^2 / (2 σ^2))\n",
        "    \"\"\"\n",
        "    x = x.reshape(-1, 1)\n",
        "    d2 = (x - x.T) ** 2\n",
        "    norm_const = 1.0 / (np.sqrt(2.0 * np.pi) * sigma)\n",
        "    K = norm_const * np.exp(-0.5 * d2 / (sigma ** 2))\n",
        "    return K\n",
        "\n",
        "def median_heuristic_sigma(x: np.ndarray) -> float:\n",
        "    \"\"\"Median heuristic for RBF width.\"\"\"\n",
        "    x = x.reshape(-1, 1)\n",
        "    d = np.abs(x - x.T)\n",
        "    # Take upper triangle distances (excluding zeros on diagonal)\n",
        "    iu = np.triu_indices_from(d, k=1)\n",
        "    med = np.median(d[iu])\n",
        "    # fallback in edge-cases\n",
        "    return float(med if med > 0 else np.std(x) + 1e-8)\n",
        "\n",
        "def linear_excir_eta(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Linear ExCIR (normalized covariance ratio):\n",
        "      eta = Cov(x,y)^2 / (Var(x) Var(y)) = corr(x,y)^2\n",
        "    \"\"\"\n",
        "    x = (x - np.mean(x))\n",
        "    y = (y - np.mean(y))\n",
        "    cov = np.mean(x * y)\n",
        "    vx = np.mean(x ** 2)\n",
        "    vy = np.mean(y ** 2)\n",
        "    if vx <= 0 or vy <= 0:\n",
        "        return 0.0\n",
        "    return float((cov ** 2) / (vx * vy))\n",
        "\n",
        "def kernel_excir_eta(x: np.ndarray, y: np.ndarray, sigma_x: float = None, sigma_y: float = None) -> float:\n",
        "    \"\"\"\n",
        "    Kernel-ExCIR via PDF-defined Gaussian kernels and normalized HSIC/RV-style ratio:\n",
        "      eta_k = <Kc, Lc>_F^2 / ( <Kc,Kc>_F * <Lc,Lc>_F )\n",
        "    where Kc, Lc are centered Gram matrices on x and y respectively.\n",
        "    \"\"\"\n",
        "    if sigma_x is None:\n",
        "        sigma_x = median_heuristic_sigma(x)\n",
        "    if sigma_y is None:\n",
        "        sigma_y = median_heuristic_sigma(y)\n",
        "\n",
        "    K = gaussian_pdf_kernel_matrix(x, sigma_x)\n",
        "    L = gaussian_pdf_kernel_matrix(y, sigma_y)\n",
        "\n",
        "    Kc = center_gram(K)\n",
        "    Lc = center_gram(L)\n",
        "\n",
        "    num = np.sum(Kc * Lc) ** 2\n",
        "    denom = (np.sum(Kc * Kc) * np.sum(Lc * Lc))\n",
        "    if denom <= 0:\n",
        "        return 0.0\n",
        "    return float(num / denom)\n",
        "\n",
        "def empirical_mi_knn(x: np.ndarray, y: np.ndarray, n_neighbors: int = 10, random_state: int = 0) -> float:\n",
        "    \"\"\"\n",
        "    Empirical mutual information I(X;Y) using scikit-learn's kNN estimator.\n",
        "    Returns MI in nats.\n",
        "    \"\"\"\n",
        "    # mutual_info_regression expects X as 2D, y as 1D\n",
        "    X = x.reshape(-1, 1)\n",
        "    mi = mutual_info_regression(X, y, n_neighbors=n_neighbors, random_state=random_state)\n",
        "    return float(mi[0])\n",
        "\n",
        "@dataclass\n",
        "class ResultsRow:\n",
        "    method: str\n",
        "    eta_linear: str\n",
        "    eta_kernel: str\n",
        "    mi: str\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Data: Non-monotone dependence\n",
        "# ---------------------------\n",
        "np.random.seed(0)\n",
        "n = 1000\n",
        "X = np.random.rand(n)  # Uniform[0,1]\n",
        "eps = np.random.randn(n)\n",
        "Y = np.sin(4 * np.pi * X) + 0.1 * eps  # oscillatory mapping + small noise\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Metrics\n",
        "# ---------------------------\n",
        "eta_lin = linear_excir_eta(X, Y)\n",
        "eta_k = kernel_excir_eta(X, Y)  # PDF-defined Gaussian kernels (median heuristic)\n",
        "mi_xy = empirical_mi_knn(X, Y, n_neighbors=10, random_state=0)\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Table print\n",
        "# ---------------------------\n",
        "rows = [\n",
        "    ResultsRow(\"ExCIR\", f\"{eta_lin:.2f}\", \"--\", f\"{mi_xy:.2f}\"),\n",
        "    ResultsRow(\"Kernel--ExCIR\", \"--\", f\"{eta_k:.2f}\", f\"{mi_xy:.2f}\")\n",
        "]\n",
        "\n",
        "# Nicely formatted table in console\n",
        "header = f\"{'Method':<16} {'η (linear)':>12} {'η^(k) (RBF pdf)':>16} {'I(X;Y) (kNN)':>14}\"\n",
        "print(header)\n",
        "print(\"-\" * len(header))\n",
        "for r in rows:\n",
        "    print(f\"{r.method:<16} {r.eta_linear:>12} {r.eta_kernel:>16} {r.mi:>14}\")\n",
        "\n",
        "# Optionally: sanity check prints\n",
        "print(\"\\nRaw values:\")\n",
        "print(f\"eta_linear      = {eta_lin:.6f}\")\n",
        "print(f\"eta_kernel (pdf)= {eta_k:.6f}\")\n",
        "print(f\"MI kNN (nats)   = {mi_xy:.6f}\")\n"
      ],
      "metadata": {
        "id": "G4vvCRY-zyKu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}